{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.over_sampling import SMOTE,RandomOverSampler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from itertools import combinations \n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn import model_selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('60s_window_wrist_chest.csv',index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    312\n",
       "3    208\n",
       "1    175\n",
       "2     90\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#60sec\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    312\n",
       "3    208\n",
       "1    175\n",
       "2     90\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#30sec\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    312\n",
       "3    208\n",
       "1    175\n",
       "2     90\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#15sec\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=df.columns.tolist()\n",
    "features\n",
    "\n",
    "removed = ['label']\n",
    "for rem in removed:\n",
    "    features.remove(rem)\n",
    "\n",
    "features_with_sub=[]\n",
    "features_with_sub[:]=features\n",
    "# removed = ['subject']\n",
    "# for rem in removed:\n",
    "#     features.remove(rem)\n",
    "\n",
    "# feature=features\n",
    "# print(len(feature))\n",
    "# len(features_with_sub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/.local/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "sm = SMOTE(random_state=2)\n",
    "X, y= sm.fit_sample(df[features_with_sub], df['label'])\n",
    "df_new=pd.concat([pd.DataFrame(X,columns=features_with_sub),pd.DataFrame(y,columns=['label'])],axis=1)\n",
    "df_new\n",
    "\n",
    "for i in range (len(list(df_new['subject']))):\n",
    "    df_new['subject'][i] = min([2,3,4,5,6,7,8,9,10,11,13,14,15,16,17], key=lambda x:abs(x-df_new['subject'][i]))\n",
    "df_new['subject']=df_new['subject'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[df_new['subject']==2]['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['net_acc_mean', 'net_acc_std', 'net_acc_min', 'net_acc_max', 'ACC_x_mean', 'ACC_x_std', 'ACC_x_min', 'ACC_x_max', 'ACC_y_mean', 'ACC_y_std', 'ACC_y_min', 'ACC_y_max', 'ACC_z_mean', 'ACC_z_std', 'ACC_z_min', 'ACC_z_max', 'BVP_mean', 'BVP_std', 'BVP_min', 'BVP_max', 'ECG_mean', 'ECG_std', 'ECG_min', 'ECG_max', 'EDA_mean', 'EDA_std', 'EDA_min', 'EDA_max', 'EDA_phasic_mean', 'EDA_phasic_std', 'EDA_phasic_min', 'EDA_phasic_max', 'EDA_smna_mean', 'EDA_smna_std', 'EDA_smna_min', 'EDA_smna_max', 'EDA_tonic_mean', 'EDA_tonic_std', 'EDA_tonic_min', 'EDA_tonic_max', 'EMG_mean', 'EMG_std', 'EMG_min', 'EMG_max', 'Resp_mean', 'Resp_std', 'Resp_min', 'Resp_max', 'TEMP_mean', 'TEMP_std', 'TEMP_min', 'TEMP_max', 'c_ACC_x_mean', 'c_ACC_x_std', 'c_ACC_x_min', 'c_ACC_x_max', 'c_ACC_y_mean', 'c_ACC_y_std', 'c_ACC_y_min', 'c_ACC_y_max', 'c_ACC_z_mean', 'c_ACC_z_std', 'c_ACC_z_min', 'c_ACC_z_max', 'c_Temp_mean', 'c_Temp_std', 'c_Temp_min', 'c_Temp_max', 'BVP_peak_freq', 'TEMP_slope', 'subject', 'label']\n"
     ]
    }
   ],
   "source": [
    "features=df.columns.tolist()\n",
    "print(features)\n",
    "\n",
    "removed = ['subject']\n",
    "for rem in removed:\n",
    "    features.remove(rem)\n",
    "\n",
    "features_with_sub=[]\n",
    "features_with_sub[:]=features\n",
    "# removed = ['subject']\n",
    "# for rem in removed:\n",
    "#     features.remove(rem)\n",
    "\n",
    "# feature=features\n",
    "# print(len(feature))\n",
    "# len(features_with_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X, y= sm.fit_sample(df[features_with_sub], df['subject'])\n",
    "df_new=pd.concat([pd.DataFrame(X,columns=features_with_sub),pd.DataFrame(y,columns=['subject'])],axis=1)\n",
    "df_new\n",
    "\n",
    "for i in range (len(list(df_new['label']))):\n",
    "    df_new['label'][i] = min([0,1,2,3], key=lambda x:abs(x-df_new['label'][i]))\n",
    "df_new['label']=df_new['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    20\n",
       "1    14\n",
       "3    13\n",
       "2     7\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new[df_new['subject']==2]['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>net_acc_mean</th>\n",
       "      <th>net_acc_std</th>\n",
       "      <th>net_acc_min</th>\n",
       "      <th>net_acc_max</th>\n",
       "      <th>ACC_x_mean</th>\n",
       "      <th>ACC_x_std</th>\n",
       "      <th>ACC_x_min</th>\n",
       "      <th>ACC_x_max</th>\n",
       "      <th>ACC_y_mean</th>\n",
       "      <th>ACC_y_std</th>\n",
       "      <th>...</th>\n",
       "      <th>c_ACC_z_min</th>\n",
       "      <th>c_ACC_z_max</th>\n",
       "      <th>c_Temp_mean</th>\n",
       "      <th>c_Temp_std</th>\n",
       "      <th>c_Temp_min</th>\n",
       "      <th>c_Temp_max</th>\n",
       "      <th>BVP_peak_freq</th>\n",
       "      <th>TEMP_slope</th>\n",
       "      <th>subject</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.025961</td>\n",
       "      <td>0.013811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087383</td>\n",
       "      <td>0.023431</td>\n",
       "      <td>0.017769</td>\n",
       "      <td>-0.037843</td>\n",
       "      <td>0.087383</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>1.222594e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.870000</td>\n",
       "      <td>0.611000</td>\n",
       "      <td>29.168923</td>\n",
       "      <td>0.064290</td>\n",
       "      <td>28.994568</td>\n",
       "      <td>29.426208</td>\n",
       "      <td>0.081425</td>\n",
       "      <td>-0.000253</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.027640</td>\n",
       "      <td>0.010597</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>0.054356</td>\n",
       "      <td>0.027640</td>\n",
       "      <td>0.010597</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>0.054356</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>7.290999e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.759400</td>\n",
       "      <td>-0.681000</td>\n",
       "      <td>28.886605</td>\n",
       "      <td>0.074846</td>\n",
       "      <td>28.730682</td>\n",
       "      <td>29.207275</td>\n",
       "      <td>0.147017</td>\n",
       "      <td>-0.000161</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.028389</td>\n",
       "      <td>0.006937</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066053</td>\n",
       "      <td>0.028378</td>\n",
       "      <td>0.006985</td>\n",
       "      <td>-0.002752</td>\n",
       "      <td>0.066053</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>4.805734e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.753400</td>\n",
       "      <td>-0.675400</td>\n",
       "      <td>28.799659</td>\n",
       "      <td>0.037924</td>\n",
       "      <td>28.679108</td>\n",
       "      <td>28.988800</td>\n",
       "      <td>0.088210</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.033268</td>\n",
       "      <td>0.007670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074998</td>\n",
       "      <td>0.032960</td>\n",
       "      <td>0.008904</td>\n",
       "      <td>-0.030962</td>\n",
       "      <td>0.074998</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>6.126303e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.787800</td>\n",
       "      <td>0.166000</td>\n",
       "      <td>28.768865</td>\n",
       "      <td>0.058639</td>\n",
       "      <td>28.584656</td>\n",
       "      <td>29.023285</td>\n",
       "      <td>0.117614</td>\n",
       "      <td>-0.000256</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.037021</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.027522</td>\n",
       "      <td>0.043347</td>\n",
       "      <td>0.037021</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.027522</td>\n",
       "      <td>0.043347</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>8.837530e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.720200</td>\n",
       "      <td>-0.657000</td>\n",
       "      <td>28.598514</td>\n",
       "      <td>0.068128</td>\n",
       "      <td>28.447449</td>\n",
       "      <td>28.882599</td>\n",
       "      <td>0.151541</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1243</td>\n",
       "      <td>0.042130</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.040968</td>\n",
       "      <td>0.043696</td>\n",
       "      <td>-0.039802</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>-0.041346</td>\n",
       "      <td>-0.038618</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>2.254555e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.419116</td>\n",
       "      <td>-0.276862</td>\n",
       "      <td>34.905210</td>\n",
       "      <td>0.030299</td>\n",
       "      <td>34.784758</td>\n",
       "      <td>35.042778</td>\n",
       "      <td>0.164873</td>\n",
       "      <td>-0.000189</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1244</td>\n",
       "      <td>0.039764</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.039219</td>\n",
       "      <td>0.039907</td>\n",
       "      <td>0.039764</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.039219</td>\n",
       "      <td>0.039907</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>1.862190e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.386909</td>\n",
       "      <td>-0.282661</td>\n",
       "      <td>34.292072</td>\n",
       "      <td>0.026637</td>\n",
       "      <td>34.169402</td>\n",
       "      <td>34.434019</td>\n",
       "      <td>0.131407</td>\n",
       "      <td>-0.000133</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1245</td>\n",
       "      <td>0.041990</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.039308</td>\n",
       "      <td>0.043921</td>\n",
       "      <td>0.003807</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>0.006112</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1.590967e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.608181</td>\n",
       "      <td>-0.500244</td>\n",
       "      <td>33.758163</td>\n",
       "      <td>0.023225</td>\n",
       "      <td>33.601703</td>\n",
       "      <td>33.916210</td>\n",
       "      <td>0.142617</td>\n",
       "      <td>-0.000093</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1246</td>\n",
       "      <td>0.040894</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.040569</td>\n",
       "      <td>0.041733</td>\n",
       "      <td>0.040894</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.040569</td>\n",
       "      <td>0.041733</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>9.076908e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.536437</td>\n",
       "      <td>-0.411297</td>\n",
       "      <td>34.653072</td>\n",
       "      <td>0.034816</td>\n",
       "      <td>34.524100</td>\n",
       "      <td>34.800521</td>\n",
       "      <td>0.137624</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1247</td>\n",
       "      <td>0.021020</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.020642</td>\n",
       "      <td>0.022018</td>\n",
       "      <td>0.021020</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.020642</td>\n",
       "      <td>0.022018</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>1.957902e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.673160</td>\n",
       "      <td>-0.525649</td>\n",
       "      <td>34.962608</td>\n",
       "      <td>0.032261</td>\n",
       "      <td>34.836949</td>\n",
       "      <td>35.138398</td>\n",
       "      <td>0.147902</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1248 rows × 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      net_acc_mean  net_acc_std  net_acc_min  net_acc_max  ACC_x_mean  \\\n",
       "0         0.025961     0.013811     0.000000     0.087383    0.023431   \n",
       "1         0.027640     0.010597     0.002752     0.054356    0.027640   \n",
       "2         0.028389     0.006937     0.000000     0.066053    0.028378   \n",
       "3         0.033268     0.007670     0.000000     0.074998    0.032960   \n",
       "4         0.037021     0.001284     0.027522     0.043347    0.037021   \n",
       "...            ...          ...          ...          ...         ...   \n",
       "1243      0.042130     0.000328     0.040968     0.043696   -0.039802   \n",
       "1244      0.039764     0.000271     0.039219     0.039907    0.039764   \n",
       "1245      0.041990     0.000231     0.039308     0.043921    0.003807   \n",
       "1246      0.040894     0.000132     0.040569     0.041733    0.040894   \n",
       "1247      0.021020     0.000285     0.020642     0.022018    0.021020   \n",
       "\n",
       "      ACC_x_std  ACC_x_min  ACC_x_max  ACC_y_mean     ACC_y_std  ...  \\\n",
       "0      0.017769  -0.037843   0.087383    0.000016  1.222594e-05  ...   \n",
       "1      0.010597   0.002752   0.054356    0.000019  7.290999e-06  ...   \n",
       "2      0.006985  -0.002752   0.066053    0.000020  4.805734e-06  ...   \n",
       "3      0.008904  -0.030962   0.074998    0.000023  6.126303e-06  ...   \n",
       "4      0.001284   0.027522   0.043347    0.000025  8.837530e-07  ...   \n",
       "...         ...        ...        ...         ...           ...  ...   \n",
       "1243   0.000328  -0.041346  -0.038618   -0.000027  2.254555e-07  ...   \n",
       "1244   0.000271   0.039219   0.039907    0.000027  1.862190e-07  ...   \n",
       "1245   0.000231   0.001499   0.006112    0.000003  1.590967e-07  ...   \n",
       "1246   0.000132   0.040569   0.041733    0.000028  9.076908e-08  ...   \n",
       "1247   0.000285   0.020642   0.022018    0.000014  1.957902e-07  ...   \n",
       "\n",
       "      c_ACC_z_min  c_ACC_z_max  c_Temp_mean  c_Temp_std  c_Temp_min  \\\n",
       "0       -0.870000     0.611000    29.168923    0.064290   28.994568   \n",
       "1       -0.759400    -0.681000    28.886605    0.074846   28.730682   \n",
       "2       -0.753400    -0.675400    28.799659    0.037924   28.679108   \n",
       "3       -0.787800     0.166000    28.768865    0.058639   28.584656   \n",
       "4       -0.720200    -0.657000    28.598514    0.068128   28.447449   \n",
       "...           ...          ...          ...         ...         ...   \n",
       "1243    -0.419116    -0.276862    34.905210    0.030299   34.784758   \n",
       "1244    -0.386909    -0.282661    34.292072    0.026637   34.169402   \n",
       "1245    -0.608181    -0.500244    33.758163    0.023225   33.601703   \n",
       "1246    -0.536437    -0.411297    34.653072    0.034816   34.524100   \n",
       "1247    -0.673160    -0.525649    34.962608    0.032261   34.836949   \n",
       "\n",
       "      c_Temp_max  BVP_peak_freq  TEMP_slope  subject  label  \n",
       "0      29.426208       0.081425   -0.000253      2.0      0  \n",
       "1      29.207275       0.147017   -0.000161      2.0      0  \n",
       "2      28.988800       0.088210    0.000535      2.0      0  \n",
       "3      29.023285       0.117614   -0.000256      2.0      0  \n",
       "4      28.882599       0.151541    0.000260      2.0      0  \n",
       "...          ...            ...         ...      ...    ...  \n",
       "1243   35.042778       0.164873   -0.000189     11.0      3  \n",
       "1244   34.434019       0.131407   -0.000133     13.0      3  \n",
       "1245   33.916210       0.142617   -0.000093      9.0      3  \n",
       "1246   34.800521       0.137624    0.000187      5.0      3  \n",
       "1247   35.138398       0.147902   -0.000013      9.0      3  \n",
       "\n",
       "[1248 rows x 72 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_d=pd.read_csv('personal_detail.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>net_acc_mean</th>\n",
       "      <th>net_acc_std</th>\n",
       "      <th>net_acc_min</th>\n",
       "      <th>net_acc_max</th>\n",
       "      <th>ACC_x_mean</th>\n",
       "      <th>ACC_x_std</th>\n",
       "      <th>ACC_x_min</th>\n",
       "      <th>ACC_x_max</th>\n",
       "      <th>ACC_y_mean</th>\n",
       "      <th>ACC_y_std</th>\n",
       "      <th>...</th>\n",
       "      <th>label</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender_ female</th>\n",
       "      <th>coffee_today_YES</th>\n",
       "      <th>sport_today_YES</th>\n",
       "      <th>smoker_YES</th>\n",
       "      <th>feel_ill_today_YES</th>\n",
       "      <th>bmi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.025961</td>\n",
       "      <td>0.013811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087383</td>\n",
       "      <td>0.023431</td>\n",
       "      <td>0.017769</td>\n",
       "      <td>-0.037843</td>\n",
       "      <td>0.087383</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>1.222594e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>175</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.027640</td>\n",
       "      <td>0.010597</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>0.054356</td>\n",
       "      <td>0.027640</td>\n",
       "      <td>0.010597</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>0.054356</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>7.290999e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>175</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.028389</td>\n",
       "      <td>0.006937</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066053</td>\n",
       "      <td>0.028378</td>\n",
       "      <td>0.006985</td>\n",
       "      <td>-0.002752</td>\n",
       "      <td>0.066053</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>4.805734e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>175</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.033268</td>\n",
       "      <td>0.007670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074998</td>\n",
       "      <td>0.032960</td>\n",
       "      <td>0.008904</td>\n",
       "      <td>-0.030962</td>\n",
       "      <td>0.074998</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>6.126303e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>175</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.037021</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.027522</td>\n",
       "      <td>0.043347</td>\n",
       "      <td>0.037021</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.027522</td>\n",
       "      <td>0.043347</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>8.837530e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>175</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1243</td>\n",
       "      <td>0.029484</td>\n",
       "      <td>0.002074</td>\n",
       "      <td>0.020918</td>\n",
       "      <td>0.053804</td>\n",
       "      <td>-0.004624</td>\n",
       "      <td>0.002074</td>\n",
       "      <td>-0.015439</td>\n",
       "      <td>0.017447</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>1.427082e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>165</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1244</td>\n",
       "      <td>0.032744</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.029211</td>\n",
       "      <td>0.034857</td>\n",
       "      <td>-0.029334</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>-0.031478</td>\n",
       "      <td>-0.025832</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>3.552867e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>165</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1245</td>\n",
       "      <td>0.030006</td>\n",
       "      <td>0.007051</td>\n",
       "      <td>0.002966</td>\n",
       "      <td>0.070357</td>\n",
       "      <td>-0.027424</td>\n",
       "      <td>0.007051</td>\n",
       "      <td>-0.067796</td>\n",
       "      <td>-0.000404</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>4.851210e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>165</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1246</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>0.027188</td>\n",
       "      <td>0.038575</td>\n",
       "      <td>-0.031250</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>-0.038575</td>\n",
       "      <td>-0.027188</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>1.055452e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>165</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1247</td>\n",
       "      <td>0.035683</td>\n",
       "      <td>0.010608</td>\n",
       "      <td>0.004897</td>\n",
       "      <td>0.063659</td>\n",
       "      <td>-0.030181</td>\n",
       "      <td>0.010608</td>\n",
       "      <td>-0.058393</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>7.299080e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "      <td>165</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1248 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      net_acc_mean  net_acc_std  net_acc_min  net_acc_max  ACC_x_mean  \\\n",
       "0         0.025961     0.013811     0.000000     0.087383    0.023431   \n",
       "1         0.027640     0.010597     0.002752     0.054356    0.027640   \n",
       "2         0.028389     0.006937     0.000000     0.066053    0.028378   \n",
       "3         0.033268     0.007670     0.000000     0.074998    0.032960   \n",
       "4         0.037021     0.001284     0.027522     0.043347    0.037021   \n",
       "...            ...          ...          ...          ...         ...   \n",
       "1243      0.029484     0.002074     0.020918     0.053804   -0.004624   \n",
       "1244      0.032744     0.000516     0.029211     0.034857   -0.029334   \n",
       "1245      0.030006     0.007051     0.002966     0.070357   -0.027424   \n",
       "1246      0.031250     0.001534     0.027188     0.038575   -0.031250   \n",
       "1247      0.035683     0.010608     0.004897     0.063659   -0.030181   \n",
       "\n",
       "      ACC_x_std  ACC_x_min  ACC_x_max  ACC_y_mean     ACC_y_std  ...  label  \\\n",
       "0      0.017769  -0.037843   0.087383    0.000016  1.222594e-05  ...      0   \n",
       "1      0.010597   0.002752   0.054356    0.000019  7.290999e-06  ...      0   \n",
       "2      0.006985  -0.002752   0.066053    0.000020  4.805734e-06  ...      0   \n",
       "3      0.008904  -0.030962   0.074998    0.000023  6.126303e-06  ...      0   \n",
       "4      0.001284   0.027522   0.043347    0.000025  8.837530e-07  ...      0   \n",
       "...         ...        ...        ...         ...           ...  ...    ...   \n",
       "1243   0.002074  -0.015439   0.017447   -0.000003  1.427082e-06  ...      1   \n",
       "1244   0.000516  -0.031478  -0.025832   -0.000020  3.552867e-07  ...      2   \n",
       "1245   0.007051  -0.067796  -0.000404   -0.000019  4.851210e-06  ...      2   \n",
       "1246   0.001534  -0.038575  -0.027188   -0.000022  1.055452e-06  ...      2   \n",
       "1247   0.010608  -0.058393   0.000368   -0.000021  7.299080e-06  ...      3   \n",
       "\n",
       "      age  height  weight  gender_ female  coffee_today_YES  sport_today_YES  \\\n",
       "0      27     175      80               0                 0                0   \n",
       "1      27     175      80               0                 0                0   \n",
       "2      27     175      80               0                 0                0   \n",
       "3      27     175      80               0                 0                0   \n",
       "4      27     175      80               0                 0                0   \n",
       "...   ...     ...     ...             ...               ...              ...   \n",
       "1243   29     165      55               1                 0                0   \n",
       "1244   29     165      55               1                 0                0   \n",
       "1245   29     165      55               1                 0                0   \n",
       "1246   29     165      55               1                 0                0   \n",
       "1247   29     165      55               1                 0                0   \n",
       "\n",
       "      smoker_YES  feel_ill_today_YES  bmi  \n",
       "0              0                   0    1  \n",
       "1              0                   0    1  \n",
       "2              0                   0    1  \n",
       "3              0                   0    1  \n",
       "4              0                   0    1  \n",
       "...          ...                 ...  ...  \n",
       "1243           0                   0    0  \n",
       "1244           0                   0    0  \n",
       "1245           0                   0    0  \n",
       "1246           0                   0    0  \n",
       "1247           0                   0    0  \n",
       "\n",
       "[1248 rows x 81 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new_1=df_new.merge(p_d,on='subject')\n",
    "df_new_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    312\n",
       "2    312\n",
       "1    312\n",
       "0    312\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new_1['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features=df_new_1.columns.tolist()\n",
    "features\n",
    "\n",
    "removed = ['label']\n",
    "for rem in removed:\n",
    "    features.remove(rem)\n",
    "features_with_sub=[]\n",
    "features_with_sub[:]=features\n",
    "removed = ['subject']\n",
    "for rem in removed:\n",
    "    features.remove(rem)\n",
    "\n",
    "feature=features\n",
    "print(len(feature))\n",
    "len(features_with_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=df_new_1[df_new_1['subject']<=9]\n",
    "test=df_new_1[df_new_1['subject']>9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = Normalizer()\n",
    "scaled_data_train = scaler.fit_transform(train[feature])\n",
    "scaled_data_test = scaler.transform(test[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "et = ExtraTreesClassifier(n_estimators=100,n_jobs=10,random_state=56)\n",
    "et.fit(scaled_data_train,train['label'])\n",
    "y_pred=et.predict(scaled_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.62      0.69       147\n",
      "           1       0.90      0.92      0.91       161\n",
      "           2       0.65      0.79      0.71       146\n",
      "           3       0.76      0.74      0.75       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.77      0.77      0.77       603\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test['label'],y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RFECV(cv=StratifiedKFold(n_splits=10, random_state=None, shuffle=False),\n",
       "      estimator=ExtraTreesClassifier(bootstrap=False, class_weight=None,\n",
       "                                     criterion='gini', max_depth=None,\n",
       "                                     max_features='auto', max_leaf_nodes=None,\n",
       "                                     min_impurity_decrease=0.0,\n",
       "                                     min_impurity_split=None,\n",
       "                                     min_samples_leaf=1, min_samples_split=2,\n",
       "                                     min_weight_fraction_leaf=0.0,\n",
       "                                     n_estimators=100, n_jobs=10,\n",
       "                                     oob_score=False, random_state=56,\n",
       "                                     verbose=0, warm_start=False),\n",
       "      min_features_to_select=1, n_jobs=None, scoring='accuracy', step=1,\n",
       "      verbose=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfecv = RFECV(estimator=et, step=1, cv=StratifiedKFold(10), scoring='accuracy')\n",
    "rfecv.fit(scaled_data_train,train['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features: 50\n"
     ]
    }
   ],
   "source": [
    "print('Optimal number of features: {}'.format(rfecv.n_features_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16, 9))\n",
    "plt.title('Recursive Feature Elimination with Cross-Validation', fontsize=18, fontweight='bold', pad=20)\n",
    "plt.xlabel('Number of features selected', fontsize=14, labelpad=20)\n",
    "plt.ylabel('% Correct Classification', fontsize=14, labelpad=20)\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_, color='#303F9F', linewidth=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6  8  9 10 11 12 13 14 15 16 19 20 21 34 37 44 50 54 55 61 65 66 72 73\n",
      " 74 75 76 77 78]\n"
     ]
    }
   ],
   "source": [
    "print(np.where(rfecv.support_ == False)[0])\n",
    "\n",
    "X = pd.DataFrame(data=scaled_data_train)\n",
    "X             \n",
    "X_test =  pd.DataFrame(data=scaled_data_test)\n",
    "\n",
    "X.drop(X.columns[np.where(rfecv.support_ == False)[0]], axis=1, inplace=True)\n",
    "X_test.drop(X_test.columns[np.where(rfecv.support_ == False)[0]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "640    3\n",
       "641    3\n",
       "642    3\n",
       "643    3\n",
       "644    3\n",
       "Name: label, Length: 645, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>7</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>22</th>\n",
       "      <th>...</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>1.916863e-05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>2.466130e-05</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.157290</td>\n",
       "      <td>-0.544442</td>\n",
       "      <td>-0.000605</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>-0.000611</td>\n",
       "      <td>-0.001207</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>0.040483</td>\n",
       "      <td>0.040840</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>-3.515287e-07</td>\n",
       "      <td>0.037473</td>\n",
       "      <td>0.242881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>2.543154e-05</td>\n",
       "      <td>6.605255e-06</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>2.543152e-05</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.100563</td>\n",
       "      <td>-0.694218</td>\n",
       "      <td>-0.000727</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000169</td>\n",
       "      <td>-0.001717</td>\n",
       "      <td>-0.001823</td>\n",
       "      <td>-0.001634</td>\n",
       "      <td>0.069327</td>\n",
       "      <td>0.070097</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>-3.863574e-07</td>\n",
       "      <td>0.064799</td>\n",
       "      <td>0.419996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>1.237292e-05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>1.245783e-05</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.104943</td>\n",
       "      <td>-0.654788</td>\n",
       "      <td>-0.002674</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000113</td>\n",
       "      <td>-0.001275</td>\n",
       "      <td>-0.001344</td>\n",
       "      <td>-0.001205</td>\n",
       "      <td>0.051368</td>\n",
       "      <td>0.051705</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>9.548836e-07</td>\n",
       "      <td>0.048158</td>\n",
       "      <td>0.312135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>9.652325e-06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>1.120539e-05</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.105996</td>\n",
       "      <td>-0.843442</td>\n",
       "      <td>-0.000510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>-0.000791</td>\n",
       "      <td>-0.000991</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.036205</td>\n",
       "      <td>0.036526</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>-3.223919e-07</td>\n",
       "      <td>0.033979</td>\n",
       "      <td>0.220236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>4.587590e-06</td>\n",
       "      <td>9.830095e-05</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>4.587588e-06</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.075116</td>\n",
       "      <td>-0.430140</td>\n",
       "      <td>-0.000999</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000094</td>\n",
       "      <td>-0.002483</td>\n",
       "      <td>-0.002572</td>\n",
       "      <td>-0.002347</td>\n",
       "      <td>0.102145</td>\n",
       "      <td>0.103160</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>9.295874e-07</td>\n",
       "      <td>0.096436</td>\n",
       "      <td>0.625048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>1.144749e-06</td>\n",
       "      <td>6.051283e-05</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>1.144749e-06</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.080976</td>\n",
       "      <td>-0.211564</td>\n",
       "      <td>-0.003666</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>-0.003799</td>\n",
       "      <td>-0.004184</td>\n",
       "      <td>-0.003548</td>\n",
       "      <td>0.156322</td>\n",
       "      <td>0.157009</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>-1.919552e-07</td>\n",
       "      <td>0.116742</td>\n",
       "      <td>0.812703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>641</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>1.386835e-05</td>\n",
       "      <td>4.128478e-07</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>1.860572e-05</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.108694</td>\n",
       "      <td>-0.653566</td>\n",
       "      <td>-0.001562</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>-0.001357</td>\n",
       "      <td>-0.001547</td>\n",
       "      <td>-0.001183</td>\n",
       "      <td>0.091593</td>\n",
       "      <td>0.092045</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>1.566840e-07</td>\n",
       "      <td>0.068320</td>\n",
       "      <td>0.475614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>642</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>1.227040e-06</td>\n",
       "      <td>1.963192e-04</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>1.227039e-06</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.030277</td>\n",
       "      <td>-0.089265</td>\n",
       "      <td>-0.003241</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>-0.002963</td>\n",
       "      <td>-0.003161</td>\n",
       "      <td>-0.002759</td>\n",
       "      <td>0.162092</td>\n",
       "      <td>0.162731</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>-2.893492e-06</td>\n",
       "      <td>0.120797</td>\n",
       "      <td>0.840935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>643</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>9.418143e-07</td>\n",
       "      <td>1.601074e-04</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>9.418139e-07</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.135472</td>\n",
       "      <td>-0.364515</td>\n",
       "      <td>-0.001507</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000318</td>\n",
       "      <td>-0.002243</td>\n",
       "      <td>-0.002477</td>\n",
       "      <td>-0.002038</td>\n",
       "      <td>0.137501</td>\n",
       "      <td>0.138144</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>-3.776197e-07</td>\n",
       "      <td>0.105901</td>\n",
       "      <td>0.737232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>644</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>1.227950e-06</td>\n",
       "      <td>8.907482e-05</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>1.227949e-06</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.111106</td>\n",
       "      <td>-0.291769</td>\n",
       "      <td>-0.003734</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>-0.002556</td>\n",
       "      <td>-0.002905</td>\n",
       "      <td>-0.002268</td>\n",
       "      <td>0.150874</td>\n",
       "      <td>0.151633</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>-5.666141e-08</td>\n",
       "      <td>0.112198</td>\n",
       "      <td>0.781070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>645 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0             1             2         3         4             5   \\\n",
       "0    0.000036  1.916863e-05  0.000000e+00  0.000121  0.000033  2.466130e-05   \n",
       "1    0.000066  2.543154e-05  6.605255e-06  0.000130  0.000066  2.543152e-05   \n",
       "2    0.000051  1.237292e-05  0.000000e+00  0.000118  0.000051  1.245783e-05   \n",
       "3    0.000042  9.652325e-06  0.000000e+00  0.000094  0.000041  1.120539e-05   \n",
       "4    0.000132  4.587590e-06  9.830095e-05  0.000155  0.000132  4.587588e-06   \n",
       "..        ...           ...           ...       ...       ...           ...   \n",
       "640  0.000062  1.144749e-06  6.051283e-05  0.000065  0.000062  1.144749e-06   \n",
       "641  0.000055  1.386835e-05  4.128478e-07  0.000146  0.000035  1.860572e-05   \n",
       "642  0.000198  1.227040e-06  1.963192e-04  0.000200  0.000198  1.227039e-06   \n",
       "643  0.000171  9.418143e-07  1.601074e-04  0.000179  0.000016  9.418139e-07   \n",
       "644  0.000091  1.227950e-06  8.907482e-05  0.000095  0.000091  1.227949e-06   \n",
       "\n",
       "           7         17        18        22  ...        59        60  \\\n",
       "0    0.000121  0.157290 -0.544442 -0.000605  ...  0.000060 -0.000611   \n",
       "1    0.000130  0.100563 -0.694218 -0.000727  ... -0.000169 -0.001717   \n",
       "2    0.000118  0.104943 -0.654788 -0.002674  ... -0.000113 -0.001275   \n",
       "3    0.000094  0.105996 -0.843442 -0.000510  ...  0.000115 -0.000791   \n",
       "4    0.000155  0.075116 -0.430140 -0.000999  ... -0.000094 -0.002483   \n",
       "..        ...       ...       ...       ...  ...       ...       ...   \n",
       "640  0.000065  0.080976 -0.211564 -0.003666  ...  0.000172 -0.003799   \n",
       "641  0.000131  0.108694 -0.653566 -0.001562  ...  0.000088 -0.001357   \n",
       "642  0.000200  0.030277 -0.089265 -0.003241  ...  0.000113 -0.002963   \n",
       "643  0.000025  0.135472 -0.364515 -0.001507  ... -0.000318 -0.002243   \n",
       "644  0.000095  0.111106 -0.291769 -0.003734  ... -0.000047 -0.002556   \n",
       "\n",
       "           62        63        64        67        68            69        70  \\\n",
       "0   -0.001207  0.000848  0.040483  0.040840  0.000113 -3.515287e-07  0.037473   \n",
       "1   -0.001823 -0.001634  0.069327  0.070097  0.000353 -3.863574e-07  0.064799   \n",
       "2   -0.001344 -0.001205  0.051368  0.051705  0.000157  9.548836e-07  0.048158   \n",
       "3   -0.000991  0.000209  0.036205  0.036526  0.000148 -3.223919e-07  0.033979   \n",
       "4   -0.002572 -0.002347  0.102145  0.103160  0.000541  9.295874e-07  0.096436   \n",
       "..        ...       ...       ...       ...       ...           ...       ...   \n",
       "640 -0.004184 -0.003548  0.156322  0.157009  0.000644 -1.919552e-07  0.116742   \n",
       "641 -0.001547 -0.001183  0.091593  0.092045  0.000309  1.566840e-07  0.068320   \n",
       "642 -0.003161 -0.002759  0.162092  0.162731  0.000660 -2.893492e-06  0.120797   \n",
       "643 -0.002477 -0.002038  0.137501  0.138144  0.000581 -3.776197e-07  0.105901   \n",
       "644 -0.002905 -0.002268  0.150874  0.151633  0.000638 -5.666141e-08  0.112198   \n",
       "\n",
       "           71  \n",
       "0    0.242881  \n",
       "1    0.419996  \n",
       "2    0.312135  \n",
       "3    0.220236  \n",
       "4    0.625048  \n",
       "..        ...  \n",
       "640  0.812703  \n",
       "641  0.475614  \n",
       "642  0.840935  \n",
       "643  0.737232  \n",
       "644  0.781070  \n",
       "\n",
       "[645 rows x 50 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "et = ExtraTreesClassifier(n_estimators=100,n_jobs=10,random_state=56)\n",
    "et.fit(X,train['label'])\n",
    "y_pred=et.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.56      0.66       147\n",
      "           1       0.89      0.91      0.90       161\n",
      "           2       0.58      0.79      0.67       146\n",
      "           3       0.68      0.64      0.66       149\n",
      "\n",
      "    accuracy                           0.73       603\n",
      "   macro avg       0.74      0.72      0.72       603\n",
      "weighted avg       0.75      0.73      0.73       603\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test['label'],y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.61      0.69       147\n",
      "           1       0.88      0.92      0.90       161\n",
      "           2       0.61      0.80      0.69       146\n",
      "           3       0.73      0.64      0.68       149\n",
      "\n",
      "    accuracy                           0.75       603\n",
      "   macro avg       0.75      0.74      0.74       603\n",
      "weighted avg       0.76      0.75      0.74       603\n",
      "\n",
      "22\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.61      0.69       147\n",
      "           1       0.91      0.88      0.89       161\n",
      "           2       0.60      0.74      0.66       146\n",
      "           3       0.71      0.75      0.73       149\n",
      "\n",
      "    accuracy                           0.75       603\n",
      "   macro avg       0.76      0.74      0.74       603\n",
      "weighted avg       0.76      0.75      0.75       603\n",
      "\n",
      "25\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.61      0.69       147\n",
      "           1       0.90      0.91      0.90       161\n",
      "           2       0.61      0.77      0.68       146\n",
      "           3       0.70      0.68      0.69       149\n",
      "\n",
      "    accuracy                           0.75       603\n",
      "   macro avg       0.75      0.74      0.74       603\n",
      "weighted avg       0.76      0.75      0.75       603\n",
      "\n",
      "30\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.61      0.70       147\n",
      "           1       0.90      0.91      0.90       161\n",
      "           2       0.58      0.76      0.66       146\n",
      "           3       0.69      0.66      0.67       149\n",
      "\n",
      "    accuracy                           0.74       603\n",
      "   macro avg       0.75      0.73      0.73       603\n",
      "weighted avg       0.75      0.74      0.74       603\n",
      "\n",
      "36\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.61      0.69       147\n",
      "           1       0.92      0.89      0.90       161\n",
      "           2       0.59      0.75      0.66       146\n",
      "           3       0.68      0.66      0.67       149\n",
      "\n",
      "    accuracy                           0.73       603\n",
      "   macro avg       0.74      0.73      0.73       603\n",
      "weighted avg       0.75      0.73      0.73       603\n",
      "\n",
      "52\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.62      0.71       147\n",
      "           1       0.91      0.86      0.88       161\n",
      "           2       0.55      0.76      0.64       146\n",
      "           3       0.68      0.64      0.66       149\n",
      "\n",
      "    accuracy                           0.72       603\n",
      "   macro avg       0.75      0.72      0.72       603\n",
      "weighted avg       0.75      0.72      0.73       603\n",
      "\n",
      "53\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.62      0.71       147\n",
      "           1       0.87      0.91      0.89       161\n",
      "           2       0.61      0.71      0.66       146\n",
      "           3       0.70      0.72      0.71       149\n",
      "\n",
      "    accuracy                           0.74       603\n",
      "   macro avg       0.75      0.74      0.74       603\n",
      "weighted avg       0.75      0.74      0.74       603\n",
      "\n",
      "71\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.61      0.70       147\n",
      "           1       0.87      0.92      0.89       161\n",
      "           2       0.60      0.74      0.66       146\n",
      "           3       0.70      0.68      0.69       149\n",
      "\n",
      "    accuracy                           0.74       603\n",
      "   macro avg       0.75      0.74      0.74       603\n",
      "weighted avg       0.75      0.74      0.74       603\n",
      "\n",
      "109\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.64      0.73       147\n",
      "           1       0.86      0.89      0.88       161\n",
      "           2       0.59      0.74      0.65       146\n",
      "           3       0.70      0.67      0.68       149\n",
      "\n",
      "    accuracy                           0.74       603\n",
      "   macro avg       0.75      0.74      0.74       603\n",
      "weighted avg       0.76      0.74      0.74       603\n",
      "\n",
      "111\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.61      0.71       147\n",
      "           1       0.92      0.89      0.90       161\n",
      "           2       0.55      0.84      0.67       146\n",
      "           3       0.74      0.62      0.67       149\n",
      "\n",
      "    accuracy                           0.74       603\n",
      "   macro avg       0.77      0.74      0.74       603\n",
      "weighted avg       0.77      0.74      0.74       603\n",
      "\n",
      "142\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.61      0.70       147\n",
      "           1       0.91      0.89      0.90       161\n",
      "           2       0.57      0.80      0.67       146\n",
      "           3       0.69      0.62      0.65       149\n",
      "\n",
      "    accuracy                           0.73       603\n",
      "   macro avg       0.75      0.73      0.73       603\n",
      "weighted avg       0.76      0.73      0.74       603\n",
      "\n",
      "151\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.64      0.73       147\n",
      "           1       0.90      0.89      0.90       161\n",
      "           2       0.60      0.79      0.69       146\n",
      "           3       0.74      0.70      0.72       149\n",
      "\n",
      "    accuracy                           0.76       603\n",
      "   macro avg       0.78      0.76      0.76       603\n",
      "weighted avg       0.78      0.76      0.76       603\n",
      "\n",
      "157\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.61      0.69       147\n",
      "           1       0.89      0.86      0.88       161\n",
      "           2       0.58      0.76      0.66       146\n",
      "           3       0.67      0.64      0.65       149\n",
      "\n",
      "    accuracy                           0.72       603\n",
      "   macro avg       0.73      0.72      0.72       603\n",
      "weighted avg       0.74      0.72      0.72       603\n",
      "\n",
      "160\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.65      0.73       147\n",
      "           1       0.90      0.91      0.90       161\n",
      "           2       0.60      0.75      0.67       146\n",
      "           3       0.72      0.69      0.71       149\n",
      "\n",
      "    accuracy                           0.75       603\n",
      "   macro avg       0.76      0.75      0.75       603\n",
      "weighted avg       0.77      0.75      0.76       603\n",
      "\n",
      "166\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.61      0.71       147\n",
      "           1       0.87      0.91      0.89       161\n",
      "           2       0.60      0.76      0.67       146\n",
      "           3       0.72      0.70      0.71       149\n",
      "\n",
      "    accuracy                           0.75       603\n",
      "   macro avg       0.76      0.75      0.75       603\n",
      "weighted avg       0.76      0.75      0.75       603\n",
      "\n",
      "171\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.62      0.71       147\n",
      "           1       0.88      0.87      0.87       161\n",
      "           2       0.60      0.72      0.65       146\n",
      "           3       0.66      0.70      0.68       149\n",
      "\n",
      "    accuracy                           0.73       603\n",
      "   macro avg       0.74      0.73      0.73       603\n",
      "weighted avg       0.74      0.73      0.73       603\n",
      "\n",
      "187\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.61      0.73       147\n",
      "           1       0.84      0.93      0.88       161\n",
      "           2       0.59      0.72      0.65       146\n",
      "           3       0.71      0.69      0.70       149\n",
      "\n",
      "    accuracy                           0.74       603\n",
      "   macro avg       0.76      0.74      0.74       603\n",
      "weighted avg       0.76      0.74      0.74       603\n",
      "\n",
      "188\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.63      0.72       147\n",
      "           1       0.89      0.94      0.92       161\n",
      "           2       0.62      0.75      0.68       146\n",
      "           3       0.69      0.69      0.69       149\n",
      "\n",
      "    accuracy                           0.75       603\n",
      "   macro avg       0.76      0.75      0.75       603\n",
      "weighted avg       0.76      0.75      0.75       603\n",
      "\n",
      "219\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.61      0.71       147\n",
      "           1       0.86      0.89      0.87       161\n",
      "           2       0.62      0.77      0.69       146\n",
      "           3       0.68      0.68      0.68       149\n",
      "\n",
      "    accuracy                           0.74       603\n",
      "   macro avg       0.75      0.74      0.74       603\n",
      "weighted avg       0.76      0.74      0.74       603\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.61      0.70       147\n",
      "           1       0.90      0.91      0.91       161\n",
      "           2       0.61      0.77      0.68       146\n",
      "           3       0.69      0.70      0.70       149\n",
      "\n",
      "    accuracy                           0.75       603\n",
      "   macro avg       0.76      0.75      0.75       603\n",
      "weighted avg       0.76      0.75      0.75       603\n",
      "\n",
      "237\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.62      0.71       147\n",
      "           1       0.91      0.90      0.91       161\n",
      "           2       0.63      0.78      0.70       146\n",
      "           3       0.68      0.70      0.69       149\n",
      "\n",
      "    accuracy                           0.75       603\n",
      "   macro avg       0.76      0.75      0.75       603\n",
      "weighted avg       0.76      0.75      0.75       603\n",
      "\n",
      "240\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.61      0.70       147\n",
      "           1       0.91      0.89      0.90       161\n",
      "           2       0.57      0.77      0.66       146\n",
      "           3       0.70      0.65      0.68       149\n",
      "\n",
      "    accuracy                           0.73       603\n",
      "   macro avg       0.75      0.73      0.73       603\n",
      "weighted avg       0.75      0.73      0.74       603\n",
      "\n",
      "267\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.61      0.72       147\n",
      "           1       0.93      0.89      0.91       161\n",
      "           2       0.59      0.76      0.66       146\n",
      "           3       0.67      0.70      0.69       149\n",
      "\n",
      "    accuracy                           0.75       603\n",
      "   macro avg       0.77      0.74      0.75       603\n",
      "weighted avg       0.77      0.75      0.75       603\n",
      "\n",
      "275\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.62      0.71       147\n",
      "           1       0.90      0.86      0.88       161\n",
      "           2       0.61      0.77      0.68       146\n",
      "           3       0.70      0.72      0.71       149\n",
      "\n",
      "    accuracy                           0.75       603\n",
      "   macro avg       0.76      0.75      0.75       603\n",
      "weighted avg       0.76      0.75      0.75       603\n",
      "\n",
      "287\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.63      0.71       147\n",
      "           1       0.89      0.86      0.87       161\n",
      "           2       0.62      0.78      0.69       146\n",
      "           3       0.70      0.69      0.69       149\n",
      "\n",
      "    accuracy                           0.74       603\n",
      "   macro avg       0.75      0.74      0.74       603\n",
      "weighted avg       0.76      0.74      0.74       603\n",
      "\n",
      "297\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.61      0.71       147\n",
      "           1       0.89      0.87      0.88       161\n",
      "           2       0.58      0.76      0.66       146\n",
      "           3       0.67      0.69      0.68       149\n",
      "\n",
      "    accuracy                           0.73       603\n",
      "   macro avg       0.75      0.73      0.73       603\n",
      "weighted avg       0.76      0.73      0.74       603\n",
      "\n",
      "301\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.61      0.69       147\n",
      "           1       0.89      0.90      0.90       161\n",
      "           2       0.65      0.77      0.71       146\n",
      "           3       0.69      0.72      0.70       149\n",
      "\n",
      "    accuracy                           0.75       603\n",
      "   macro avg       0.76      0.75      0.75       603\n",
      "weighted avg       0.76      0.75      0.75       603\n",
      "\n",
      "303\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.61      0.71       147\n",
      "           1       0.91      0.92      0.91       161\n",
      "           2       0.57      0.77      0.65       146\n",
      "           3       0.66      0.60      0.63       149\n",
      "\n",
      "    accuracy                           0.73       603\n",
      "   macro avg       0.74      0.72      0.73       603\n",
      "weighted avg       0.75      0.73      0.73       603\n",
      "\n",
      "322\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.61      0.71       147\n",
      "           1       0.89      0.90      0.90       161\n",
      "           2       0.59      0.79      0.68       146\n",
      "           3       0.70      0.67      0.69       149\n",
      "\n",
      "    accuracy                           0.75       603\n",
      "   macro avg       0.76      0.74      0.74       603\n",
      "weighted avg       0.77      0.75      0.75       603\n",
      "\n",
      "357\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.63      0.72       147\n",
      "           1       0.88      0.88      0.88       161\n",
      "           2       0.58      0.76      0.66       146\n",
      "           3       0.73      0.70      0.72       149\n",
      "\n",
      "    accuracy                           0.75       603\n",
      "   macro avg       0.76      0.74      0.75       603\n",
      "weighted avg       0.76      0.75      0.75       603\n",
      "\n",
      "450\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.61      0.69       147\n",
      "           1       0.90      0.88      0.89       161\n",
      "           2       0.64      0.75      0.69       146\n",
      "           3       0.65      0.72      0.69       149\n",
      "\n",
      "    accuracy                           0.74       603\n",
      "   macro avg       0.75      0.74      0.74       603\n",
      "weighted avg       0.75      0.74      0.74       603\n",
      "\n",
      "461\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.63      0.70       147\n",
      "           1       0.86      0.89      0.88       161\n",
      "           2       0.60      0.71      0.65       146\n",
      "           3       0.67      0.66      0.66       149\n",
      "\n",
      "    accuracy                           0.72       603\n",
      "   macro avg       0.73      0.72      0.72       603\n",
      "weighted avg       0.73      0.72      0.72       603\n",
      "\n",
      "474\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.61      0.71       147\n",
      "           1       0.88      0.91      0.90       161\n",
      "           2       0.60      0.76      0.67       146\n",
      "           3       0.71      0.72      0.72       149\n",
      "\n",
      "    accuracy                           0.75       603\n",
      "   macro avg       0.77      0.75      0.75       603\n",
      "weighted avg       0.77      0.75      0.75       603\n",
      "\n",
      "479\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.62      0.70       147\n",
      "           1       0.91      0.89      0.90       161\n",
      "           2       0.56      0.75      0.64       146\n",
      "           3       0.68      0.62      0.65       149\n",
      "\n",
      "    accuracy                           0.72       603\n",
      "   macro avg       0.74      0.72      0.72       603\n",
      "weighted avg       0.74      0.72      0.73       603\n",
      "\n",
      "493\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.61      0.68       147\n",
      "           1       0.86      0.91      0.88       161\n",
      "           2       0.63      0.73      0.68       146\n",
      "           3       0.71      0.72      0.72       149\n",
      "\n",
      "    accuracy                           0.74       603\n",
      "   macro avg       0.75      0.74      0.74       603\n",
      "weighted avg       0.75      0.74      0.74       603\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range (501):\n",
    "    #print (i)\n",
    "    et = ExtraTreesClassifier(n_estimators=100,n_jobs=10,random_state=i)\n",
    "    et.fit(X,train['label'])\n",
    "    y_pred=et.predict(X_test)\n",
    "    #print(classification_report(test['label'],y_pred))\n",
    "    if ((classification_report(test['label'],y_pred,output_dict=True)['0']['recall'])>.60 and (classification_report(test['label'],y_pred,output_dict=True)['2']['recall'])>.60 and (classification_report(test['label'],y_pred,output_dict=True)['3']['recall'])>.60):\n",
    "        print(i)\n",
    "        print(classification_report(test['label'],y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STACKED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7280265339966833"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(test['label'],y_pred,output_dict = True)['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "12\n",
      "14\n",
      "56\n",
      "75\n",
      "92\n",
      "116\n",
      "128\n",
      "135\n",
      "140\n",
      "151\n",
      "182\n",
      "194\n",
      "206\n",
      "208\n",
      "215\n",
      "223\n",
      "228\n",
      "286\n",
      "295\n",
      "301\n",
      "302\n",
      "304\n",
      "307\n",
      "311\n",
      "324\n",
      "332\n",
      "343\n",
      "344\n",
      "345\n",
      "351\n",
      "356\n",
      "390\n",
      "406\n",
      "421\n",
      "431\n",
      "487\n",
      "490\n",
      "495\n"
     ]
    }
   ],
   "source": [
    "rnd_st = []\n",
    "for i in range (501):\n",
    "    et = ExtraTreesClassifier(n_estimators=100,n_jobs=10,random_state=i ,)\n",
    "    et.fit(scaled_data_train,train['label'])\n",
    "    y_pred=et.predict(scaled_data_test)\n",
    "    if ((classification_report(test['label'],y_pred,output_dict=True)['0']['recall'])>.60 and (classification_report(test['label'],y_pred,output_dict=True)['2']['recall'])>.60 and (classification_report(test['label'],y_pred,output_dict=True)['3']['recall'])>.60 and (classification_report(test['label'],y_pred,output_dict=True)['accuracy'])>.74):\n",
    "        print(i)\n",
    "        rnd_st.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = []\n",
    "for st in rnd_st:\n",
    "    clf.append(ExtraTreesClassifier(n_estimators=100,n_jobs=10,random_state=st))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.67      0.72       147\n",
      "           1       0.91      0.92      0.91       161\n",
      "           2       0.66      0.71      0.69       146\n",
      "           3       0.72      0.77      0.75       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.77      0.77      0.77       603\n",
      "\n",
      "5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.68      0.74       147\n",
      "           1       0.90      0.91      0.91       161\n",
      "           2       0.67      0.72      0.70       146\n",
      "           3       0.71      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "15\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.90      0.93      0.91       161\n",
      "           2       0.67      0.71      0.69       146\n",
      "           3       0.72      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "16\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.67      0.73       147\n",
      "           1       0.91      0.93      0.92       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.72      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "25\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.90      0.91      0.91       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.72      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "27\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.67      0.72       147\n",
      "           1       0.91      0.92      0.92       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.73      0.77      0.75       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.77      0.77      0.77       603\n",
      "\n",
      "32\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.66      0.73       147\n",
      "           1       0.91      0.93      0.92       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.72      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "36\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.91      0.93      0.92       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.72      0.76      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "65\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.91      0.91      0.91       161\n",
      "           2       0.66      0.73      0.69       146\n",
      "           3       0.73      0.77      0.75       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "70\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.68      0.74       147\n",
      "           1       0.91      0.92      0.92       161\n",
      "           2       0.65      0.71      0.68       146\n",
      "           3       0.72      0.76      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "72\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.91      0.93      0.92       161\n",
      "           2       0.66      0.73      0.69       146\n",
      "           3       0.73      0.77      0.75       149\n",
      "\n",
      "    accuracy                           0.78       603\n",
      "   macro avg       0.78      0.77      0.77       603\n",
      "weighted avg       0.78      0.78      0.78       603\n",
      "\n",
      "73\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.68      0.73       147\n",
      "           1       0.91      0.93      0.92       161\n",
      "           2       0.66      0.71      0.68       146\n",
      "           3       0.72      0.75      0.73       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.77      0.77      0.77       603\n",
      "\n",
      "77\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.67      0.73       147\n",
      "           1       0.91      0.92      0.91       161\n",
      "           2       0.66      0.73      0.69       146\n",
      "           3       0.71      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "85\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.68      0.73       147\n",
      "           1       0.91      0.91      0.91       161\n",
      "           2       0.67      0.72      0.70       146\n",
      "           3       0.72      0.76      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.77      0.77      0.77       603\n",
      "\n",
      "86\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.68      0.74       147\n",
      "           1       0.91      0.93      0.92       161\n",
      "           2       0.67      0.71      0.69       146\n",
      "           3       0.72      0.77      0.75       149\n",
      "\n",
      "    accuracy                           0.78       603\n",
      "   macro avg       0.78      0.77      0.77       603\n",
      "weighted avg       0.78      0.78      0.78       603\n",
      "\n",
      "87\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.67      0.73       147\n",
      "           1       0.91      0.92      0.91       161\n",
      "           2       0.65      0.72      0.68       146\n",
      "           3       0.72      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "95\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.91      0.91      0.91       161\n",
      "           2       0.66      0.73      0.69       146\n",
      "           3       0.72      0.76      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "111\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.68      0.74       147\n",
      "           1       0.90      0.92      0.91       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.72      0.76      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "113\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.67      0.72       147\n",
      "           1       0.91      0.92      0.91       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.73      0.77      0.75       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.77      0.77      0.77       603\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.90      0.92      0.91       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.72      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.77      0.77      0.77       603\n",
      "\n",
      "136\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.91      0.92      0.91       161\n",
      "           2       0.67      0.71      0.69       146\n",
      "           3       0.72      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "140\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.67      0.74       147\n",
      "           1       0.91      0.93      0.92       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.71      0.76      0.73       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "141\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.91      0.93      0.92       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.72      0.76      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.77      0.77      0.77       603\n",
      "\n",
      "144\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.91      0.92      0.91       161\n",
      "           2       0.66      0.73      0.69       146\n",
      "           3       0.72      0.76      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "145\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.67      0.74       147\n",
      "           1       0.91      0.92      0.91       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.71      0.76      0.73       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "150\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.91      0.92      0.91       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.72      0.76      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.77      0.77      0.77       603\n",
      "\n",
      "153\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.65      0.73       147\n",
      "           1       0.90      0.93      0.91       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.71      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "162\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.91      0.92      0.91       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.72      0.76      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.77      0.77      0.77       603\n",
      "\n",
      "172\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.68      0.74       147\n",
      "           1       0.91      0.91      0.91       161\n",
      "           2       0.65      0.73      0.69       146\n",
      "           3       0.72      0.75      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "173\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.66      0.72       147\n",
      "           1       0.91      0.93      0.92       161\n",
      "           2       0.67      0.71      0.69       146\n",
      "           3       0.71      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.77      0.77      0.77       603\n",
      "\n",
      "184\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.68      0.74       147\n",
      "           1       0.90      0.92      0.91       161\n",
      "           2       0.65      0.71      0.68       146\n",
      "           3       0.72      0.76      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.77      0.77      0.77       603\n",
      "\n",
      "187\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.67      0.74       147\n",
      "           1       0.91      0.92      0.91       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.71      0.76      0.73       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "188\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.67      0.74       147\n",
      "           1       0.91      0.92      0.92       161\n",
      "           2       0.65      0.73      0.69       146\n",
      "           3       0.72      0.75      0.73       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "189\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.68      0.74       147\n",
      "           1       0.91      0.92      0.91       161\n",
      "           2       0.67      0.73      0.70       146\n",
      "           3       0.73      0.77      0.75       149\n",
      "\n",
      "    accuracy                           0.78       603\n",
      "   macro avg       0.78      0.77      0.77       603\n",
      "weighted avg       0.78      0.78      0.78       603\n",
      "\n",
      "197\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.91      0.93      0.92       161\n",
      "           2       0.67      0.73      0.70       146\n",
      "           3       0.72      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.78       603\n",
      "   macro avg       0.78      0.77      0.77       603\n",
      "weighted avg       0.78      0.78      0.78       603\n",
      "\n",
      "221\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.90      0.92      0.91       161\n",
      "           2       0.66      0.71      0.68       146\n",
      "           3       0.72      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.77      0.77      0.77       603\n",
      "\n",
      "234\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.91      0.92      0.92       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.72      0.78      0.75       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "235\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.91      0.93      0.92       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.73      0.77      0.75       149\n",
      "\n",
      "    accuracy                           0.78       603\n",
      "   macro avg       0.78      0.77      0.77       603\n",
      "weighted avg       0.78      0.78      0.78       603\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.67      0.74       147\n",
      "           1       0.91      0.92      0.91       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.71      0.76      0.73       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "248\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.67      0.74       147\n",
      "           1       0.91      0.92      0.91       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.72      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "260\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.67      0.74       147\n",
      "           1       0.91      0.93      0.92       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.72      0.76      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "263\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.68      0.74       147\n",
      "           1       0.91      0.92      0.91       161\n",
      "           2       0.65      0.71      0.68       146\n",
      "           3       0.72      0.76      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.77      0.77      0.77       603\n",
      "\n",
      "275\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.67      0.74       147\n",
      "           1       0.91      0.93      0.92       161\n",
      "           2       0.65      0.72      0.68       146\n",
      "           3       0.72      0.76      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.78      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "284\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.68      0.73       147\n",
      "           1       0.91      0.93      0.92       161\n",
      "           2       0.67      0.72      0.69       146\n",
      "           3       0.72      0.76      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "292\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.91      0.91      0.91       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.72      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "295\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.91      0.92      0.91       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.72      0.77      0.75       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "306\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.91      0.92      0.91       161\n",
      "           2       0.66      0.73      0.70       146\n",
      "           3       0.72      0.76      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.78      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "311\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.91      0.91      0.91       161\n",
      "           2       0.67      0.73      0.70       146\n",
      "           3       0.71      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "322\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.90      0.92      0.91       161\n",
      "           2       0.68      0.73      0.70       146\n",
      "           3       0.71      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.69      0.73       147\n",
      "           1       0.91      0.92      0.92       161\n",
      "           2       0.66      0.71      0.68       146\n",
      "           3       0.72      0.76      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.77      0.77      0.77       603\n",
      "\n",
      "338\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.67      0.72       147\n",
      "           1       0.91      0.92      0.91       161\n",
      "           2       0.68      0.71      0.69       146\n",
      "           3       0.72      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.77      0.77      0.77       603\n",
      "\n",
      "341\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.91      0.92      0.92       161\n",
      "           2       0.66      0.71      0.68       146\n",
      "           3       0.71      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "350\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.91      0.92      0.91       161\n",
      "           2       0.67      0.72      0.69       146\n",
      "           3       0.71      0.76      0.73       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.77      0.77      0.77       603\n",
      "\n",
      "361\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.67      0.73       147\n",
      "           1       0.91      0.94      0.92       161\n",
      "           2       0.66      0.71      0.68       146\n",
      "           3       0.72      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "362\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.67      0.74       147\n",
      "           1       0.91      0.91      0.91       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.72      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "366\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.67      0.73       147\n",
      "           1       0.91      0.93      0.92       161\n",
      "           2       0.65      0.72      0.68       146\n",
      "           3       0.72      0.76      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "386\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.91      0.93      0.92       161\n",
      "           2       0.67      0.71      0.69       146\n",
      "           3       0.71      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.67      0.74       147\n",
      "           1       0.91      0.93      0.92       161\n",
      "           2       0.67      0.73      0.70       146\n",
      "           3       0.72      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.78       603\n",
      "   macro avg       0.78      0.77      0.77       603\n",
      "weighted avg       0.78      0.78      0.78       603\n",
      "\n",
      "396\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.68      0.74       147\n",
      "           1       0.91      0.93      0.92       161\n",
      "           2       0.66      0.73      0.69       146\n",
      "           3       0.72      0.75      0.73       149\n",
      "\n",
      "    accuracy                           0.78       603\n",
      "   macro avg       0.78      0.77      0.77       603\n",
      "weighted avg       0.78      0.78      0.78       603\n",
      "\n",
      "418\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.68      0.74       147\n",
      "           1       0.91      0.93      0.92       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.72      0.76      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.78      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "431\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.68      0.74       147\n",
      "           1       0.91      0.91      0.91       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.72      0.76      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "448\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.67      0.73       147\n",
      "           1       0.90      0.92      0.91       161\n",
      "           2       0.65      0.72      0.68       146\n",
      "           3       0.73      0.77      0.75       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "453\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.91      0.93      0.92       161\n",
      "           2       0.67      0.72      0.69       146\n",
      "           3       0.72      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "456\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.67      0.73       147\n",
      "           1       0.91      0.93      0.92       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.71      0.75      0.73       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.77      0.77      0.77       603\n",
      "\n",
      "458\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.91      0.92      0.91       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.72      0.76      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "461\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.67      0.72       147\n",
      "           1       0.91      0.92      0.91       161\n",
      "           2       0.67      0.72      0.69       146\n",
      "           3       0.72      0.76      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.77      0.77      0.77       603\n",
      "\n",
      "472\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.91      0.92      0.91       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.72      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "504\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.68      0.73       147\n",
      "           1       0.91      0.93      0.92       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.72      0.75      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "509\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.90      0.93      0.91       161\n",
      "           2       0.66      0.71      0.68       146\n",
      "           3       0.73      0.77      0.75       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.77      0.77      0.77       603\n",
      "\n",
      "515\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.67      0.73       147\n",
      "           1       0.91      0.92      0.91       161\n",
      "           2       0.67      0.73      0.70       146\n",
      "           3       0.72      0.75      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.77      0.77      0.77       603\n",
      "\n",
      "517\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.68      0.74       147\n",
      "           1       0.91      0.92      0.92       161\n",
      "           2       0.66      0.73      0.69       146\n",
      "           3       0.72      0.76      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.78      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "524\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.67      0.74       147\n",
      "           1       0.90      0.92      0.91       161\n",
      "           2       0.65      0.71      0.68       146\n",
      "           3       0.72      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "549\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.91      0.94      0.92       161\n",
      "           2       0.67      0.73      0.70       146\n",
      "           3       0.72      0.76      0.74       149\n",
      "\n",
      "    accuracy                           0.78       603\n",
      "   macro avg       0.78      0.77      0.77       603\n",
      "weighted avg       0.78      0.78      0.78       603\n",
      "\n",
      "563\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.91      0.93      0.92       161\n",
      "           2       0.66      0.71      0.68       146\n",
      "           3       0.72      0.76      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "565\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.90      0.92      0.91       161\n",
      "           2       0.67      0.72      0.69       146\n",
      "           3       0.73      0.77      0.75       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "567\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.67      0.73       147\n",
      "           1       0.91      0.93      0.92       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.72      0.76      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "592\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.68      0.74       147\n",
      "           1       0.91      0.93      0.92       161\n",
      "           2       0.66      0.71      0.69       146\n",
      "           3       0.71      0.75      0.73       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "601\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.91      0.92      0.91       161\n",
      "           2       0.67      0.73      0.70       146\n",
      "           3       0.72      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "619\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.67      0.73       147\n",
      "           1       0.90      0.93      0.91       161\n",
      "           2       0.66      0.71      0.68       146\n",
      "           3       0.72      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "624\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.68      0.73       147\n",
      "           1       0.91      0.92      0.92       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.72      0.75      0.73       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.77      0.77      0.77       603\n",
      "\n",
      "628\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.67      0.73       147\n",
      "           1       0.91      0.92      0.91       161\n",
      "           2       0.66      0.73      0.69       146\n",
      "           3       0.71      0.76      0.73       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "630\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.68      0.73       147\n",
      "           1       0.91      0.93      0.92       161\n",
      "           2       0.67      0.71      0.69       146\n",
      "           3       0.72      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "645\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.67      0.73       147\n",
      "           1       0.91      0.93      0.92       161\n",
      "           2       0.67      0.72      0.70       146\n",
      "           3       0.72      0.76      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.77      0.77      0.77       603\n",
      "\n",
      "646\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.90      0.92      0.91       161\n",
      "           2       0.67      0.72      0.70       146\n",
      "           3       0.72      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "647\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.91      0.92      0.92       161\n",
      "           2       0.67      0.71      0.69       146\n",
      "           3       0.71      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.77      0.77      0.77       603\n",
      "\n",
      "667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.67      0.74       147\n",
      "           1       0.90      0.93      0.91       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.72      0.76      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "687\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.66      0.72       147\n",
      "           1       0.90      0.92      0.91       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.72      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.77      0.77      0.77       603\n",
      "\n",
      "694\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.67      0.73       147\n",
      "           1       0.90      0.93      0.92       161\n",
      "           2       0.67      0.71      0.69       146\n",
      "           3       0.72      0.76      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.77      0.77      0.77       603\n",
      "\n",
      "703\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.66      0.73       147\n",
      "           1       0.90      0.93      0.91       161\n",
      "           2       0.66      0.73      0.69       146\n",
      "           3       0.72      0.76      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "706\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.67      0.73       147\n",
      "           1       0.90      0.93      0.91       161\n",
      "           2       0.66      0.71      0.69       146\n",
      "           3       0.72      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "716\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.68      0.74       147\n",
      "           1       0.91      0.93      0.92       161\n",
      "           2       0.66      0.71      0.69       146\n",
      "           3       0.72      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "725\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.91      0.91      0.91       161\n",
      "           2       0.66      0.73      0.69       146\n",
      "           3       0.72      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "730\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.67      0.73       147\n",
      "           1       0.90      0.92      0.91       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.72      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "745\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.68      0.74       147\n",
      "           1       0.91      0.91      0.91       161\n",
      "           2       0.66      0.71      0.69       146\n",
      "           3       0.72      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "746\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.68      0.73       147\n",
      "           1       0.90      0.92      0.91       161\n",
      "           2       0.67      0.71      0.69       146\n",
      "           3       0.72      0.76      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.77      0.77      0.77       603\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.66      0.72       147\n",
      "           1       0.91      0.93      0.92       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.72      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.67      0.73       147\n",
      "           1       0.90      0.92      0.91       161\n",
      "           2       0.66      0.71      0.69       146\n",
      "           3       0.73      0.77      0.75       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.77      0.77      0.77       603\n",
      "\n",
      "788\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.67      0.73       147\n",
      "           1       0.91      0.92      0.91       161\n",
      "           2       0.67      0.72      0.69       146\n",
      "           3       0.72      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "790\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.68      0.73       147\n",
      "           1       0.90      0.92      0.91       161\n",
      "           2       0.66      0.71      0.68       146\n",
      "           3       0.73      0.77      0.75       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.77      0.77      0.77       603\n",
      "\n",
      "791\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.69      0.74       147\n",
      "           1       0.91      0.91      0.91       161\n",
      "           2       0.66      0.71      0.68       146\n",
      "           3       0.72      0.75      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.77      0.77      0.77       603\n",
      "\n",
      "802\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.67      0.74       147\n",
      "           1       0.91      0.93      0.92       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.72      0.76      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "825\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.67      0.73       147\n",
      "           1       0.91      0.91      0.91       161\n",
      "           2       0.65      0.72      0.68       146\n",
      "           3       0.72      0.77      0.75       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "827\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.68      0.74       147\n",
      "           1       0.91      0.92      0.91       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.72      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.78      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "832\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.91      0.93      0.92       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.72      0.76      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.77      0.77      0.77       603\n",
      "\n",
      "857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.91      0.93      0.92       161\n",
      "           2       0.66      0.71      0.68       146\n",
      "           3       0.72      0.77      0.75       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "873\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.67      0.73       147\n",
      "           1       0.91      0.92      0.91       161\n",
      "           2       0.65      0.72      0.68       146\n",
      "           3       0.72      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "893\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.91      0.93      0.92       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.72      0.75      0.73       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "908\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.67      0.72       147\n",
      "           1       0.90      0.93      0.91       161\n",
      "           2       0.67      0.71      0.69       146\n",
      "           3       0.72      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.77      0.77      0.77       603\n",
      "\n",
      "926\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.91      0.92      0.91       161\n",
      "           2       0.65      0.73      0.69       146\n",
      "           3       0.72      0.76      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "964\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.67      0.72       147\n",
      "           1       0.91      0.92      0.92       161\n",
      "           2       0.67      0.73      0.70       146\n",
      "           3       0.72      0.75      0.73       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.77      0.77      0.77       603\n",
      "\n",
      "973\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.67      0.74       147\n",
      "           1       0.91      0.92      0.91       161\n",
      "           2       0.66      0.72      0.69       146\n",
      "           3       0.71      0.76      0.73       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "974\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.68      0.74       147\n",
      "           1       0.90      0.93      0.91       161\n",
      "           2       0.68      0.73      0.70       146\n",
      "           3       0.72      0.76      0.74       149\n",
      "\n",
      "    accuracy                           0.78       603\n",
      "   macro avg       0.78      0.77      0.77       603\n",
      "weighted avg       0.78      0.78      0.78       603\n",
      "\n",
      "978\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       147\n",
      "           1       0.91      0.93      0.92       161\n",
      "           2       0.66      0.73      0.69       146\n",
      "           3       0.73      0.77      0.75       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.78      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n",
      "984\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.67      0.73       147\n",
      "           1       0.91      0.93      0.92       161\n",
      "           2       0.67      0.71      0.69       146\n",
      "           3       0.72      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.78      0.77      0.77       603\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "986\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.67      0.73       147\n",
      "           1       0.91      0.91      0.91       161\n",
      "           2       0.67      0.73      0.70       146\n",
      "           3       0.72      0.77      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.77      0.77      0.77       603\n",
      "\n",
      "998\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.69      0.74       147\n",
      "           1       0.91      0.92      0.92       161\n",
      "           2       0.66      0.71      0.68       146\n",
      "           3       0.72      0.75      0.73       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.77      0.77      0.77       603\n",
      "\n",
      "999\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.67      0.73       147\n",
      "           1       0.90      0.92      0.91       161\n",
      "           2       0.67      0.72      0.69       146\n",
      "           3       0.72      0.76      0.74       149\n",
      "\n",
      "    accuracy                           0.77       603\n",
      "   macro avg       0.77      0.77      0.77       603\n",
      "weighted avg       0.77      0.77      0.77       603\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stacked random state:\n",
    "# 72\n",
    "# 86\n",
    "# 235\n",
    "# 388\n",
    "# 396\n",
    "\n",
    "for i in range (1000):\n",
    "    meta = ExtraTreesClassifier(n_estimators=100,n_jobs=10,random_state=i)\n",
    "    sclf = StackingClassifier(classifiers=clf, meta_classifier=meta)\n",
    "    sclf.fit(scaled_data_train,train['label'])\n",
    "    y_pred_sta=sclf.predict(scaled_data_test)\n",
    "    if (classification_report(test['label'],y_pred_sta,output_dict=True)['accuracy']>.77):\n",
    "        print (i)\n",
    "        print(classification_report(test['label'],y_pred_sta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "72\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.80      0.67      0.73       147\n",
    "           1       0.91      0.93      0.92       161\n",
    "           2       0.66      0.73      0.69       146\n",
    "           3       0.73      0.77      0.75       149\n",
    "\n",
    "    accuracy                           0.78       603\n",
    "   macro avg       0.78      0.77      0.77       603\n",
    "weighted avg       0.78      0.78      0.78       603\n",
    "\n",
    "\n",
    "86\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.81      0.68      0.74       147\n",
    "           1       0.91      0.93      0.92       161\n",
    "           2       0.67      0.71      0.69       146\n",
    "           3       0.72      0.77      0.75       149\n",
    "\n",
    "    accuracy                           0.78       603\n",
    "   macro avg       0.78      0.77      0.77       603\n",
    "weighted avg       0.78      0.78      0.78       603\n",
    "\n",
    "235\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.80      0.67      0.73       147\n",
    "           1       0.91      0.93      0.92       161\n",
    "           2       0.66      0.72      0.69       146\n",
    "           3       0.73      0.77      0.75       149\n",
    "\n",
    "    accuracy                           0.78       603\n",
    "   macro avg       0.78      0.77      0.77       603\n",
    "weighted avg       0.78      0.78      0.78       603\n",
    "\n",
    "388\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.81      0.67      0.74       147\n",
    "           1       0.91      0.93      0.92       161\n",
    "           2       0.67      0.73      0.70       146\n",
    "           3       0.72      0.77      0.74       149\n",
    "\n",
    "    accuracy                           0.78       603\n",
    "   macro avg       0.78      0.77      0.77       603\n",
    "weighted avg       0.78      0.78      0.78       603\n",
    "\n",
    "396\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.82      0.68      0.74       147\n",
    "           1       0.91      0.93      0.92       161\n",
    "           2       0.66      0.73      0.69       146\n",
    "           3       0.72      0.75      0.73       149\n",
    "\n",
    "    accuracy                           0.78       603\n",
    "   macro avg       0.78      0.77      0.77       603\n",
    "weighted avg       0.78      0.78      0.78       603"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
