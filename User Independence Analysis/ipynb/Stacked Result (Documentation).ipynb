{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/sf/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sf/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sf/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sf/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sf/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sf/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/sf/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sf/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sf/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sf/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sf/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sf/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.64 s, sys: 1.06 s, total: 3.7 s\n",
      "Wall time: 27.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.over_sampling import SMOTE,RandomOverSampler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from itertools import combinations \n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn import model_selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['net_acc_mean',\n",
       " 'net_acc_std',\n",
       " 'net_acc_min',\n",
       " 'net_acc_max',\n",
       " 'ACC_x_mean',\n",
       " 'ACC_x_std',\n",
       " 'ACC_x_min',\n",
       " 'ACC_x_max',\n",
       " 'ACC_y_mean',\n",
       " 'ACC_y_std',\n",
       " 'ACC_y_min',\n",
       " 'ACC_y_max',\n",
       " 'ACC_z_mean',\n",
       " 'ACC_z_std',\n",
       " 'ACC_z_min',\n",
       " 'ACC_z_max',\n",
       " 'BVP_mean',\n",
       " 'BVP_std',\n",
       " 'BVP_min',\n",
       " 'BVP_max',\n",
       " 'EDA_mean',\n",
       " 'EDA_std',\n",
       " 'EDA_min',\n",
       " 'EDA_max',\n",
       " 'EDA_phasic_mean',\n",
       " 'EDA_phasic_std',\n",
       " 'EDA_phasic_min',\n",
       " 'EDA_phasic_max',\n",
       " 'EDA_smna_mean',\n",
       " 'EDA_smna_std',\n",
       " 'EDA_smna_min',\n",
       " 'EDA_smna_max',\n",
       " 'EDA_tonic_mean',\n",
       " 'EDA_tonic_std',\n",
       " 'EDA_tonic_min',\n",
       " 'EDA_tonic_max',\n",
       " 'Resp_mean',\n",
       " 'Resp_std',\n",
       " 'Resp_min',\n",
       " 'Resp_max',\n",
       " 'TEMP_mean',\n",
       " 'TEMP_std',\n",
       " 'TEMP_min',\n",
       " 'TEMP_max',\n",
       " 'BVP_peak_freq',\n",
       " 'TEMP_slope',\n",
       " 'subject',\n",
       " 'label']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('60s_window.csv',index_col=0)\n",
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['net_acc_mean',\n",
       " 'net_acc_std',\n",
       " 'net_acc_min',\n",
       " 'net_acc_max',\n",
       " 'ACC_x_mean',\n",
       " 'ACC_x_std',\n",
       " 'ACC_x_min',\n",
       " 'ACC_x_max',\n",
       " 'ACC_y_mean',\n",
       " 'ACC_y_std',\n",
       " 'ACC_y_min',\n",
       " 'ACC_y_max',\n",
       " 'ACC_z_mean',\n",
       " 'ACC_z_std',\n",
       " 'ACC_z_min',\n",
       " 'ACC_z_max',\n",
       " 'BVP_mean',\n",
       " 'BVP_std',\n",
       " 'BVP_min',\n",
       " 'BVP_max',\n",
       " 'ECG_mean',\n",
       " 'ECG_std',\n",
       " 'ECG_min',\n",
       " 'ECG_max',\n",
       " 'EDA_mean',\n",
       " 'EDA_std',\n",
       " 'EDA_min',\n",
       " 'EDA_max',\n",
       " 'EDA_phasic_mean',\n",
       " 'EDA_phasic_std',\n",
       " 'EDA_phasic_min',\n",
       " 'EDA_phasic_max',\n",
       " 'EDA_smna_mean',\n",
       " 'EDA_smna_std',\n",
       " 'EDA_smna_min',\n",
       " 'EDA_smna_max',\n",
       " 'EDA_tonic_mean',\n",
       " 'EDA_tonic_std',\n",
       " 'EDA_tonic_min',\n",
       " 'EDA_tonic_max',\n",
       " 'EMG_mean',\n",
       " 'EMG_std',\n",
       " 'EMG_min',\n",
       " 'EMG_max',\n",
       " 'Resp_mean',\n",
       " 'Resp_std',\n",
       " 'Resp_min',\n",
       " 'Resp_max',\n",
       " 'TEMP_mean',\n",
       " 'TEMP_std',\n",
       " 'TEMP_min',\n",
       " 'TEMP_max',\n",
       " 'c_ACC_x_mean',\n",
       " 'c_ACC_x_std',\n",
       " 'c_ACC_x_min',\n",
       " 'c_ACC_x_max',\n",
       " 'c_ACC_y_mean',\n",
       " 'c_ACC_y_std',\n",
       " 'c_ACC_y_min',\n",
       " 'c_ACC_y_max',\n",
       " 'c_ACC_z_mean',\n",
       " 'c_ACC_z_std',\n",
       " 'c_ACC_z_min',\n",
       " 'c_ACC_z_max',\n",
       " 'c_Temp_mean',\n",
       " 'c_Temp_std',\n",
       " 'c_Temp_min',\n",
       " 'c_Temp_max',\n",
       " 'BVP_peak_freq',\n",
       " 'TEMP_slope',\n",
       " 'subject',\n",
       " 'label']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df=pd.read_csv('60s_window_wrist_chest.csv',index_col=0)\n",
    "# df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>net_acc_mean</th>\n",
       "      <th>net_acc_std</th>\n",
       "      <th>net_acc_min</th>\n",
       "      <th>net_acc_max</th>\n",
       "      <th>ACC_x_mean</th>\n",
       "      <th>ACC_x_std</th>\n",
       "      <th>ACC_x_min</th>\n",
       "      <th>ACC_x_max</th>\n",
       "      <th>ACC_y_mean</th>\n",
       "      <th>ACC_y_std</th>\n",
       "      <th>...</th>\n",
       "      <th>c_ACC_z_min</th>\n",
       "      <th>c_ACC_z_max</th>\n",
       "      <th>c_Temp_mean</th>\n",
       "      <th>c_Temp_std</th>\n",
       "      <th>c_Temp_min</th>\n",
       "      <th>c_Temp_max</th>\n",
       "      <th>BVP_peak_freq</th>\n",
       "      <th>TEMP_slope</th>\n",
       "      <th>subject</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.025961</td>\n",
       "      <td>0.013811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087383</td>\n",
       "      <td>0.023431</td>\n",
       "      <td>0.017769</td>\n",
       "      <td>-0.037843</td>\n",
       "      <td>0.087383</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>1.222594e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.8700</td>\n",
       "      <td>0.6110</td>\n",
       "      <td>29.168923</td>\n",
       "      <td>0.064290</td>\n",
       "      <td>28.994568</td>\n",
       "      <td>29.426208</td>\n",
       "      <td>0.081425</td>\n",
       "      <td>-0.000253</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.027640</td>\n",
       "      <td>0.010597</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>0.054356</td>\n",
       "      <td>0.027640</td>\n",
       "      <td>0.010597</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>0.054356</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>7.290999e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7594</td>\n",
       "      <td>-0.6810</td>\n",
       "      <td>28.886605</td>\n",
       "      <td>0.074846</td>\n",
       "      <td>28.730682</td>\n",
       "      <td>29.207275</td>\n",
       "      <td>0.147017</td>\n",
       "      <td>-0.000161</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.028389</td>\n",
       "      <td>0.006937</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066053</td>\n",
       "      <td>0.028378</td>\n",
       "      <td>0.006985</td>\n",
       "      <td>-0.002752</td>\n",
       "      <td>0.066053</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>4.805734e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7534</td>\n",
       "      <td>-0.6754</td>\n",
       "      <td>28.799659</td>\n",
       "      <td>0.037924</td>\n",
       "      <td>28.679108</td>\n",
       "      <td>28.988800</td>\n",
       "      <td>0.088210</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.033268</td>\n",
       "      <td>0.007670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074998</td>\n",
       "      <td>0.032960</td>\n",
       "      <td>0.008904</td>\n",
       "      <td>-0.030962</td>\n",
       "      <td>0.074998</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>6.126303e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7878</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>28.768865</td>\n",
       "      <td>0.058639</td>\n",
       "      <td>28.584656</td>\n",
       "      <td>29.023285</td>\n",
       "      <td>0.117614</td>\n",
       "      <td>-0.000256</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.037021</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.027522</td>\n",
       "      <td>0.043347</td>\n",
       "      <td>0.037021</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.027522</td>\n",
       "      <td>0.043347</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>8.837530e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7202</td>\n",
       "      <td>-0.6570</td>\n",
       "      <td>28.598514</td>\n",
       "      <td>0.068128</td>\n",
       "      <td>28.447449</td>\n",
       "      <td>28.882599</td>\n",
       "      <td>0.151541</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.036762</td>\n",
       "      <td>0.007911</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058485</td>\n",
       "      <td>-0.036741</td>\n",
       "      <td>0.008011</td>\n",
       "      <td>-0.058485</td>\n",
       "      <td>0.008257</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>5.512148e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.9478</td>\n",
       "      <td>-0.7038</td>\n",
       "      <td>33.943786</td>\n",
       "      <td>0.026213</td>\n",
       "      <td>33.808136</td>\n",
       "      <td>34.097076</td>\n",
       "      <td>0.119876</td>\n",
       "      <td>-0.000075</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>781</td>\n",
       "      <td>0.032120</td>\n",
       "      <td>0.005324</td>\n",
       "      <td>0.001376</td>\n",
       "      <td>0.055732</td>\n",
       "      <td>-0.032117</td>\n",
       "      <td>0.005343</td>\n",
       "      <td>-0.055732</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>3.676049e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.9446</td>\n",
       "      <td>-0.7414</td>\n",
       "      <td>33.939625</td>\n",
       "      <td>0.025553</td>\n",
       "      <td>33.753479</td>\n",
       "      <td>34.144348</td>\n",
       "      <td>0.065592</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>782</td>\n",
       "      <td>0.026901</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.024770</td>\n",
       "      <td>0.028210</td>\n",
       "      <td>-0.026901</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>-0.028210</td>\n",
       "      <td>-0.024770</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>3.554577e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.9718</td>\n",
       "      <td>-0.7698</td>\n",
       "      <td>34.002778</td>\n",
       "      <td>0.034897</td>\n",
       "      <td>33.864288</td>\n",
       "      <td>34.191650</td>\n",
       "      <td>0.108567</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>783</td>\n",
       "      <td>0.027999</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.025458</td>\n",
       "      <td>0.029586</td>\n",
       "      <td>-0.027999</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>-0.029586</td>\n",
       "      <td>-0.025458</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>2.944295e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.9350</td>\n",
       "      <td>-0.8154</td>\n",
       "      <td>34.024391</td>\n",
       "      <td>0.029791</td>\n",
       "      <td>33.917480</td>\n",
       "      <td>34.188568</td>\n",
       "      <td>0.115352</td>\n",
       "      <td>-0.000095</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>784</td>\n",
       "      <td>0.027407</td>\n",
       "      <td>0.005238</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>0.074310</td>\n",
       "      <td>-0.027312</td>\n",
       "      <td>0.005708</td>\n",
       "      <td>-0.074310</td>\n",
       "      <td>0.017201</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>3.927736e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.9224</td>\n",
       "      <td>-0.8070</td>\n",
       "      <td>34.039778</td>\n",
       "      <td>0.025709</td>\n",
       "      <td>33.884033</td>\n",
       "      <td>34.205383</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>-0.000301</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>785 rows × 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            net_acc_mean  net_acc_std  net_acc_min  net_acc_max  ACC_x_mean  \\\n",
       "Unnamed: 0                                                                    \n",
       "0               0.025961     0.013811     0.000000     0.087383    0.023431   \n",
       "1               0.027640     0.010597     0.002752     0.054356    0.027640   \n",
       "2               0.028389     0.006937     0.000000     0.066053    0.028378   \n",
       "3               0.033268     0.007670     0.000000     0.074998    0.032960   \n",
       "4               0.037021     0.001284     0.027522     0.043347    0.037021   \n",
       "...                  ...          ...          ...          ...         ...   \n",
       "780             0.036762     0.007911     0.000000     0.058485   -0.036741   \n",
       "781             0.032120     0.005324     0.001376     0.055732   -0.032117   \n",
       "782             0.026901     0.000517     0.024770     0.028210   -0.026901   \n",
       "783             0.027999     0.000428     0.025458     0.029586   -0.027999   \n",
       "784             0.027407     0.005238     0.000688     0.074310   -0.027312   \n",
       "\n",
       "            ACC_x_std  ACC_x_min  ACC_x_max  ACC_y_mean     ACC_y_std  ...  \\\n",
       "Unnamed: 0                                                             ...   \n",
       "0            0.017769  -0.037843   0.087383    0.000016  1.222594e-05  ...   \n",
       "1            0.010597   0.002752   0.054356    0.000019  7.290999e-06  ...   \n",
       "2            0.006985  -0.002752   0.066053    0.000020  4.805734e-06  ...   \n",
       "3            0.008904  -0.030962   0.074998    0.000023  6.126303e-06  ...   \n",
       "4            0.001284   0.027522   0.043347    0.000025  8.837530e-07  ...   \n",
       "...               ...        ...        ...         ...           ...  ...   \n",
       "780          0.008011  -0.058485   0.008257   -0.000025  5.512148e-06  ...   \n",
       "781          0.005343  -0.055732   0.002752   -0.000022  3.676049e-06  ...   \n",
       "782          0.000517  -0.028210  -0.024770   -0.000019  3.554577e-07  ...   \n",
       "783          0.000428  -0.029586  -0.025458   -0.000019  2.944295e-07  ...   \n",
       "784          0.005708  -0.074310   0.017201   -0.000019  3.927736e-06  ...   \n",
       "\n",
       "            c_ACC_z_min  c_ACC_z_max  c_Temp_mean  c_Temp_std  c_Temp_min  \\\n",
       "Unnamed: 0                                                                  \n",
       "0               -0.8700       0.6110    29.168923    0.064290   28.994568   \n",
       "1               -0.7594      -0.6810    28.886605    0.074846   28.730682   \n",
       "2               -0.7534      -0.6754    28.799659    0.037924   28.679108   \n",
       "3               -0.7878       0.1660    28.768865    0.058639   28.584656   \n",
       "4               -0.7202      -0.6570    28.598514    0.068128   28.447449   \n",
       "...                 ...          ...          ...         ...         ...   \n",
       "780             -0.9478      -0.7038    33.943786    0.026213   33.808136   \n",
       "781             -0.9446      -0.7414    33.939625    0.025553   33.753479   \n",
       "782             -0.9718      -0.7698    34.002778    0.034897   33.864288   \n",
       "783             -0.9350      -0.8154    34.024391    0.029791   33.917480   \n",
       "784             -0.9224      -0.8070    34.039778    0.025709   33.884033   \n",
       "\n",
       "            c_Temp_max  BVP_peak_freq  TEMP_slope  subject  label  \n",
       "Unnamed: 0                                                         \n",
       "0            29.426208       0.081425   -0.000253        2      0  \n",
       "1            29.207275       0.147017   -0.000161        2      0  \n",
       "2            28.988800       0.088210    0.000535        2      0  \n",
       "3            29.023285       0.117614   -0.000256        2      0  \n",
       "4            28.882599       0.151541    0.000260        2      0  \n",
       "...                ...            ...         ...      ...    ...  \n",
       "780          34.097076       0.119876   -0.000075       17      3  \n",
       "781          34.144348       0.065592   -0.000117       17      3  \n",
       "782          34.191650       0.108567    0.000454       17      3  \n",
       "783          34.188568       0.115352   -0.000095       17      3  \n",
       "784          34.205383       0.115385   -0.000301       17      3  \n",
       "\n",
       "[785 rows x 72 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df=pd.read_csv('60s_window_wrist_chest.csv',index_col=0)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features=df.columns.tolist()\n",
    "features\n",
    "\n",
    "removed = ['label']\n",
    "for rem in removed:\n",
    "    features.remove(rem)\n",
    "\n",
    "features_with_sub=[]\n",
    "features_with_sub[:]=features\n",
    "removed = ['subject']\n",
    "for rem in removed:\n",
    "    features.remove(rem)\n",
    "\n",
    "feature=features\n",
    "print(len(feature))\n",
    "len(features_with_sub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>net_acc_mean</th>\n",
       "      <th>net_acc_std</th>\n",
       "      <th>net_acc_min</th>\n",
       "      <th>net_acc_max</th>\n",
       "      <th>ACC_x_mean</th>\n",
       "      <th>ACC_x_std</th>\n",
       "      <th>ACC_x_min</th>\n",
       "      <th>ACC_x_max</th>\n",
       "      <th>ACC_y_mean</th>\n",
       "      <th>ACC_y_std</th>\n",
       "      <th>...</th>\n",
       "      <th>Resp_min</th>\n",
       "      <th>Resp_max</th>\n",
       "      <th>TEMP_mean</th>\n",
       "      <th>TEMP_std</th>\n",
       "      <th>TEMP_min</th>\n",
       "      <th>TEMP_max</th>\n",
       "      <th>BVP_peak_freq</th>\n",
       "      <th>TEMP_slope</th>\n",
       "      <th>subject</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.025961</td>\n",
       "      <td>0.013811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087383</td>\n",
       "      <td>0.023431</td>\n",
       "      <td>0.017769</td>\n",
       "      <td>-0.037843</td>\n",
       "      <td>0.087383</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>1.222594e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.805847</td>\n",
       "      <td>6.742859</td>\n",
       "      <td>35.807285</td>\n",
       "      <td>0.024986</td>\n",
       "      <td>35.750000</td>\n",
       "      <td>35.870000</td>\n",
       "      <td>0.081425</td>\n",
       "      <td>-0.000253</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.027640</td>\n",
       "      <td>0.010597</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>0.054356</td>\n",
       "      <td>0.027640</td>\n",
       "      <td>0.010597</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>0.054356</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>7.290999e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.914429</td>\n",
       "      <td>3.730774</td>\n",
       "      <td>35.706833</td>\n",
       "      <td>0.024641</td>\n",
       "      <td>35.660000</td>\n",
       "      <td>35.750000</td>\n",
       "      <td>0.147017</td>\n",
       "      <td>-0.000161</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.028389</td>\n",
       "      <td>0.006937</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066053</td>\n",
       "      <td>0.028378</td>\n",
       "      <td>0.006985</td>\n",
       "      <td>-0.002752</td>\n",
       "      <td>0.066053</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>4.805734e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.242493</td>\n",
       "      <td>3.450012</td>\n",
       "      <td>35.775430</td>\n",
       "      <td>0.037082</td>\n",
       "      <td>35.710000</td>\n",
       "      <td>35.840000</td>\n",
       "      <td>0.088210</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.033268</td>\n",
       "      <td>0.007670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074998</td>\n",
       "      <td>0.032960</td>\n",
       "      <td>0.008904</td>\n",
       "      <td>-0.030962</td>\n",
       "      <td>0.074998</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>6.126303e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.646729</td>\n",
       "      <td>5.216980</td>\n",
       "      <td>35.830724</td>\n",
       "      <td>0.025266</td>\n",
       "      <td>35.770000</td>\n",
       "      <td>35.890000</td>\n",
       "      <td>0.117614</td>\n",
       "      <td>-0.000256</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.037021</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.027522</td>\n",
       "      <td>0.043347</td>\n",
       "      <td>0.037021</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.027522</td>\n",
       "      <td>0.043347</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>8.837530e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.777100</td>\n",
       "      <td>3.028870</td>\n",
       "      <td>35.798869</td>\n",
       "      <td>0.020909</td>\n",
       "      <td>35.770000</td>\n",
       "      <td>35.840000</td>\n",
       "      <td>0.151541</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1243</td>\n",
       "      <td>0.042130</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.040968</td>\n",
       "      <td>0.043696</td>\n",
       "      <td>-0.039802</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>-0.041346</td>\n",
       "      <td>-0.038618</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>2.254555e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.510088</td>\n",
       "      <td>8.729651</td>\n",
       "      <td>31.505537</td>\n",
       "      <td>0.015360</td>\n",
       "      <td>31.483510</td>\n",
       "      <td>31.544919</td>\n",
       "      <td>0.164873</td>\n",
       "      <td>-0.000189</td>\n",
       "      <td>11.176097</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1244</td>\n",
       "      <td>0.039764</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.039219</td>\n",
       "      <td>0.039907</td>\n",
       "      <td>0.039764</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.039219</td>\n",
       "      <td>0.039907</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>1.862190e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.202333</td>\n",
       "      <td>7.398872</td>\n",
       "      <td>33.696923</td>\n",
       "      <td>0.019939</td>\n",
       "      <td>33.660000</td>\n",
       "      <td>33.741212</td>\n",
       "      <td>0.131407</td>\n",
       "      <td>-0.000133</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1245</td>\n",
       "      <td>0.041990</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.039308</td>\n",
       "      <td>0.043921</td>\n",
       "      <td>0.003807</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>0.006112</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1.590967e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.508980</td>\n",
       "      <td>6.769601</td>\n",
       "      <td>33.929413</td>\n",
       "      <td>0.014302</td>\n",
       "      <td>33.889531</td>\n",
       "      <td>33.958050</td>\n",
       "      <td>0.142617</td>\n",
       "      <td>-0.000093</td>\n",
       "      <td>9.166227</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1246</td>\n",
       "      <td>0.040894</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.040569</td>\n",
       "      <td>0.041733</td>\n",
       "      <td>0.040894</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.040569</td>\n",
       "      <td>0.041733</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>9.076908e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.100251</td>\n",
       "      <td>8.296914</td>\n",
       "      <td>32.774272</td>\n",
       "      <td>0.021280</td>\n",
       "      <td>32.740925</td>\n",
       "      <td>32.847081</td>\n",
       "      <td>0.137624</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>5.346098</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1247</td>\n",
       "      <td>0.021020</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.020642</td>\n",
       "      <td>0.022018</td>\n",
       "      <td>0.021020</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.020642</td>\n",
       "      <td>0.022018</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>1.957902e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.790579</td>\n",
       "      <td>6.949140</td>\n",
       "      <td>31.249947</td>\n",
       "      <td>0.016483</td>\n",
       "      <td>31.222351</td>\n",
       "      <td>31.291567</td>\n",
       "      <td>0.147902</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1248 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      net_acc_mean  net_acc_std  net_acc_min  net_acc_max  ACC_x_mean  \\\n",
       "0         0.025961     0.013811     0.000000     0.087383    0.023431   \n",
       "1         0.027640     0.010597     0.002752     0.054356    0.027640   \n",
       "2         0.028389     0.006937     0.000000     0.066053    0.028378   \n",
       "3         0.033268     0.007670     0.000000     0.074998    0.032960   \n",
       "4         0.037021     0.001284     0.027522     0.043347    0.037021   \n",
       "...            ...          ...          ...          ...         ...   \n",
       "1243      0.042130     0.000328     0.040968     0.043696   -0.039802   \n",
       "1244      0.039764     0.000271     0.039219     0.039907    0.039764   \n",
       "1245      0.041990     0.000231     0.039308     0.043921    0.003807   \n",
       "1246      0.040894     0.000132     0.040569     0.041733    0.040894   \n",
       "1247      0.021020     0.000285     0.020642     0.022018    0.021020   \n",
       "\n",
       "      ACC_x_std  ACC_x_min  ACC_x_max  ACC_y_mean     ACC_y_std  ...  \\\n",
       "0      0.017769  -0.037843   0.087383    0.000016  1.222594e-05  ...   \n",
       "1      0.010597   0.002752   0.054356    0.000019  7.290999e-06  ...   \n",
       "2      0.006985  -0.002752   0.066053    0.000020  4.805734e-06  ...   \n",
       "3      0.008904  -0.030962   0.074998    0.000023  6.126303e-06  ...   \n",
       "4      0.001284   0.027522   0.043347    0.000025  8.837530e-07  ...   \n",
       "...         ...        ...        ...         ...           ...  ...   \n",
       "1243   0.000328  -0.041346  -0.038618   -0.000027  2.254555e-07  ...   \n",
       "1244   0.000271   0.039219   0.039907    0.000027  1.862190e-07  ...   \n",
       "1245   0.000231   0.001499   0.006112    0.000003  1.590967e-07  ...   \n",
       "1246   0.000132   0.040569   0.041733    0.000028  9.076908e-08  ...   \n",
       "1247   0.000285   0.020642   0.022018    0.000014  1.957902e-07  ...   \n",
       "\n",
       "       Resp_min  Resp_max  TEMP_mean  TEMP_std   TEMP_min   TEMP_max  \\\n",
       "0     -8.805847  6.742859  35.807285  0.024986  35.750000  35.870000   \n",
       "1     -2.914429  3.730774  35.706833  0.024641  35.660000  35.750000   \n",
       "2     -3.242493  3.450012  35.775430  0.037082  35.710000  35.840000   \n",
       "3     -6.646729  5.216980  35.830724  0.025266  35.770000  35.890000   \n",
       "4     -2.777100  3.028870  35.798869  0.020909  35.770000  35.840000   \n",
       "...         ...       ...        ...       ...        ...        ...   \n",
       "1243 -10.510088  8.729651  31.505537  0.015360  31.483510  31.544919   \n",
       "1244  -9.202333  7.398872  33.696923  0.019939  33.660000  33.741212   \n",
       "1245  -8.508980  6.769601  33.929413  0.014302  33.889531  33.958050   \n",
       "1246  -8.100251  8.296914  32.774272  0.021280  32.740925  32.847081   \n",
       "1247  -7.790579  6.949140  31.249947  0.016483  31.222351  31.291567   \n",
       "\n",
       "      BVP_peak_freq  TEMP_slope    subject  label  \n",
       "0          0.081425   -0.000253   2.000000      0  \n",
       "1          0.147017   -0.000161   2.000000      0  \n",
       "2          0.088210    0.000535   2.000000      0  \n",
       "3          0.117614   -0.000256   2.000000      0  \n",
       "4          0.151541    0.000260   2.000000      0  \n",
       "...             ...         ...        ...    ...  \n",
       "1243       0.164873   -0.000189  11.176097      3  \n",
       "1244       0.131407   -0.000133  13.000000      3  \n",
       "1245       0.142617   -0.000093   9.166227      3  \n",
       "1246       0.137624    0.000187   5.346098      3  \n",
       "1247       0.147902   -0.000013   9.000000      3  \n",
       "\n",
       "[1248 rows x 48 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm = SMOTE(random_state=2)\n",
    "X, y= sm.fit_sample(df[features_with_sub], df['label'])\n",
    "df_new=pd.concat([pd.DataFrame(X,columns=features_with_sub),pd.DataFrame(y,columns=['label'])],axis=1)\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i in range (len(list(df_new['subject']))):\n",
    "    df_new['subject'][i] = min([2,3,4,5,6,7,8,9,10,11,13,14,15,16,17], key=lambda x:abs(x-df_new['subject'][i]))\n",
    "df_new['subject']=df_new['subject'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_d=pd.read_csv('personal_detail.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>net_acc_mean</th>\n",
       "      <th>net_acc_std</th>\n",
       "      <th>net_acc_min</th>\n",
       "      <th>net_acc_max</th>\n",
       "      <th>ACC_x_mean</th>\n",
       "      <th>ACC_x_std</th>\n",
       "      <th>ACC_x_min</th>\n",
       "      <th>ACC_x_max</th>\n",
       "      <th>ACC_y_mean</th>\n",
       "      <th>ACC_y_std</th>\n",
       "      <th>...</th>\n",
       "      <th>label</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>gender_ female</th>\n",
       "      <th>coffee_today_YES</th>\n",
       "      <th>sport_today_YES</th>\n",
       "      <th>smoker_YES</th>\n",
       "      <th>feel_ill_today_YES</th>\n",
       "      <th>bmi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.025961</td>\n",
       "      <td>0.013811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087383</td>\n",
       "      <td>0.023431</td>\n",
       "      <td>0.017769</td>\n",
       "      <td>-0.037843</td>\n",
       "      <td>0.087383</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>1.222594e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>175</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.027640</td>\n",
       "      <td>0.010597</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>0.054356</td>\n",
       "      <td>0.027640</td>\n",
       "      <td>0.010597</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>0.054356</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>7.290999e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>175</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.028389</td>\n",
       "      <td>0.006937</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066053</td>\n",
       "      <td>0.028378</td>\n",
       "      <td>0.006985</td>\n",
       "      <td>-0.002752</td>\n",
       "      <td>0.066053</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>4.805734e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>175</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.033268</td>\n",
       "      <td>0.007670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074998</td>\n",
       "      <td>0.032960</td>\n",
       "      <td>0.008904</td>\n",
       "      <td>-0.030962</td>\n",
       "      <td>0.074998</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>6.126303e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>175</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.037021</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.027522</td>\n",
       "      <td>0.043347</td>\n",
       "      <td>0.037021</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.027522</td>\n",
       "      <td>0.043347</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>8.837530e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>175</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1243</td>\n",
       "      <td>0.029484</td>\n",
       "      <td>0.002074</td>\n",
       "      <td>0.020918</td>\n",
       "      <td>0.053804</td>\n",
       "      <td>-0.004624</td>\n",
       "      <td>0.002074</td>\n",
       "      <td>-0.015439</td>\n",
       "      <td>0.017447</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>1.427082e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>165</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1244</td>\n",
       "      <td>0.032744</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.029211</td>\n",
       "      <td>0.034857</td>\n",
       "      <td>-0.029334</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>-0.031478</td>\n",
       "      <td>-0.025832</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>3.552867e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>165</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1245</td>\n",
       "      <td>0.030006</td>\n",
       "      <td>0.007051</td>\n",
       "      <td>0.002966</td>\n",
       "      <td>0.070357</td>\n",
       "      <td>-0.027424</td>\n",
       "      <td>0.007051</td>\n",
       "      <td>-0.067796</td>\n",
       "      <td>-0.000404</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>4.851210e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>165</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1246</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>0.027188</td>\n",
       "      <td>0.038575</td>\n",
       "      <td>-0.031250</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>-0.038575</td>\n",
       "      <td>-0.027188</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>1.055452e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>165</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1247</td>\n",
       "      <td>0.035683</td>\n",
       "      <td>0.010608</td>\n",
       "      <td>0.004897</td>\n",
       "      <td>0.063659</td>\n",
       "      <td>-0.030181</td>\n",
       "      <td>0.010608</td>\n",
       "      <td>-0.058393</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>7.299080e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "      <td>165</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1248 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      net_acc_mean  net_acc_std  net_acc_min  net_acc_max  ACC_x_mean  \\\n",
       "0         0.025961     0.013811     0.000000     0.087383    0.023431   \n",
       "1         0.027640     0.010597     0.002752     0.054356    0.027640   \n",
       "2         0.028389     0.006937     0.000000     0.066053    0.028378   \n",
       "3         0.033268     0.007670     0.000000     0.074998    0.032960   \n",
       "4         0.037021     0.001284     0.027522     0.043347    0.037021   \n",
       "...            ...          ...          ...          ...         ...   \n",
       "1243      0.029484     0.002074     0.020918     0.053804   -0.004624   \n",
       "1244      0.032744     0.000516     0.029211     0.034857   -0.029334   \n",
       "1245      0.030006     0.007051     0.002966     0.070357   -0.027424   \n",
       "1246      0.031250     0.001534     0.027188     0.038575   -0.031250   \n",
       "1247      0.035683     0.010608     0.004897     0.063659   -0.030181   \n",
       "\n",
       "      ACC_x_std  ACC_x_min  ACC_x_max  ACC_y_mean     ACC_y_std  ...  label  \\\n",
       "0      0.017769  -0.037843   0.087383    0.000016  1.222594e-05  ...      0   \n",
       "1      0.010597   0.002752   0.054356    0.000019  7.290999e-06  ...      0   \n",
       "2      0.006985  -0.002752   0.066053    0.000020  4.805734e-06  ...      0   \n",
       "3      0.008904  -0.030962   0.074998    0.000023  6.126303e-06  ...      0   \n",
       "4      0.001284   0.027522   0.043347    0.000025  8.837530e-07  ...      0   \n",
       "...         ...        ...        ...         ...           ...  ...    ...   \n",
       "1243   0.002074  -0.015439   0.017447   -0.000003  1.427082e-06  ...      1   \n",
       "1244   0.000516  -0.031478  -0.025832   -0.000020  3.552867e-07  ...      2   \n",
       "1245   0.007051  -0.067796  -0.000404   -0.000019  4.851210e-06  ...      2   \n",
       "1246   0.001534  -0.038575  -0.027188   -0.000022  1.055452e-06  ...      2   \n",
       "1247   0.010608  -0.058393   0.000368   -0.000021  7.299080e-06  ...      3   \n",
       "\n",
       "      age  height  weight  gender_ female  coffee_today_YES  sport_today_YES  \\\n",
       "0      27     175      80               0                 0                0   \n",
       "1      27     175      80               0                 0                0   \n",
       "2      27     175      80               0                 0                0   \n",
       "3      27     175      80               0                 0                0   \n",
       "4      27     175      80               0                 0                0   \n",
       "...   ...     ...     ...             ...               ...              ...   \n",
       "1243   29     165      55               1                 0                0   \n",
       "1244   29     165      55               1                 0                0   \n",
       "1245   29     165      55               1                 0                0   \n",
       "1246   29     165      55               1                 0                0   \n",
       "1247   29     165      55               1                 0                0   \n",
       "\n",
       "      smoker_YES  feel_ill_today_YES  bmi  \n",
       "0              0                   0    1  \n",
       "1              0                   0    1  \n",
       "2              0                   0    1  \n",
       "3              0                   0    1  \n",
       "4              0                   0    1  \n",
       "...          ...                 ...  ...  \n",
       "1243           0                   0    0  \n",
       "1244           0                   0    0  \n",
       "1245           0                   0    0  \n",
       "1246           0                   0    0  \n",
       "1247           0                   0    0  \n",
       "\n",
       "[1248 rows x 57 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new_1=df_new.merge(p_d,on='subject')\n",
    "df_new_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    312\n",
       "2    312\n",
       "1    312\n",
       "0    312\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new_1['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features=df_new_1.columns.tolist()\n",
    "features\n",
    "\n",
    "removed = ['label']\n",
    "for rem in removed:\n",
    "    features.remove(rem)\n",
    "features_with_sub=[]\n",
    "features_with_sub[:]=features\n",
    "removed = ['subject']\n",
    "for rem in removed:\n",
    "    features.remove(rem)\n",
    "\n",
    "feature=features\n",
    "print(len(feature))\n",
    "len(features_with_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=df_new_1[df_new_1['subject']<=9]\n",
    "test=df_new_1[df_new_1['subject']>9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = Normalizer()\n",
    "scaled_data_train = scaler.fit_transform(train[feature])\n",
    "scaled_data_test = scaler.transform(test[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "et = ExtraTreesClassifier(n_estimators=100,n_jobs=10,random_state=56)\n",
    "et.fit(scaled_data_train,train['label'])\n",
    "y_pred=et.predict(scaled_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.65      0.70       147\n",
      "           1       0.89      0.90      0.90       161\n",
      "           2       0.55      0.59      0.57       147\n",
      "           3       0.71      0.75      0.73       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.72      0.73       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test['label'],y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RFECV(cv=StratifiedKFold(n_splits=10, random_state=None, shuffle=False),\n",
       "      estimator=ExtraTreesClassifier(bootstrap=False, class_weight=None,\n",
       "                                     criterion='gini', max_depth=None,\n",
       "                                     max_features='auto', max_leaf_nodes=None,\n",
       "                                     min_impurity_decrease=0.0,\n",
       "                                     min_impurity_split=None,\n",
       "                                     min_samples_leaf=1, min_samples_split=2,\n",
       "                                     min_weight_fraction_leaf=0.0,\n",
       "                                     n_estimators=100, n_jobs=10,\n",
       "                                     oob_score=False, random_state=56,\n",
       "                                     verbose=0, warm_start=False),\n",
       "      min_features_to_select=1, n_jobs=None, scoring='accuracy', step=1,\n",
       "      verbose=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfecv = RFECV(estimator=et, step=1, cv=StratifiedKFold(10), scoring='accuracy')\n",
    "rfecv.fit(scaled_data_train,train['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features: 26\n"
     ]
    }
   ],
   "source": [
    "print('Optimal number of features: {}'.format(rfecv.n_features_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16, 9))\n",
    "plt.title('Recursive Feature Elimination with Cross-Validation', fontsize=18, fontweight='bold', pad=20)\n",
    "plt.xlabel('Number of features selected', fontsize=14, labelpad=20)\n",
    "plt.ylabel('% Correct Classification', fontsize=14, labelpad=20)\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_, color='#303F9F', linewidth=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  6  8  9 10 11 12 13 14 15 16 19 21 23 26 30 33 36 39 40 42 45 48 49\n",
      " 50 51 52 53 54]\n"
     ]
    }
   ],
   "source": [
    "print(np.where(rfecv.support_ == False)[0])\n",
    "\n",
    "X = pd.DataFrame(data=scaled_data_train)\n",
    "X             \n",
    "X_test =  pd.DataFrame(data=scaled_data_test)\n",
    "\n",
    "X.drop(X.columns[np.where(rfecv.support_ == False)[0]], axis=1, inplace=True)\n",
    "X_test.drop(X_test.columns[np.where(rfecv.support_ == False)[0]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "et = ExtraTreesClassifier(n_estimators=100,n_jobs=10,random_state=56)\n",
    "et.fit(X,train['label'])\n",
    "y_pred=et.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.58      0.69       147\n",
      "           1       0.76      0.93      0.83       161\n",
      "           2       0.52      0.65      0.58       147\n",
      "           3       0.69      0.57      0.62       150\n",
      "\n",
      "    accuracy                           0.69       605\n",
      "   macro avg       0.70      0.68      0.68       605\n",
      "weighted avg       0.70      0.69      0.68       605\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test['label'],y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.64      0.71       147\n",
      "           1       0.74      0.91      0.82       161\n",
      "           2       0.57      0.62      0.59       147\n",
      "           3       0.70      0.61      0.65       150\n",
      "\n",
      "    accuracy                           0.70       605\n",
      "   macro avg       0.70      0.69      0.69       605\n",
      "weighted avg       0.70      0.70      0.70       605\n",
      "\n",
      "13\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.62      0.72       147\n",
      "           1       0.78      0.93      0.84       161\n",
      "           2       0.53      0.63      0.58       147\n",
      "           3       0.69      0.61      0.65       150\n",
      "\n",
      "    accuracy                           0.70       605\n",
      "   macro avg       0.72      0.70      0.70       605\n",
      "weighted avg       0.72      0.70      0.70       605\n",
      "\n",
      "14\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.61      0.69       147\n",
      "           1       0.78      0.91      0.84       161\n",
      "           2       0.55      0.65      0.59       147\n",
      "           3       0.75      0.67      0.70       150\n",
      "\n",
      "    accuracy                           0.71       605\n",
      "   macro avg       0.72      0.71      0.71       605\n",
      "weighted avg       0.72      0.71      0.71       605\n",
      "\n",
      "27\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.63      0.72       147\n",
      "           1       0.80      0.90      0.85       161\n",
      "           2       0.52      0.63      0.57       147\n",
      "           3       0.69      0.61      0.65       150\n",
      "\n",
      "    accuracy                           0.70       605\n",
      "   macro avg       0.71      0.69      0.70       605\n",
      "weighted avg       0.71      0.70      0.70       605\n",
      "\n",
      "29\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.61      0.69       147\n",
      "           1       0.84      0.91      0.87       161\n",
      "           2       0.54      0.70      0.61       147\n",
      "           3       0.76      0.65      0.70       150\n",
      "\n",
      "    accuracy                           0.72       605\n",
      "   macro avg       0.74      0.72      0.72       605\n",
      "weighted avg       0.74      0.72      0.72       605\n",
      "\n",
      "33\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.61      0.71       147\n",
      "           1       0.76      0.93      0.84       161\n",
      "           2       0.54      0.62      0.58       147\n",
      "           3       0.72      0.63      0.67       150\n",
      "\n",
      "    accuracy                           0.70       605\n",
      "   macro avg       0.71      0.70      0.70       605\n",
      "weighted avg       0.71      0.70      0.70       605\n",
      "\n",
      "45\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.65      0.73       147\n",
      "           1       0.81      0.90      0.85       161\n",
      "           2       0.53      0.67      0.59       147\n",
      "           3       0.75      0.64      0.69       150\n",
      "\n",
      "    accuracy                           0.72       605\n",
      "   macro avg       0.74      0.72      0.72       605\n",
      "weighted avg       0.74      0.72      0.72       605\n",
      "\n",
      "54\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.66      0.74       147\n",
      "           1       0.78      0.90      0.84       161\n",
      "           2       0.52      0.61      0.56       147\n",
      "           3       0.68      0.61      0.64       150\n",
      "\n",
      "    accuracy                           0.70       605\n",
      "   macro avg       0.71      0.69      0.69       605\n",
      "weighted avg       0.71      0.70      0.70       605\n",
      "\n",
      "60\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.63      0.71       147\n",
      "           1       0.79      0.91      0.85       161\n",
      "           2       0.53      0.64      0.58       147\n",
      "           3       0.73      0.65      0.69       150\n",
      "\n",
      "    accuracy                           0.71       605\n",
      "   macro avg       0.72      0.71      0.71       605\n",
      "weighted avg       0.72      0.71      0.71       605\n",
      "\n",
      "64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.62      0.70       147\n",
      "           1       0.82      0.91      0.86       161\n",
      "           2       0.58      0.69      0.63       147\n",
      "           3       0.75      0.69      0.72       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.74      0.73      0.73       605\n",
      "weighted avg       0.74      0.73      0.73       605\n",
      "\n",
      "73\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.63      0.71       147\n",
      "           1       0.78      0.91      0.84       161\n",
      "           2       0.57      0.65      0.61       147\n",
      "           3       0.73      0.66      0.69       150\n",
      "\n",
      "    accuracy                           0.72       605\n",
      "   macro avg       0.72      0.71      0.71       605\n",
      "weighted avg       0.72      0.72      0.72       605\n",
      "\n",
      "85\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.67      0.76       147\n",
      "           1       0.80      0.92      0.85       161\n",
      "           2       0.52      0.63      0.57       147\n",
      "           3       0.73      0.61      0.66       150\n",
      "\n",
      "    accuracy                           0.71       605\n",
      "   macro avg       0.73      0.71      0.71       605\n",
      "weighted avg       0.73      0.71      0.71       605\n",
      "\n",
      "86\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.64      0.74       147\n",
      "           1       0.79      0.91      0.84       161\n",
      "           2       0.56      0.71      0.62       147\n",
      "           3       0.72      0.61      0.66       150\n",
      "\n",
      "    accuracy                           0.72       605\n",
      "   macro avg       0.74      0.72      0.72       605\n",
      "weighted avg       0.74      0.72      0.72       605\n",
      "\n",
      "88\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.62      0.71       147\n",
      "           1       0.77      0.91      0.84       161\n",
      "           2       0.54      0.66      0.60       147\n",
      "           3       0.73      0.61      0.66       150\n",
      "\n",
      "    accuracy                           0.70       605\n",
      "   macro avg       0.72      0.70      0.70       605\n",
      "weighted avg       0.72      0.70      0.70       605\n",
      "\n",
      "89\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.63      0.72       147\n",
      "           1       0.79      0.94      0.86       161\n",
      "           2       0.56      0.70      0.62       147\n",
      "           3       0.76      0.61      0.68       150\n",
      "\n",
      "    accuracy                           0.72       605\n",
      "   macro avg       0.74      0.72      0.72       605\n",
      "weighted avg       0.74      0.72      0.72       605\n",
      "\n",
      "90\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.64      0.71       147\n",
      "           1       0.72      0.92      0.81       161\n",
      "           2       0.58      0.61      0.59       147\n",
      "           3       0.71      0.61      0.65       150\n",
      "\n",
      "    accuracy                           0.70       605\n",
      "   macro avg       0.70      0.69      0.69       605\n",
      "weighted avg       0.70      0.70      0.69       605\n",
      "\n",
      "92\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.66      0.74       147\n",
      "           1       0.78      0.89      0.83       161\n",
      "           2       0.54      0.67      0.60       147\n",
      "           3       0.76      0.63      0.69       150\n",
      "\n",
      "    accuracy                           0.72       605\n",
      "   macro avg       0.73      0.71      0.71       605\n",
      "weighted avg       0.73      0.72      0.72       605\n",
      "\n",
      "108\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.66      0.75       147\n",
      "           1       0.78      0.94      0.85       161\n",
      "           2       0.57      0.65      0.61       147\n",
      "           3       0.73      0.65      0.69       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.74      0.72      0.72       605\n",
      "weighted avg       0.74      0.73      0.73       605\n",
      "\n",
      "110\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.62      0.71       147\n",
      "           1       0.72      0.90      0.80       161\n",
      "           2       0.56      0.63      0.59       147\n",
      "           3       0.72      0.62      0.66       150\n",
      "\n",
      "    accuracy                           0.70       605\n",
      "   macro avg       0.71      0.69      0.69       605\n",
      "weighted avg       0.71      0.70      0.69       605\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.63      0.72       147\n",
      "           1       0.80      0.93      0.86       161\n",
      "           2       0.59      0.69      0.64       147\n",
      "           3       0.76      0.68      0.72       150\n",
      "\n",
      "    accuracy                           0.74       605\n",
      "   macro avg       0.74      0.73      0.73       605\n",
      "weighted avg       0.75      0.74      0.74       605\n",
      "\n",
      "116\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.61      0.69       147\n",
      "           1       0.79      0.94      0.86       161\n",
      "           2       0.58      0.65      0.61       147\n",
      "           3       0.75      0.69      0.72       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.72      0.72       605\n",
      "weighted avg       0.73      0.73      0.72       605\n",
      "\n",
      "119\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.62      0.70       147\n",
      "           1       0.74      0.93      0.82       161\n",
      "           2       0.56      0.61      0.58       147\n",
      "           3       0.69      0.61      0.65       150\n",
      "\n",
      "    accuracy                           0.69       605\n",
      "   macro avg       0.70      0.69      0.69       605\n",
      "weighted avg       0.70      0.69      0.69       605\n",
      "\n",
      "122\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.65      0.74       147\n",
      "           1       0.81      0.93      0.86       161\n",
      "           2       0.58      0.69      0.63       147\n",
      "           3       0.70      0.63      0.66       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.74      0.72      0.72       605\n",
      "weighted avg       0.74      0.73      0.73       605\n",
      "\n",
      "124\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.64      0.74       147\n",
      "           1       0.78      0.92      0.85       161\n",
      "           2       0.53      0.63      0.57       147\n",
      "           3       0.71      0.65      0.68       150\n",
      "\n",
      "    accuracy                           0.71       605\n",
      "   macro avg       0.73      0.71      0.71       605\n",
      "weighted avg       0.73      0.71      0.71       605\n",
      "\n",
      "135\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.63      0.71       147\n",
      "           1       0.81      0.89      0.85       161\n",
      "           2       0.55      0.67      0.60       147\n",
      "           3       0.70      0.63      0.66       150\n",
      "\n",
      "    accuracy                           0.71       605\n",
      "   macro avg       0.72      0.71      0.71       605\n",
      "weighted avg       0.72      0.71      0.71       605\n",
      "\n",
      "149\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.64      0.72       147\n",
      "           1       0.74      0.92      0.82       161\n",
      "           2       0.57      0.61      0.59       147\n",
      "           3       0.70      0.63      0.66       150\n",
      "\n",
      "    accuracy                           0.70       605\n",
      "   macro avg       0.71      0.70      0.70       605\n",
      "weighted avg       0.71      0.70      0.70       605\n",
      "\n",
      "162\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.66      0.75       147\n",
      "           1       0.71      0.92      0.80       161\n",
      "           2       0.58      0.63      0.60       147\n",
      "           3       0.71      0.61      0.65       150\n",
      "\n",
      "    accuracy                           0.71       605\n",
      "   macro avg       0.72      0.70      0.70       605\n",
      "weighted avg       0.72      0.71      0.71       605\n",
      "\n",
      "182\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.61      0.68       147\n",
      "           1       0.74      0.89      0.81       161\n",
      "           2       0.56      0.63      0.59       147\n",
      "           3       0.72      0.62      0.66       150\n",
      "\n",
      "    accuracy                           0.69       605\n",
      "   macro avg       0.70      0.69      0.69       605\n",
      "weighted avg       0.70      0.69      0.69       605\n",
      "\n",
      "183\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.61      0.69       147\n",
      "           1       0.77      0.91      0.84       161\n",
      "           2       0.55      0.63      0.59       147\n",
      "           3       0.76      0.66      0.70       150\n",
      "\n",
      "    accuracy                           0.71       605\n",
      "   macro avg       0.72      0.70      0.70       605\n",
      "weighted avg       0.72      0.71      0.71       605\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range (201):\n",
    "    #print (i)\n",
    "    et = ExtraTreesClassifier(n_estimators=100,n_jobs=10,random_state=i)\n",
    "    et.fit(X,train['label'])\n",
    "    y_pred=et.predict(X_test)\n",
    "    #print(classification_report(test['label'],y_pred))\n",
    "    if ((classification_report(test['label'],y_pred,output_dict=True)['0']['recall'])>.60 and (classification_report(test['label'],y_pred,output_dict=True)['2']['recall'])>.60 and (classification_report(test['label'],y_pred,output_dict=True)['3']['recall'])>.60):\n",
    "        print(i)\n",
    "        print(classification_report(test['label'],y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STACKED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6859504132231405"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(test['label'],y_pred,output_dict = True)['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "36\n",
      "38\n",
      "42\n",
      "65\n",
      "110\n",
      "119\n",
      "140\n",
      "162\n",
      "195\n",
      "199\n",
      "217\n",
      "226\n",
      "231\n",
      "242\n",
      "275\n",
      "293\n",
      "311\n",
      "312\n",
      "314\n",
      "321\n",
      "359\n",
      "376\n",
      "392\n",
      "395\n",
      "398\n",
      "402\n",
      "409\n",
      "461\n",
      "483\n"
     ]
    }
   ],
   "source": [
    "rnd_st = []\n",
    "for i in range (501):\n",
    "    et = ExtraTreesClassifier(n_estimators=100,n_jobs=10,random_state=i ,)\n",
    "    et.fit(scaled_data_train,train['label'])\n",
    "    y_pred=et.predict(scaled_data_test)\n",
    "    if ((classification_report(test['label'],y_pred,output_dict=True)['0']['recall'])>.60 and (classification_report(test['label'],y_pred,output_dict=True)['2']['recall'])>.60 and (classification_report(test['label'],y_pred,output_dict=True)['3']['recall'])>.60 and (classification_report(test['label'],y_pred,output_dict=True)['accuracy'])>.72):\n",
    "        print(i)\n",
    "        rnd_st.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = []\n",
    "for st in rnd_st:\n",
    "    clf.append(ExtraTreesClassifier(n_estimators=100,n_jobs=10,random_state=st))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.67      0.72       147\n",
      "           1       0.82      0.93      0.87       161\n",
      "           2       0.58      0.61      0.60       147\n",
      "           3       0.73      0.69      0.71       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.66      0.71       147\n",
      "           1       0.83      0.93      0.88       161\n",
      "           2       0.59      0.63      0.61       147\n",
      "           3       0.74      0.71      0.72       150\n",
      "\n",
      "    accuracy                           0.74       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.74      0.74      0.73       605\n",
      "\n",
      "2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.66      0.71       147\n",
      "           1       0.82      0.93      0.87       161\n",
      "           2       0.59      0.61      0.60       147\n",
      "           3       0.73      0.71      0.72       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.73      0.72       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.66      0.72       147\n",
      "           1       0.83      0.93      0.88       161\n",
      "           2       0.59      0.64      0.61       147\n",
      "           3       0.73      0.70      0.72       150\n",
      "\n",
      "    accuracy                           0.74       605\n",
      "   macro avg       0.74      0.73      0.73       605\n",
      "weighted avg       0.74      0.74      0.73       605\n",
      "\n",
      "4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.67      0.72       147\n",
      "           1       0.83      0.93      0.88       161\n",
      "           2       0.58      0.62      0.60       147\n",
      "           3       0.74      0.70      0.72       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.66      0.71       147\n",
      "           1       0.83      0.93      0.88       161\n",
      "           2       0.59      0.63      0.61       147\n",
      "           3       0.74      0.70      0.72       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.65      0.71       147\n",
      "           1       0.83      0.94      0.88       161\n",
      "           2       0.58      0.63      0.60       147\n",
      "           3       0.73      0.69      0.71       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.66      0.71       147\n",
      "           1       0.82      0.93      0.87       161\n",
      "           2       0.59      0.61      0.60       147\n",
      "           3       0.73      0.69      0.71       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.72      0.72       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.67      0.72       147\n",
      "           1       0.81      0.93      0.87       161\n",
      "           2       0.59      0.61      0.60       147\n",
      "           3       0.73      0.70      0.72       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "9\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.66      0.72       147\n",
      "           1       0.83      0.94      0.88       161\n",
      "           2       0.60      0.63      0.61       147\n",
      "           3       0.72      0.70      0.71       150\n",
      "\n",
      "    accuracy                           0.74       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.74      0.74      0.73       605\n",
      "\n",
      "10\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.66      0.71       147\n",
      "           1       0.82      0.93      0.87       161\n",
      "           2       0.58      0.61      0.60       147\n",
      "           3       0.73      0.70      0.72       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.73      0.72       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "11\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.65      0.71       147\n",
      "           1       0.82      0.93      0.87       161\n",
      "           2       0.59      0.63      0.61       147\n",
      "           3       0.74      0.71      0.72       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "12\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.67      0.71       147\n",
      "           1       0.84      0.93      0.88       161\n",
      "           2       0.59      0.63      0.61       147\n",
      "           3       0.73      0.70      0.72       150\n",
      "\n",
      "    accuracy                           0.74       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.74      0.74      0.73       605\n",
      "\n",
      "13\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.67      0.72       147\n",
      "           1       0.83      0.94      0.88       161\n",
      "           2       0.59      0.61      0.60       147\n",
      "           3       0.73      0.70      0.71       150\n",
      "\n",
      "    accuracy                           0.74       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.73      0.74      0.73       605\n",
      "\n",
      "14\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.65      0.70       147\n",
      "           1       0.83      0.93      0.88       161\n",
      "           2       0.57      0.61      0.59       147\n",
      "           3       0.73      0.70      0.72       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.72      0.72      0.72       605\n",
      "weighted avg       0.73      0.73      0.72       605\n",
      "\n",
      "15\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.65      0.70       147\n",
      "           1       0.83      0.93      0.88       161\n",
      "           2       0.58      0.63      0.60       147\n",
      "           3       0.72      0.70      0.71       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.72      0.72       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "16\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.65      0.71       147\n",
      "           1       0.83      0.93      0.88       161\n",
      "           2       0.59      0.63      0.61       147\n",
      "           3       0.73      0.70      0.72       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "17\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.66      0.72       147\n",
      "           1       0.84      0.93      0.88       161\n",
      "           2       0.58      0.62      0.60       147\n",
      "           3       0.74      0.71      0.73       150\n",
      "\n",
      "    accuracy                           0.74       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.74      0.74      0.73       605\n",
      "\n",
      "18\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.65      0.71       147\n",
      "           1       0.82      0.94      0.87       161\n",
      "           2       0.59      0.62      0.60       147\n",
      "           3       0.73      0.70      0.72       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.65      0.70       147\n",
      "           1       0.83      0.93      0.88       161\n",
      "           2       0.57      0.63      0.60       147\n",
      "           3       0.73      0.69      0.71       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.72      0.72       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "20\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.67      0.71       147\n",
      "           1       0.83      0.93      0.87       161\n",
      "           2       0.59      0.61      0.60       147\n",
      "           3       0.73      0.70      0.72       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "21\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.66      0.71       147\n",
      "           1       0.82      0.93      0.87       161\n",
      "           2       0.58      0.61      0.60       147\n",
      "           3       0.73      0.69      0.71       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.72      0.72       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "22\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.67      0.72       147\n",
      "           1       0.84      0.94      0.89       161\n",
      "           2       0.59      0.62      0.60       147\n",
      "           3       0.73      0.69      0.71       150\n",
      "\n",
      "    accuracy                           0.74       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.74      0.74      0.74       605\n",
      "\n",
      "23\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.66      0.72       147\n",
      "           1       0.83      0.94      0.88       161\n",
      "           2       0.59      0.62      0.60       147\n",
      "           3       0.73      0.71      0.72       150\n",
      "\n",
      "    accuracy                           0.74       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.74      0.74      0.73       605\n",
      "\n",
      "24\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.65      0.71       147\n",
      "           1       0.84      0.94      0.89       161\n",
      "           2       0.57      0.61      0.59       147\n",
      "           3       0.73      0.71      0.72       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "25\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.67      0.72       147\n",
      "           1       0.83      0.93      0.88       161\n",
      "           2       0.58      0.61      0.59       147\n",
      "           3       0.72      0.69      0.71       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.72      0.72       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "26\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.66      0.71       147\n",
      "           1       0.82      0.93      0.87       161\n",
      "           2       0.59      0.63      0.61       147\n",
      "           3       0.74      0.70      0.72       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "27\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.67      0.72       147\n",
      "           1       0.82      0.93      0.87       161\n",
      "           2       0.58      0.61      0.59       147\n",
      "           3       0.73      0.69      0.71       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.72      0.72       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "28\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.65      0.72       147\n",
      "           1       0.84      0.94      0.89       161\n",
      "           2       0.58      0.63      0.61       147\n",
      "           3       0.73      0.71      0.72       150\n",
      "\n",
      "    accuracy                           0.74       605\n",
      "   macro avg       0.74      0.73      0.73       605\n",
      "weighted avg       0.74      0.74      0.74       605\n",
      "\n",
      "29\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.65      0.71       147\n",
      "           1       0.82      0.94      0.88       161\n",
      "           2       0.58      0.61      0.60       147\n",
      "           3       0.73      0.70      0.71       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.73      0.72       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "30\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.65      0.71       147\n",
      "           1       0.83      0.94      0.88       161\n",
      "           2       0.58      0.61      0.60       147\n",
      "           3       0.73      0.69      0.71       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.72      0.72       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "31\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.67      0.72       147\n",
      "           1       0.82      0.93      0.87       161\n",
      "           2       0.59      0.62      0.61       147\n",
      "           3       0.74      0.71      0.72       150\n",
      "\n",
      "    accuracy                           0.74       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.74      0.74      0.73       605\n",
      "\n",
      "32\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.67      0.71       147\n",
      "           1       0.83      0.93      0.88       161\n",
      "           2       0.59      0.61      0.60       147\n",
      "           3       0.73      0.70      0.71       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "33\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.66      0.70       147\n",
      "           1       0.83      0.93      0.87       161\n",
      "           2       0.58      0.61      0.60       147\n",
      "           3       0.74      0.69      0.71       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.72      0.72      0.72       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "34\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.67      0.72       147\n",
      "           1       0.83      0.94      0.88       161\n",
      "           2       0.59      0.62      0.60       147\n",
      "           3       0.73      0.69      0.71       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "35\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.65      0.71       147\n",
      "           1       0.81      0.93      0.87       161\n",
      "           2       0.57      0.61      0.59       147\n",
      "           3       0.73      0.69      0.71       150\n",
      "\n",
      "    accuracy                           0.72       605\n",
      "   macro avg       0.72      0.72      0.72       605\n",
      "weighted avg       0.73      0.72      0.72       605\n",
      "\n",
      "36\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.66      0.72       147\n",
      "           1       0.83      0.93      0.88       161\n",
      "           2       0.59      0.63      0.61       147\n",
      "           3       0.73      0.70      0.72       150\n",
      "\n",
      "    accuracy                           0.74       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.74      0.74      0.73       605\n",
      "\n",
      "37\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.66      0.71       147\n",
      "           1       0.82      0.93      0.87       161\n",
      "           2       0.58      0.61      0.60       147\n",
      "           3       0.73      0.69      0.71       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.72      0.72       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.66      0.71       147\n",
      "           1       0.83      0.94      0.88       161\n",
      "           2       0.59      0.62      0.60       147\n",
      "           3       0.74      0.71      0.72       150\n",
      "\n",
      "    accuracy                           0.74       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.74      0.74      0.73       605\n",
      "\n",
      "39\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.67      0.72       147\n",
      "           1       0.82      0.93      0.87       161\n",
      "           2       0.59      0.63      0.61       147\n",
      "           3       0.74      0.69      0.71       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "40\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.65      0.71       147\n",
      "           1       0.82      0.93      0.87       161\n",
      "           2       0.59      0.63      0.61       147\n",
      "           3       0.73      0.69      0.71       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.72      0.72       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "41\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.65      0.71       147\n",
      "           1       0.82      0.93      0.87       161\n",
      "           2       0.59      0.63      0.61       147\n",
      "           3       0.74      0.71      0.72       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "42\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.67      0.72       147\n",
      "           1       0.84      0.93      0.88       161\n",
      "           2       0.59      0.63      0.61       147\n",
      "           3       0.73      0.70      0.72       150\n",
      "\n",
      "    accuracy                           0.74       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.74      0.74      0.73       605\n",
      "\n",
      "43\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.65      0.71       147\n",
      "           1       0.82      0.93      0.87       161\n",
      "           2       0.59      0.61      0.60       147\n",
      "           3       0.72      0.70      0.71       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.72      0.72      0.72       605\n",
      "weighted avg       0.73      0.73      0.72       605\n",
      "\n",
      "44\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.66      0.71       147\n",
      "           1       0.83      0.93      0.88       161\n",
      "           2       0.59      0.63      0.61       147\n",
      "           3       0.74      0.71      0.72       150\n",
      "\n",
      "    accuracy                           0.74       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.74      0.74      0.73       605\n",
      "\n",
      "45\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.65      0.71       147\n",
      "           1       0.83      0.93      0.87       161\n",
      "           2       0.58      0.62      0.60       147\n",
      "           3       0.74      0.71      0.72       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "46\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.67      0.72       147\n",
      "           1       0.83      0.93      0.88       161\n",
      "           2       0.60      0.63      0.61       147\n",
      "           3       0.73      0.71      0.72       150\n",
      "\n",
      "    accuracy                           0.74       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.74      0.74      0.73       605\n",
      "\n",
      "47\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.67      0.71       147\n",
      "           1       0.83      0.93      0.88       161\n",
      "           2       0.58      0.61      0.60       147\n",
      "           3       0.74      0.70      0.72       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "48\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.67      0.72       147\n",
      "           1       0.84      0.93      0.88       161\n",
      "           2       0.59      0.63      0.61       147\n",
      "           3       0.73      0.71      0.72       150\n",
      "\n",
      "    accuracy                           0.74       605\n",
      "   macro avg       0.74      0.73      0.73       605\n",
      "weighted avg       0.74      0.74      0.74       605\n",
      "\n",
      "49\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.67      0.73       147\n",
      "           1       0.83      0.93      0.88       161\n",
      "           2       0.59      0.63      0.61       147\n",
      "           3       0.74      0.70      0.72       150\n",
      "\n",
      "    accuracy                           0.74       605\n",
      "   macro avg       0.74      0.73      0.73       605\n",
      "weighted avg       0.74      0.74      0.74       605\n",
      "\n",
      "50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.65      0.71       147\n",
      "           1       0.84      0.93      0.88       161\n",
      "           2       0.59      0.65      0.62       147\n",
      "           3       0.73      0.70      0.72       150\n",
      "\n",
      "    accuracy                           0.74       605\n",
      "   macro avg       0.74      0.73      0.73       605\n",
      "weighted avg       0.74      0.74      0.73       605\n",
      "\n",
      "51\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.67      0.72       147\n",
      "           1       0.83      0.93      0.87       161\n",
      "           2       0.60      0.63      0.62       147\n",
      "           3       0.73      0.71      0.72       150\n",
      "\n",
      "    accuracy                           0.74       605\n",
      "   macro avg       0.74      0.73      0.73       605\n",
      "weighted avg       0.74      0.74      0.74       605\n",
      "\n",
      "52\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.67      0.72       147\n",
      "           1       0.83      0.93      0.88       161\n",
      "           2       0.59      0.62      0.60       147\n",
      "           3       0.73      0.70      0.72       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "53\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.66      0.71       147\n",
      "           1       0.83      0.93      0.88       161\n",
      "           2       0.60      0.63      0.61       147\n",
      "           3       0.74      0.71      0.73       150\n",
      "\n",
      "    accuracy                           0.74       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.74      0.74      0.74       605\n",
      "\n",
      "54\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.67      0.72       147\n",
      "           1       0.82      0.93      0.87       161\n",
      "           2       0.60      0.63      0.62       147\n",
      "           3       0.74      0.69      0.72       150\n",
      "\n",
      "    accuracy                           0.74       605\n",
      "   macro avg       0.74      0.73      0.73       605\n",
      "weighted avg       0.74      0.74      0.74       605\n",
      "\n",
      "55\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.65      0.71       147\n",
      "           1       0.83      0.93      0.87       161\n",
      "           2       0.59      0.63      0.61       147\n",
      "           3       0.73      0.70      0.72       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "56\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.67      0.72       147\n",
      "           1       0.82      0.93      0.87       161\n",
      "           2       0.59      0.62      0.60       147\n",
      "           3       0.73      0.70      0.71       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.67      0.72       147\n",
      "           1       0.82      0.93      0.87       161\n",
      "           2       0.58      0.61      0.60       147\n",
      "           3       0.73      0.70      0.72       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "58\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.67      0.72       147\n",
      "           1       0.84      0.93      0.88       161\n",
      "           2       0.59      0.63      0.61       147\n",
      "           3       0.74      0.71      0.72       150\n",
      "\n",
      "    accuracy                           0.74       605\n",
      "   macro avg       0.74      0.73      0.73       605\n",
      "weighted avg       0.74      0.74      0.74       605\n",
      "\n",
      "59\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.66      0.72       147\n",
      "           1       0.86      0.93      0.89       161\n",
      "           2       0.58      0.65      0.62       147\n",
      "           3       0.74      0.69      0.71       150\n",
      "\n",
      "    accuracy                           0.74       605\n",
      "   macro avg       0.74      0.73      0.73       605\n",
      "weighted avg       0.74      0.74      0.74       605\n",
      "\n",
      "60\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.66      0.71       147\n",
      "           1       0.83      0.93      0.88       161\n",
      "           2       0.58      0.62      0.60       147\n",
      "           3       0.73      0.70      0.71       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "61\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.67      0.72       147\n",
      "           1       0.83      0.93      0.88       161\n",
      "           2       0.59      0.62      0.60       147\n",
      "           3       0.72      0.70      0.71       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "62\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.67      0.72       147\n",
      "           1       0.82      0.94      0.88       161\n",
      "           2       0.59      0.60      0.60       147\n",
      "           3       0.73      0.71      0.72       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "63\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.65      0.71       147\n",
      "           1       0.83      0.93      0.88       161\n",
      "           2       0.58      0.63      0.60       147\n",
      "           3       0.74      0.71      0.72       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.67      0.72       147\n",
      "           1       0.83      0.93      0.88       161\n",
      "           2       0.60      0.62      0.61       147\n",
      "           3       0.74      0.71      0.73       150\n",
      "\n",
      "    accuracy                           0.74       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.74      0.74      0.74       605\n",
      "\n",
      "65\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.66      0.71       147\n",
      "           1       0.84      0.93      0.88       161\n",
      "           2       0.58      0.62      0.60       147\n",
      "           3       0.73      0.70      0.71       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "66\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.66      0.72       147\n",
      "           1       0.83      0.93      0.88       161\n",
      "           2       0.59      0.63      0.61       147\n",
      "           3       0.73      0.71      0.72       150\n",
      "\n",
      "    accuracy                           0.74       605\n",
      "   macro avg       0.74      0.73      0.73       605\n",
      "weighted avg       0.74      0.74      0.74       605\n",
      "\n",
      "67\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.67      0.72       147\n",
      "           1       0.84      0.93      0.88       161\n",
      "           2       0.59      0.61      0.60       147\n",
      "           3       0.73      0.71      0.72       150\n",
      "\n",
      "    accuracy                           0.74       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.73      0.74      0.73       605\n",
      "\n",
      "68\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.66      0.71       147\n",
      "           1       0.83      0.93      0.87       161\n",
      "           2       0.59      0.62      0.60       147\n",
      "           3       0.72      0.70      0.71       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "69\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.66      0.72       147\n",
      "           1       0.82      0.93      0.87       161\n",
      "           2       0.59      0.63      0.61       147\n",
      "           3       0.74      0.70      0.72       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.74      0.73      0.73       605\n",
      "\n",
      "70\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.66      0.71       147\n",
      "           1       0.82      0.93      0.87       161\n",
      "           2       0.59      0.63      0.61       147\n",
      "           3       0.74      0.70      0.72       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "71\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.67      0.72       147\n",
      "           1       0.83      0.93      0.88       161\n",
      "           2       0.58      0.63      0.61       147\n",
      "           3       0.74      0.69      0.71       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "72\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.66      0.71       147\n",
      "           1       0.83      0.93      0.88       161\n",
      "           2       0.59      0.62      0.60       147\n",
      "           3       0.73      0.70      0.71       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "73\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.67      0.72       147\n",
      "           1       0.84      0.93      0.88       161\n",
      "           2       0.59      0.62      0.60       147\n",
      "           3       0.73      0.69      0.71       150\n",
      "\n",
      "    accuracy                           0.73       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.73      0.73      0.73       605\n",
      "\n",
      "74\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.68      0.73       147\n",
      "           1       0.83      0.93      0.87       161\n",
      "           2       0.60      0.63      0.61       147\n",
      "           3       0.74      0.71      0.72       150\n",
      "\n",
      "    accuracy                           0.74       605\n",
      "   macro avg       0.74      0.73      0.73       605\n",
      "weighted avg       0.74      0.74      0.74       605\n",
      "\n",
      "75\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.66      0.72       147\n",
      "           1       0.84      0.94      0.89       161\n",
      "           2       0.59      0.62      0.60       147\n",
      "           3       0.72      0.70      0.71       150\n",
      "\n",
      "    accuracy                           0.74       605\n",
      "   macro avg       0.73      0.73      0.73       605\n",
      "weighted avg       0.74      0.74      0.73       605\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stacked random state:\n",
    "# 72\n",
    "# 86\n",
    "# 235\n",
    "# 388\n",
    "# 396\n",
    "\n",
    "for i in range (500):\n",
    "    meta = ExtraTreesClassifier(n_estimators=100,n_jobs=10,random_state=i)\n",
    "    sclf = StackingClassifier(classifiers=clf, meta_classifier=meta)\n",
    "    sclf.fit(scaled_data_train,train['label'])\n",
    "    y_pred_sta=sclf.predict(scaled_data_test)\n",
    "    if (classification_report(test['label'],y_pred_sta,output_dict=True)['accuracy']>.72):\n",
    "        print (i)\n",
    "        print(classification_report(test['label'],y_pred_sta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
