{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26 µs, sys: 5 µs, total: 31 µs\n",
      "Wall time: 34.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34 µs, sys: 7 µs, total: 41 µs\n",
      "Wall time: 45.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from itertools import combinations \n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>net_acc_mean</th>\n",
       "      <th>net_acc_std</th>\n",
       "      <th>net_acc_min</th>\n",
       "      <th>net_acc_max</th>\n",
       "      <th>ACC_x_mean</th>\n",
       "      <th>ACC_x_std</th>\n",
       "      <th>ACC_x_min</th>\n",
       "      <th>ACC_x_max</th>\n",
       "      <th>ACC_y_mean</th>\n",
       "      <th>ACC_y_std</th>\n",
       "      <th>...</th>\n",
       "      <th>Resp_min</th>\n",
       "      <th>Resp_max</th>\n",
       "      <th>TEMP_mean</th>\n",
       "      <th>TEMP_std</th>\n",
       "      <th>TEMP_min</th>\n",
       "      <th>TEMP_max</th>\n",
       "      <th>BVP_peak_freq</th>\n",
       "      <th>TEMP_slope</th>\n",
       "      <th>subject</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.025961</td>\n",
       "      <td>0.013811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087383</td>\n",
       "      <td>0.023431</td>\n",
       "      <td>0.017769</td>\n",
       "      <td>-0.037843</td>\n",
       "      <td>0.087383</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>1.222594e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.805847</td>\n",
       "      <td>6.742859</td>\n",
       "      <td>35.807285</td>\n",
       "      <td>0.024986</td>\n",
       "      <td>35.75</td>\n",
       "      <td>35.87</td>\n",
       "      <td>0.081425</td>\n",
       "      <td>-0.000253</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.027640</td>\n",
       "      <td>0.010597</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>0.054356</td>\n",
       "      <td>0.027640</td>\n",
       "      <td>0.010597</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>0.054356</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>7.290999e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.914429</td>\n",
       "      <td>3.730774</td>\n",
       "      <td>35.706833</td>\n",
       "      <td>0.024641</td>\n",
       "      <td>35.66</td>\n",
       "      <td>35.75</td>\n",
       "      <td>0.147017</td>\n",
       "      <td>-0.000161</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.028389</td>\n",
       "      <td>0.006937</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066053</td>\n",
       "      <td>0.028378</td>\n",
       "      <td>0.006985</td>\n",
       "      <td>-0.002752</td>\n",
       "      <td>0.066053</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>4.805734e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.242493</td>\n",
       "      <td>3.450012</td>\n",
       "      <td>35.775430</td>\n",
       "      <td>0.037082</td>\n",
       "      <td>35.71</td>\n",
       "      <td>35.84</td>\n",
       "      <td>0.088210</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.033268</td>\n",
       "      <td>0.007670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074998</td>\n",
       "      <td>0.032960</td>\n",
       "      <td>0.008904</td>\n",
       "      <td>-0.030962</td>\n",
       "      <td>0.074998</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>6.126303e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.646729</td>\n",
       "      <td>5.216980</td>\n",
       "      <td>35.830724</td>\n",
       "      <td>0.025266</td>\n",
       "      <td>35.77</td>\n",
       "      <td>35.89</td>\n",
       "      <td>0.117614</td>\n",
       "      <td>-0.000256</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.037021</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.027522</td>\n",
       "      <td>0.043347</td>\n",
       "      <td>0.037021</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.027522</td>\n",
       "      <td>0.043347</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>8.837530e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.777100</td>\n",
       "      <td>3.028870</td>\n",
       "      <td>35.798869</td>\n",
       "      <td>0.020909</td>\n",
       "      <td>35.77</td>\n",
       "      <td>35.84</td>\n",
       "      <td>0.151541</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.036762</td>\n",
       "      <td>0.007911</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058485</td>\n",
       "      <td>-0.036741</td>\n",
       "      <td>0.008011</td>\n",
       "      <td>-0.058485</td>\n",
       "      <td>0.008257</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>5.512148e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-23.149109</td>\n",
       "      <td>20.591736</td>\n",
       "      <td>32.305747</td>\n",
       "      <td>0.016641</td>\n",
       "      <td>32.27</td>\n",
       "      <td>32.33</td>\n",
       "      <td>0.119876</td>\n",
       "      <td>-0.000075</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>781</td>\n",
       "      <td>0.032120</td>\n",
       "      <td>0.005324</td>\n",
       "      <td>0.001376</td>\n",
       "      <td>0.055732</td>\n",
       "      <td>-0.032117</td>\n",
       "      <td>0.005343</td>\n",
       "      <td>-0.055732</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>3.676049e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-18.075562</td>\n",
       "      <td>22.718811</td>\n",
       "      <td>32.265837</td>\n",
       "      <td>0.015829</td>\n",
       "      <td>32.23</td>\n",
       "      <td>32.29</td>\n",
       "      <td>0.065592</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>782</td>\n",
       "      <td>0.026901</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.024770</td>\n",
       "      <td>0.028210</td>\n",
       "      <td>-0.026901</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>-0.028210</td>\n",
       "      <td>-0.024770</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>3.554577e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-20.307922</td>\n",
       "      <td>14.802551</td>\n",
       "      <td>32.316878</td>\n",
       "      <td>0.033005</td>\n",
       "      <td>32.23</td>\n",
       "      <td>32.37</td>\n",
       "      <td>0.108567</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>783</td>\n",
       "      <td>0.027999</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.025458</td>\n",
       "      <td>0.029586</td>\n",
       "      <td>-0.027999</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>-0.029586</td>\n",
       "      <td>-0.025458</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>2.944295e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.500671</td>\n",
       "      <td>11.759949</td>\n",
       "      <td>32.343756</td>\n",
       "      <td>0.015542</td>\n",
       "      <td>32.31</td>\n",
       "      <td>32.37</td>\n",
       "      <td>0.115352</td>\n",
       "      <td>-0.000095</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>784</td>\n",
       "      <td>0.027407</td>\n",
       "      <td>0.005238</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>0.074310</td>\n",
       "      <td>-0.027312</td>\n",
       "      <td>0.005708</td>\n",
       "      <td>-0.074310</td>\n",
       "      <td>0.017201</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>3.927736e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.940613</td>\n",
       "      <td>23.454285</td>\n",
       "      <td>32.291810</td>\n",
       "      <td>0.022358</td>\n",
       "      <td>32.25</td>\n",
       "      <td>32.33</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>-0.000301</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>785 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            net_acc_mean  net_acc_std  net_acc_min  net_acc_max  ACC_x_mean  \\\n",
       "Unnamed: 0                                                                    \n",
       "0               0.025961     0.013811     0.000000     0.087383    0.023431   \n",
       "1               0.027640     0.010597     0.002752     0.054356    0.027640   \n",
       "2               0.028389     0.006937     0.000000     0.066053    0.028378   \n",
       "3               0.033268     0.007670     0.000000     0.074998    0.032960   \n",
       "4               0.037021     0.001284     0.027522     0.043347    0.037021   \n",
       "...                  ...          ...          ...          ...         ...   \n",
       "780             0.036762     0.007911     0.000000     0.058485   -0.036741   \n",
       "781             0.032120     0.005324     0.001376     0.055732   -0.032117   \n",
       "782             0.026901     0.000517     0.024770     0.028210   -0.026901   \n",
       "783             0.027999     0.000428     0.025458     0.029586   -0.027999   \n",
       "784             0.027407     0.005238     0.000688     0.074310   -0.027312   \n",
       "\n",
       "            ACC_x_std  ACC_x_min  ACC_x_max  ACC_y_mean     ACC_y_std  ...  \\\n",
       "Unnamed: 0                                                             ...   \n",
       "0            0.017769  -0.037843   0.087383    0.000016  1.222594e-05  ...   \n",
       "1            0.010597   0.002752   0.054356    0.000019  7.290999e-06  ...   \n",
       "2            0.006985  -0.002752   0.066053    0.000020  4.805734e-06  ...   \n",
       "3            0.008904  -0.030962   0.074998    0.000023  6.126303e-06  ...   \n",
       "4            0.001284   0.027522   0.043347    0.000025  8.837530e-07  ...   \n",
       "...               ...        ...        ...         ...           ...  ...   \n",
       "780          0.008011  -0.058485   0.008257   -0.000025  5.512148e-06  ...   \n",
       "781          0.005343  -0.055732   0.002752   -0.000022  3.676049e-06  ...   \n",
       "782          0.000517  -0.028210  -0.024770   -0.000019  3.554577e-07  ...   \n",
       "783          0.000428  -0.029586  -0.025458   -0.000019  2.944295e-07  ...   \n",
       "784          0.005708  -0.074310   0.017201   -0.000019  3.927736e-06  ...   \n",
       "\n",
       "             Resp_min   Resp_max  TEMP_mean  TEMP_std  TEMP_min  TEMP_max  \\\n",
       "Unnamed: 0                                                                  \n",
       "0           -8.805847   6.742859  35.807285  0.024986     35.75     35.87   \n",
       "1           -2.914429   3.730774  35.706833  0.024641     35.66     35.75   \n",
       "2           -3.242493   3.450012  35.775430  0.037082     35.71     35.84   \n",
       "3           -6.646729   5.216980  35.830724  0.025266     35.77     35.89   \n",
       "4           -2.777100   3.028870  35.798869  0.020909     35.77     35.84   \n",
       "...               ...        ...        ...       ...       ...       ...   \n",
       "780        -23.149109  20.591736  32.305747  0.016641     32.27     32.33   \n",
       "781        -18.075562  22.718811  32.265837  0.015829     32.23     32.29   \n",
       "782        -20.307922  14.802551  32.316878  0.033005     32.23     32.37   \n",
       "783         -8.500671  11.759949  32.343756  0.015542     32.31     32.37   \n",
       "784        -21.940613  23.454285  32.291810  0.022358     32.25     32.33   \n",
       "\n",
       "            BVP_peak_freq  TEMP_slope  subject  label  \n",
       "Unnamed: 0                                             \n",
       "0                0.081425   -0.000253        2      0  \n",
       "1                0.147017   -0.000161        2      0  \n",
       "2                0.088210    0.000535        2      0  \n",
       "3                0.117614   -0.000256        2      0  \n",
       "4                0.151541    0.000260        2      0  \n",
       "...                   ...         ...      ...    ...  \n",
       "780              0.119876   -0.000075       17      3  \n",
       "781              0.065592   -0.000117       17      3  \n",
       "782              0.108567    0.000454       17      3  \n",
       "783              0.115352   -0.000095       17      3  \n",
       "784              0.115385   -0.000301       17      3  \n",
       "\n",
       "[785 rows x 48 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('60s_window.csv',index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df['subject'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['net_acc_mean',\n",
       " 'net_acc_std',\n",
       " 'net_acc_min',\n",
       " 'net_acc_max',\n",
       " 'ACC_x_mean',\n",
       " 'ACC_x_std',\n",
       " 'ACC_x_min',\n",
       " 'ACC_x_max',\n",
       " 'ACC_y_mean',\n",
       " 'ACC_y_std',\n",
       " 'ACC_y_min',\n",
       " 'ACC_y_max',\n",
       " 'ACC_z_mean',\n",
       " 'ACC_z_std',\n",
       " 'ACC_z_min',\n",
       " 'ACC_z_max',\n",
       " 'BVP_mean',\n",
       " 'BVP_std',\n",
       " 'BVP_min',\n",
       " 'BVP_max',\n",
       " 'EDA_mean',\n",
       " 'EDA_std',\n",
       " 'EDA_min',\n",
       " 'EDA_max',\n",
       " 'EDA_phasic_mean',\n",
       " 'EDA_phasic_std',\n",
       " 'EDA_phasic_min',\n",
       " 'EDA_phasic_max',\n",
       " 'EDA_smna_mean',\n",
       " 'EDA_smna_std',\n",
       " 'EDA_smna_min',\n",
       " 'EDA_smna_max',\n",
       " 'EDA_tonic_mean',\n",
       " 'EDA_tonic_std',\n",
       " 'EDA_tonic_min',\n",
       " 'EDA_tonic_max',\n",
       " 'Resp_mean',\n",
       " 'Resp_std',\n",
       " 'Resp_min',\n",
       " 'Resp_max',\n",
       " 'TEMP_mean',\n",
       " 'TEMP_std',\n",
       " 'TEMP_min',\n",
       " 'TEMP_max',\n",
       " 'BVP_peak_freq',\n",
       " 'TEMP_slope',\n",
       " 'subject']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features=df.columns.tolist()\n",
    "\n",
    "to_remove = [fea for fea in features if \"label\"  in fea ]\n",
    "features = [x for x in features if x not in to_remove]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "to_remove = [fea for fea in features if \"label\"  in fea or \"subject\"  in fea]\n",
    "feature = [x for x in features if x not in to_remove]\n",
    "len(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train=df[df['subject']<=8]\n",
    "# test=df[df['subject']>8]\n",
    "\n",
    "# X_train = train[feature]\n",
    "# y_train = train['label']\n",
    "# X_test = test[feature]\n",
    "# y_test = test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=2)\n",
    "# X_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "# X_test_res, y_test_res = sm.fit_sample(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y= sm.fit_sample(df[features], df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([312, 312, 312, 312])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>net_acc_mean</th>\n",
       "      <th>net_acc_std</th>\n",
       "      <th>net_acc_min</th>\n",
       "      <th>net_acc_max</th>\n",
       "      <th>ACC_x_mean</th>\n",
       "      <th>ACC_x_std</th>\n",
       "      <th>ACC_x_min</th>\n",
       "      <th>ACC_x_max</th>\n",
       "      <th>ACC_y_mean</th>\n",
       "      <th>ACC_y_std</th>\n",
       "      <th>...</th>\n",
       "      <th>Resp_min</th>\n",
       "      <th>Resp_max</th>\n",
       "      <th>TEMP_mean</th>\n",
       "      <th>TEMP_std</th>\n",
       "      <th>TEMP_min</th>\n",
       "      <th>TEMP_max</th>\n",
       "      <th>BVP_peak_freq</th>\n",
       "      <th>TEMP_slope</th>\n",
       "      <th>subject</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.025961</td>\n",
       "      <td>0.013811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087383</td>\n",
       "      <td>0.023431</td>\n",
       "      <td>0.017769</td>\n",
       "      <td>-0.037843</td>\n",
       "      <td>0.087383</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>1.222594e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.805847</td>\n",
       "      <td>6.742859</td>\n",
       "      <td>35.807285</td>\n",
       "      <td>0.024986</td>\n",
       "      <td>35.750000</td>\n",
       "      <td>35.870000</td>\n",
       "      <td>0.081425</td>\n",
       "      <td>-0.000253</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.027640</td>\n",
       "      <td>0.010597</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>0.054356</td>\n",
       "      <td>0.027640</td>\n",
       "      <td>0.010597</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>0.054356</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>7.290999e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.914429</td>\n",
       "      <td>3.730774</td>\n",
       "      <td>35.706833</td>\n",
       "      <td>0.024641</td>\n",
       "      <td>35.660000</td>\n",
       "      <td>35.750000</td>\n",
       "      <td>0.147017</td>\n",
       "      <td>-0.000161</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.028389</td>\n",
       "      <td>0.006937</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066053</td>\n",
       "      <td>0.028378</td>\n",
       "      <td>0.006985</td>\n",
       "      <td>-0.002752</td>\n",
       "      <td>0.066053</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>4.805734e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.242493</td>\n",
       "      <td>3.450012</td>\n",
       "      <td>35.775430</td>\n",
       "      <td>0.037082</td>\n",
       "      <td>35.710000</td>\n",
       "      <td>35.840000</td>\n",
       "      <td>0.088210</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.033268</td>\n",
       "      <td>0.007670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074998</td>\n",
       "      <td>0.032960</td>\n",
       "      <td>0.008904</td>\n",
       "      <td>-0.030962</td>\n",
       "      <td>0.074998</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>6.126303e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.646729</td>\n",
       "      <td>5.216980</td>\n",
       "      <td>35.830724</td>\n",
       "      <td>0.025266</td>\n",
       "      <td>35.770000</td>\n",
       "      <td>35.890000</td>\n",
       "      <td>0.117614</td>\n",
       "      <td>-0.000256</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.037021</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.027522</td>\n",
       "      <td>0.043347</td>\n",
       "      <td>0.037021</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.027522</td>\n",
       "      <td>0.043347</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>8.837530e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.777100</td>\n",
       "      <td>3.028870</td>\n",
       "      <td>35.798869</td>\n",
       "      <td>0.020909</td>\n",
       "      <td>35.770000</td>\n",
       "      <td>35.840000</td>\n",
       "      <td>0.151541</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1243</td>\n",
       "      <td>0.042130</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.040968</td>\n",
       "      <td>0.043696</td>\n",
       "      <td>-0.039802</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>-0.041346</td>\n",
       "      <td>-0.038618</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>2.254555e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.510088</td>\n",
       "      <td>8.729651</td>\n",
       "      <td>31.505537</td>\n",
       "      <td>0.015360</td>\n",
       "      <td>31.483510</td>\n",
       "      <td>31.544919</td>\n",
       "      <td>0.164873</td>\n",
       "      <td>-0.000189</td>\n",
       "      <td>11.176097</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1244</td>\n",
       "      <td>0.039764</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.039219</td>\n",
       "      <td>0.039907</td>\n",
       "      <td>0.039764</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.039219</td>\n",
       "      <td>0.039907</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>1.862190e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.202333</td>\n",
       "      <td>7.398872</td>\n",
       "      <td>33.696923</td>\n",
       "      <td>0.019939</td>\n",
       "      <td>33.660000</td>\n",
       "      <td>33.741212</td>\n",
       "      <td>0.131407</td>\n",
       "      <td>-0.000133</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1245</td>\n",
       "      <td>0.041990</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.039308</td>\n",
       "      <td>0.043921</td>\n",
       "      <td>0.003807</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>0.006112</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1.590967e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.508980</td>\n",
       "      <td>6.769601</td>\n",
       "      <td>33.929413</td>\n",
       "      <td>0.014302</td>\n",
       "      <td>33.889531</td>\n",
       "      <td>33.958050</td>\n",
       "      <td>0.142617</td>\n",
       "      <td>-0.000093</td>\n",
       "      <td>9.166227</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1246</td>\n",
       "      <td>0.040894</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.040569</td>\n",
       "      <td>0.041733</td>\n",
       "      <td>0.040894</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.040569</td>\n",
       "      <td>0.041733</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>9.076908e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.100251</td>\n",
       "      <td>8.296914</td>\n",
       "      <td>32.774272</td>\n",
       "      <td>0.021280</td>\n",
       "      <td>32.740925</td>\n",
       "      <td>32.847081</td>\n",
       "      <td>0.137624</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>5.346098</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1247</td>\n",
       "      <td>0.021020</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.020642</td>\n",
       "      <td>0.022018</td>\n",
       "      <td>0.021020</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.020642</td>\n",
       "      <td>0.022018</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>1.957902e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.790579</td>\n",
       "      <td>6.949140</td>\n",
       "      <td>31.249947</td>\n",
       "      <td>0.016483</td>\n",
       "      <td>31.222351</td>\n",
       "      <td>31.291567</td>\n",
       "      <td>0.147902</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1248 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      net_acc_mean  net_acc_std  net_acc_min  net_acc_max  ACC_x_mean  \\\n",
       "0         0.025961     0.013811     0.000000     0.087383    0.023431   \n",
       "1         0.027640     0.010597     0.002752     0.054356    0.027640   \n",
       "2         0.028389     0.006937     0.000000     0.066053    0.028378   \n",
       "3         0.033268     0.007670     0.000000     0.074998    0.032960   \n",
       "4         0.037021     0.001284     0.027522     0.043347    0.037021   \n",
       "...            ...          ...          ...          ...         ...   \n",
       "1243      0.042130     0.000328     0.040968     0.043696   -0.039802   \n",
       "1244      0.039764     0.000271     0.039219     0.039907    0.039764   \n",
       "1245      0.041990     0.000231     0.039308     0.043921    0.003807   \n",
       "1246      0.040894     0.000132     0.040569     0.041733    0.040894   \n",
       "1247      0.021020     0.000285     0.020642     0.022018    0.021020   \n",
       "\n",
       "      ACC_x_std  ACC_x_min  ACC_x_max  ACC_y_mean     ACC_y_std  ...  \\\n",
       "0      0.017769  -0.037843   0.087383    0.000016  1.222594e-05  ...   \n",
       "1      0.010597   0.002752   0.054356    0.000019  7.290999e-06  ...   \n",
       "2      0.006985  -0.002752   0.066053    0.000020  4.805734e-06  ...   \n",
       "3      0.008904  -0.030962   0.074998    0.000023  6.126303e-06  ...   \n",
       "4      0.001284   0.027522   0.043347    0.000025  8.837530e-07  ...   \n",
       "...         ...        ...        ...         ...           ...  ...   \n",
       "1243   0.000328  -0.041346  -0.038618   -0.000027  2.254555e-07  ...   \n",
       "1244   0.000271   0.039219   0.039907    0.000027  1.862190e-07  ...   \n",
       "1245   0.000231   0.001499   0.006112    0.000003  1.590967e-07  ...   \n",
       "1246   0.000132   0.040569   0.041733    0.000028  9.076908e-08  ...   \n",
       "1247   0.000285   0.020642   0.022018    0.000014  1.957902e-07  ...   \n",
       "\n",
       "       Resp_min  Resp_max  TEMP_mean  TEMP_std   TEMP_min   TEMP_max  \\\n",
       "0     -8.805847  6.742859  35.807285  0.024986  35.750000  35.870000   \n",
       "1     -2.914429  3.730774  35.706833  0.024641  35.660000  35.750000   \n",
       "2     -3.242493  3.450012  35.775430  0.037082  35.710000  35.840000   \n",
       "3     -6.646729  5.216980  35.830724  0.025266  35.770000  35.890000   \n",
       "4     -2.777100  3.028870  35.798869  0.020909  35.770000  35.840000   \n",
       "...         ...       ...        ...       ...        ...        ...   \n",
       "1243 -10.510088  8.729651  31.505537  0.015360  31.483510  31.544919   \n",
       "1244  -9.202333  7.398872  33.696923  0.019939  33.660000  33.741212   \n",
       "1245  -8.508980  6.769601  33.929413  0.014302  33.889531  33.958050   \n",
       "1246  -8.100251  8.296914  32.774272  0.021280  32.740925  32.847081   \n",
       "1247  -7.790579  6.949140  31.249947  0.016483  31.222351  31.291567   \n",
       "\n",
       "      BVP_peak_freq  TEMP_slope    subject  label  \n",
       "0          0.081425   -0.000253   2.000000      0  \n",
       "1          0.147017   -0.000161   2.000000      0  \n",
       "2          0.088210    0.000535   2.000000      0  \n",
       "3          0.117614   -0.000256   2.000000      0  \n",
       "4          0.151541    0.000260   2.000000      0  \n",
       "...             ...         ...        ...    ...  \n",
       "1243       0.164873   -0.000189  11.176097      3  \n",
       "1244       0.131407   -0.000133  13.000000      3  \n",
       "1245       0.142617   -0.000093   9.166227      3  \n",
       "1246       0.137624    0.000187   5.346098      3  \n",
       "1247       0.147902   -0.000013   9.000000      3  \n",
       "\n",
       "[1248 rows x 48 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new=pd.concat([pd.DataFrame(X,columns=features),pd.DataFrame(y,columns=['label'])],axis=1)\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(list(df_new['subject']))):\n",
    "    df_new['subject'][i]=min([2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17], key=lambda x:abs(x-list(df_new['subject'])[i])) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['subject']=df_new['subject'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 13, 14, 15, 16, 17])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new['subject'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=df_new[df_new['subject']<=9]\n",
    "test=df_new[df_new['subject']>9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.67      0.66       147\n",
      "           1       0.90      0.80      0.85       161\n",
      "           2       0.50      0.66      0.57       147\n",
      "           3       0.67      0.53      0.59       150\n",
      "\n",
      "    accuracy                           0.67       605\n",
      "   macro avg       0.68      0.66      0.67       605\n",
      "weighted avg       0.69      0.67      0.67       605\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_data_train = scaler.fit_transform(train[feature])\n",
    "scaled_data_test = scaler.fit_transform(test[feature])\n",
    "\n",
    "\n",
    "et = ExtraTreesClassifier(n_estimators=50, n_jobs=10)\n",
    "et.fit(scaled_data_train,train['label'])\n",
    "y_pred=et.predict(scaled_data_test)\n",
    "print(classification_report(test['label'],y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.59      0.67       147\n",
      "           1       0.71      0.94      0.81       161\n",
      "           2       0.55      0.59      0.57       147\n",
      "           3       0.68      0.57      0.62       150\n",
      "\n",
      "    accuracy                           0.68       605\n",
      "   macro avg       0.68      0.67      0.67       605\n",
      "weighted avg       0.68      0.68      0.67       605\n",
      "\n",
      "CPU times: user 203 ms, sys: 93.9 ms, total: 296 ms\n",
      "Wall time: 235 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scaler = Normalizer()\n",
    "scaled_data_train = scaler.fit_transform(train[feature])\n",
    "scaled_data_test = scaler.fit_transform(test[feature])\n",
    "et = ExtraTreesClassifier(n_estimators=50, n_jobs=10)\n",
    "et.fit(scaled_data_train,train['label'])\n",
    "y_pred=et.predict(scaled_data_test)\n",
    "print(classification_report(test['label'],y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.44      0.53       147\n",
      "           1       0.79      0.87      0.83       161\n",
      "           2       0.56      0.58      0.57       147\n",
      "           3       0.61      0.71      0.66       150\n",
      "\n",
      "    accuracy                           0.66       605\n",
      "   macro avg       0.65      0.65      0.64       605\n",
      "weighted avg       0.66      0.66      0.65       605\n",
      "\n"
     ]
    }
   ],
   "source": [
    "et = ExtraTreesClassifier(n_estimators=50, n_jobs=10,)\n",
    "et.fit(train[feature],train['label'])\n",
    "y_pred=et.predict(test[feature])\n",
    "print(classification_report(test['label'],y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_res' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "et = ExtraTreesClassifier(n_estimators=50, n_jobs=10, verbose=2)\n",
    "et.fit(X_train_res,y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  21 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=10)]: Done  50 out of  50 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 50building tree 2 of 50building tree 3 of 50building tree 4 of 50\n",
      "building tree 5 of 50\n",
      "building tree 6 of 50\n",
      "building tree 7 of 50building tree 8 of 50\n",
      "\n",
      "\n",
      "building tree 9 of 50\n",
      "\n",
      "building tree 10 of 50\n",
      "\n",
      "building tree 11 of 50\n",
      "building tree 12 of 50\n",
      "building tree 13 of 50\n",
      "building tree 14 of 50building tree 15 of 50\n",
      "\n",
      "building tree 16 of 50\n",
      "building tree 17 of 50building tree 18 of 50\n",
      "\n",
      "building tree 19 of 50\n",
      "building tree 20 of 50\n",
      "building tree 21 of 50\n",
      "building tree 22 of 50\n",
      "building tree 23 of 50\n",
      "building tree 24 of 50\n",
      "building tree 25 of 50\n",
      "building tree 26 of 50\n",
      "building tree 27 of 50\n",
      "building tree 28 of 50\n",
      "building tree 29 of 50\n",
      "building tree 30 of 50\n",
      "building tree 31 of 50\n",
      "building tree 32 of 50\n",
      "building tree 33 of 50\n",
      "building tree 34 of 50\n",
      "building tree 35 of 50\n",
      "building tree 36 of 50\n",
      "building tree 37 of 50\n",
      "building tree 38 of 50\n",
      "building tree 39 of 50\n",
      "building tree 40 of 50\n",
      "building tree 41 of 50\n",
      "building tree 42 of 50\n",
      "building tree 43 of 50\n",
      "building tree 44 of 50\n",
      "building tree 45 of 50\n",
      "building tree 46 of 50\n",
      "building tree 47 of 50\n",
      "building tree 48 of 50\n",
      "building tree 49 of 50\n",
      "building tree 50 of 50\n",
      "CPU times: user 204 ms, sys: 93.4 ms, total: 297 ms\n",
      "Wall time: 227 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "                     max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                     min_samples_leaf=1, min_samples_split=2,\n",
       "                     min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=10,\n",
       "                     oob_score=False, random_state=None, verbose=2,\n",
       "                     warm_start=False)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "et = ExtraTreesClassifier(n_estimators=50, n_jobs=10, verbose=2)\n",
    "et.fit(train[feature],train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = ExtraTreesClassifier(n_estimators=50, n_jobs=10, verbose=1,random_state=0)\n",
    "clf2 = DecisionTreeClassifier()\n",
    "clf3 = RandomForestClassifier(n_estimators=10)\n",
    "clf4 = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "sclf = StackingClassifier(classifiers=[clf4,  clf1], meta_classifier=clf3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=10)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=10)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=10)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=10)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=10)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=10)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=10)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=10)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=10)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=10)]: Done  50 out of  50 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.90 (+/- 0.06) [ExtraTreesClassifier]\n",
      "Accuracy: 0.79 (+/- 0.09) [DecisionTreeClassifier]\n",
      "Accuracy: 0.87 (+/- 0.06) [RandomForestClassifier]\n"
     ]
    }
   ],
   "source": [
    "for clf, label in zip([clf1, clf2, clf3, sclf], ['ExtraTreesClassifier','DecisionTreeClassifier','RandomForestClassifier']):\n",
    "\n",
    "    scores = model_selection.cross_val_score(clf, train[features], train['label'],cv=10, scoring='accuracy')\n",
    "    \n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=10)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=10)]: Done  50 out of  50 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.39      0.49       147\n",
      "           1       0.78      0.83      0.81       161\n",
      "           2       0.59      0.65      0.61       147\n",
      "           3       0.56      0.69      0.61       150\n",
      "\n",
      "    accuracy                           0.64       605\n",
      "   macro avg       0.65      0.64      0.63       605\n",
      "weighted avg       0.65      0.64      0.64       605\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=10)]: Done  50 out of  50 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "sclf.fit( train[feature],  train['label'])\n",
    "y_pred=sclf.predict(test[feature])\n",
    "print(classification_report(test['label'],y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of features of the model must match the input. Model n_features is 48 and input n_features is 605 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-312-dbed5fcfa08e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/mlxtend/classifier/stacking_classification.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \"\"\"\n\u001b[1;32m    246\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'clfs_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mmeta_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_meta_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_features_in_secondary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/mlxtend/classifier/stacking_classification.py\u001b[0m in \u001b[0;36mpredict_meta_features\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m             \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclfs_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/mlxtend/classifier/stacking_classification.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m             \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclfs_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \"\"\"\n\u001b[0;32m--> 545\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'estimators_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;31m# Check data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;31m# Assign chunk of trees to jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    357\u001b[0m                                  \"call `fit` before exploiting the model.\")\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    400\u001b[0m                              \u001b[0;34m\"match the input. Model n_features is %s and \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m                              \u001b[0;34m\"input n_features is %s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m                              % (self.n_features_, n_features))\n\u001b[0m\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features is 48 and input n_features is 605 "
     ]
    }
   ],
   "source": [
    "y_pred=sclf.predict(test['label'])\n",
    "print(classification_report(test['label'],y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.57      0.49       147\n",
      "           1       0.70      0.70      0.70       161\n",
      "           2       0.42      0.33      0.37       147\n",
      "           3       0.58      0.51      0.54       150\n",
      "\n",
      "    accuracy                           0.53       605\n",
      "   macro avg       0.53      0.53      0.52       605\n",
      "weighted avg       0.54      0.53      0.53       605\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='poly', random_state=0, gamma=.01, C=1)\n",
    "# Train the classifier\n",
    "svm.fit( train[feature],  train['label'])\n",
    "y_pred=svm.predict(test[feature])\n",
    "print(classification_report(test['label'],y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.54      0.63       147\n",
      "           1       0.88      0.78      0.83       161\n",
      "           2       0.57      0.71      0.63       147\n",
      "           3       0.63      0.72      0.67       150\n",
      "\n",
      "    accuracy                           0.69       605\n",
      "   macro avg       0.71      0.69      0.69       605\n",
      "weighted avg       0.71      0.69      0.69       605\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()\n",
    "model.fit( train[feature],  train['label'])\n",
    "y_pred=model.predict(test[feature])\n",
    "print(classification_report(test['label'],y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50.5 ms, sys: 1.59 ms, total: 52.1 ms\n",
      "Wall time: 107 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  21 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=10)]: Done  50 out of  50 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred=et.predict(test[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.59      0.68       147\n",
      "           1       0.75      0.91      0.82       161\n",
      "           2       0.60      0.68      0.63       147\n",
      "           3       0.70      0.61      0.65       150\n",
      "\n",
      "    accuracy                           0.70       605\n",
      "   macro avg       0.71      0.70      0.70       605\n",
      "weighted avg       0.71      0.70      0.70       605\n",
      "\n",
      "CPU times: user 5.85 ms, sys: 54 µs, total: 5.91 ms\n",
      "Wall time: 4.74 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(classification_report(test['label'],y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "           0       0.73      0.55      0.63       147\n",
    "           1       0.89      0.88      0.88       161\n",
    "           2       0.53      0.80      0.63       147\n",
    "           3       0.75      0.57      0.64       150\n",
    "\n",
    "    accuracy                           0.70       605\n",
    "   macro avg       0.72      0.70      0.70       605\n",
    "weighted avg       0.73      0.70      0.70       605"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of features of the model must match the input. Model n_features is 46 and input n_features is 47 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \"\"\"\n\u001b[0;32m--> 545\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'estimators_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;31m# Check data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;31m# Assign chunk of trees to jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    357\u001b[0m                                  \"call `fit` before exploiting the model.\")\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    400\u001b[0m                              \u001b[0;34m\"match the input. Model n_features is %s and \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m                              \u001b[0;34m\"input n_features is %s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m                              % (self.n_features_, n_features))\n\u001b[0m\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features is 46 and input n_features is 47 "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred=et.predict(X_test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.54      0.59       168\n",
      "           1       0.72      0.95      0.82       168\n",
      "           2       0.53      0.15      0.23       168\n",
      "           3       0.51      0.80      0.62       168\n",
      "\n",
      "    accuracy                           0.61       672\n",
      "   macro avg       0.60      0.61      0.57       672\n",
      "weighted avg       0.60      0.61      0.57       672\n",
      "\n",
      "CPU times: user 5.91 ms, sys: 120 µs, total: 6.03 ms\n",
      "Wall time: 4.45 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(classification_report(y_test_res,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02763924, 0.00884209, 0.03382173, 0.01837484, 0.02394089,\n",
       "       0.00875603, 0.01667229, 0.01530304, 0.02401515, 0.01022684,\n",
       "       0.01777898, 0.01683037, 0.02152285, 0.01002841, 0.01969484,\n",
       "       0.01637563, 0.00254085, 0.01140942, 0.01074339, 0.0093354 ,\n",
       "       0.05118239, 0.00432433, 0.05112299, 0.05011299, 0.01265779,\n",
       "       0.01077112, 0.01138274, 0.01408509, 0.0052504 , 0.00441432,\n",
       "       0.00149352, 0.00378826, 0.06773331, 0.01048411, 0.08070434,\n",
       "       0.08505767, 0.00247955, 0.00516083, 0.00339077, 0.00296881,\n",
       "       0.06256175, 0.00134438, 0.06504994, 0.06096728, 0.00658133,\n",
       "       0.00107774])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "et.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>net_acc_mean</th>\n",
       "      <th>net_acc_std</th>\n",
       "      <th>net_acc_min</th>\n",
       "      <th>net_acc_max</th>\n",
       "      <th>ACC_x_mean</th>\n",
       "      <th>ACC_x_std</th>\n",
       "      <th>ACC_x_min</th>\n",
       "      <th>ACC_x_max</th>\n",
       "      <th>ACC_y_mean</th>\n",
       "      <th>ACC_y_std</th>\n",
       "      <th>...</th>\n",
       "      <th>Resp_min</th>\n",
       "      <th>Resp_max</th>\n",
       "      <th>TEMP_mean</th>\n",
       "      <th>TEMP_std</th>\n",
       "      <th>TEMP_min</th>\n",
       "      <th>TEMP_max</th>\n",
       "      <th>BVP_peak_freq</th>\n",
       "      <th>TEMP_slope</th>\n",
       "      <th>subject</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.025961</td>\n",
       "      <td>0.013811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087383</td>\n",
       "      <td>0.023431</td>\n",
       "      <td>0.017769</td>\n",
       "      <td>-0.037843</td>\n",
       "      <td>0.087383</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>1.222594e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.805847</td>\n",
       "      <td>6.742859</td>\n",
       "      <td>35.807285</td>\n",
       "      <td>0.024986</td>\n",
       "      <td>35.750000</td>\n",
       "      <td>35.870000</td>\n",
       "      <td>0.081425</td>\n",
       "      <td>-0.000253</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.027640</td>\n",
       "      <td>0.010597</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>0.054356</td>\n",
       "      <td>0.027640</td>\n",
       "      <td>0.010597</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>0.054356</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>7.290999e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.914429</td>\n",
       "      <td>3.730774</td>\n",
       "      <td>35.706833</td>\n",
       "      <td>0.024641</td>\n",
       "      <td>35.660000</td>\n",
       "      <td>35.750000</td>\n",
       "      <td>0.147017</td>\n",
       "      <td>-0.000161</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.028389</td>\n",
       "      <td>0.006937</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066053</td>\n",
       "      <td>0.028378</td>\n",
       "      <td>0.006985</td>\n",
       "      <td>-0.002752</td>\n",
       "      <td>0.066053</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>4.805734e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.242493</td>\n",
       "      <td>3.450012</td>\n",
       "      <td>35.775430</td>\n",
       "      <td>0.037082</td>\n",
       "      <td>35.710000</td>\n",
       "      <td>35.840000</td>\n",
       "      <td>0.088210</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.033268</td>\n",
       "      <td>0.007670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074998</td>\n",
       "      <td>0.032960</td>\n",
       "      <td>0.008904</td>\n",
       "      <td>-0.030962</td>\n",
       "      <td>0.074998</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>6.126303e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.646729</td>\n",
       "      <td>5.216980</td>\n",
       "      <td>35.830724</td>\n",
       "      <td>0.025266</td>\n",
       "      <td>35.770000</td>\n",
       "      <td>35.890000</td>\n",
       "      <td>0.117614</td>\n",
       "      <td>-0.000256</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.037021</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.027522</td>\n",
       "      <td>0.043347</td>\n",
       "      <td>0.037021</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.027522</td>\n",
       "      <td>0.043347</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>8.837530e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.777100</td>\n",
       "      <td>3.028870</td>\n",
       "      <td>35.798869</td>\n",
       "      <td>0.020909</td>\n",
       "      <td>35.770000</td>\n",
       "      <td>35.840000</td>\n",
       "      <td>0.151541</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1243</td>\n",
       "      <td>0.042130</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.040968</td>\n",
       "      <td>0.043696</td>\n",
       "      <td>-0.039802</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>-0.041346</td>\n",
       "      <td>-0.038618</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>2.254555e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.510088</td>\n",
       "      <td>8.729651</td>\n",
       "      <td>31.505537</td>\n",
       "      <td>0.015360</td>\n",
       "      <td>31.483510</td>\n",
       "      <td>31.544919</td>\n",
       "      <td>0.164873</td>\n",
       "      <td>-0.000189</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1244</td>\n",
       "      <td>0.039764</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.039219</td>\n",
       "      <td>0.039907</td>\n",
       "      <td>0.039764</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.039219</td>\n",
       "      <td>0.039907</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>1.862190e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.202333</td>\n",
       "      <td>7.398872</td>\n",
       "      <td>33.696923</td>\n",
       "      <td>0.019939</td>\n",
       "      <td>33.660000</td>\n",
       "      <td>33.741212</td>\n",
       "      <td>0.131407</td>\n",
       "      <td>-0.000133</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1245</td>\n",
       "      <td>0.041990</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.039308</td>\n",
       "      <td>0.043921</td>\n",
       "      <td>0.003807</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>0.006112</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1.590967e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.508980</td>\n",
       "      <td>6.769601</td>\n",
       "      <td>33.929413</td>\n",
       "      <td>0.014302</td>\n",
       "      <td>33.889531</td>\n",
       "      <td>33.958050</td>\n",
       "      <td>0.142617</td>\n",
       "      <td>-0.000093</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1246</td>\n",
       "      <td>0.040894</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.040569</td>\n",
       "      <td>0.041733</td>\n",
       "      <td>0.040894</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.040569</td>\n",
       "      <td>0.041733</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>9.076908e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.100251</td>\n",
       "      <td>8.296914</td>\n",
       "      <td>32.774272</td>\n",
       "      <td>0.021280</td>\n",
       "      <td>32.740925</td>\n",
       "      <td>32.847081</td>\n",
       "      <td>0.137624</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1247</td>\n",
       "      <td>0.021020</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.020642</td>\n",
       "      <td>0.022018</td>\n",
       "      <td>0.021020</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.020642</td>\n",
       "      <td>0.022018</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>1.957902e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.790579</td>\n",
       "      <td>6.949140</td>\n",
       "      <td>31.249947</td>\n",
       "      <td>0.016483</td>\n",
       "      <td>31.222351</td>\n",
       "      <td>31.291567</td>\n",
       "      <td>0.147902</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1248 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      net_acc_mean  net_acc_std  net_acc_min  net_acc_max  ACC_x_mean  \\\n",
       "0         0.025961     0.013811     0.000000     0.087383    0.023431   \n",
       "1         0.027640     0.010597     0.002752     0.054356    0.027640   \n",
       "2         0.028389     0.006937     0.000000     0.066053    0.028378   \n",
       "3         0.033268     0.007670     0.000000     0.074998    0.032960   \n",
       "4         0.037021     0.001284     0.027522     0.043347    0.037021   \n",
       "...            ...          ...          ...          ...         ...   \n",
       "1243      0.042130     0.000328     0.040968     0.043696   -0.039802   \n",
       "1244      0.039764     0.000271     0.039219     0.039907    0.039764   \n",
       "1245      0.041990     0.000231     0.039308     0.043921    0.003807   \n",
       "1246      0.040894     0.000132     0.040569     0.041733    0.040894   \n",
       "1247      0.021020     0.000285     0.020642     0.022018    0.021020   \n",
       "\n",
       "      ACC_x_std  ACC_x_min  ACC_x_max  ACC_y_mean     ACC_y_std  ...  \\\n",
       "0      0.017769  -0.037843   0.087383    0.000016  1.222594e-05  ...   \n",
       "1      0.010597   0.002752   0.054356    0.000019  7.290999e-06  ...   \n",
       "2      0.006985  -0.002752   0.066053    0.000020  4.805734e-06  ...   \n",
       "3      0.008904  -0.030962   0.074998    0.000023  6.126303e-06  ...   \n",
       "4      0.001284   0.027522   0.043347    0.000025  8.837530e-07  ...   \n",
       "...         ...        ...        ...         ...           ...  ...   \n",
       "1243   0.000328  -0.041346  -0.038618   -0.000027  2.254555e-07  ...   \n",
       "1244   0.000271   0.039219   0.039907    0.000027  1.862190e-07  ...   \n",
       "1245   0.000231   0.001499   0.006112    0.000003  1.590967e-07  ...   \n",
       "1246   0.000132   0.040569   0.041733    0.000028  9.076908e-08  ...   \n",
       "1247   0.000285   0.020642   0.022018    0.000014  1.957902e-07  ...   \n",
       "\n",
       "       Resp_min  Resp_max  TEMP_mean  TEMP_std   TEMP_min   TEMP_max  \\\n",
       "0     -8.805847  6.742859  35.807285  0.024986  35.750000  35.870000   \n",
       "1     -2.914429  3.730774  35.706833  0.024641  35.660000  35.750000   \n",
       "2     -3.242493  3.450012  35.775430  0.037082  35.710000  35.840000   \n",
       "3     -6.646729  5.216980  35.830724  0.025266  35.770000  35.890000   \n",
       "4     -2.777100  3.028870  35.798869  0.020909  35.770000  35.840000   \n",
       "...         ...       ...        ...       ...        ...        ...   \n",
       "1243 -10.510088  8.729651  31.505537  0.015360  31.483510  31.544919   \n",
       "1244  -9.202333  7.398872  33.696923  0.019939  33.660000  33.741212   \n",
       "1245  -8.508980  6.769601  33.929413  0.014302  33.889531  33.958050   \n",
       "1246  -8.100251  8.296914  32.774272  0.021280  32.740925  32.847081   \n",
       "1247  -7.790579  6.949140  31.249947  0.016483  31.222351  31.291567   \n",
       "\n",
       "      BVP_peak_freq  TEMP_slope  subject  label  \n",
       "0          0.081425   -0.000253        2      0  \n",
       "1          0.147017   -0.000161        2      0  \n",
       "2          0.088210    0.000535        2      0  \n",
       "3          0.117614   -0.000256        2      0  \n",
       "4          0.151541    0.000260        2      0  \n",
       "...             ...         ...      ...    ...  \n",
       "1243       0.164873   -0.000189       11      3  \n",
       "1244       0.131407   -0.000133       13      3  \n",
       "1245       0.142617   -0.000093        9      3  \n",
       "1246       0.137624    0.000187        5      3  \n",
       "1247       0.147902   -0.000013        9      3  \n",
       "\n",
       "[1248 rows x 48 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(32, input_dim=46))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(18))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(8))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>net_acc_mean</th>\n",
       "      <th>net_acc_std</th>\n",
       "      <th>net_acc_min</th>\n",
       "      <th>net_acc_max</th>\n",
       "      <th>ACC_x_mean</th>\n",
       "      <th>ACC_x_std</th>\n",
       "      <th>ACC_x_min</th>\n",
       "      <th>ACC_x_max</th>\n",
       "      <th>ACC_y_mean</th>\n",
       "      <th>ACC_y_std</th>\n",
       "      <th>...</th>\n",
       "      <th>TEMP_min</th>\n",
       "      <th>TEMP_max</th>\n",
       "      <th>BVP_peak_freq</th>\n",
       "      <th>TEMP_slope</th>\n",
       "      <th>subject</th>\n",
       "      <th>label</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.025961</td>\n",
       "      <td>0.013811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087383</td>\n",
       "      <td>0.023431</td>\n",
       "      <td>0.017769</td>\n",
       "      <td>-0.037843</td>\n",
       "      <td>0.087383</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>1.222594e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>35.750000</td>\n",
       "      <td>35.870000</td>\n",
       "      <td>0.081425</td>\n",
       "      <td>-0.000253</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.027640</td>\n",
       "      <td>0.010597</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>0.054356</td>\n",
       "      <td>0.027640</td>\n",
       "      <td>0.010597</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>0.054356</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>7.290999e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>35.660000</td>\n",
       "      <td>35.750000</td>\n",
       "      <td>0.147017</td>\n",
       "      <td>-0.000161</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.028389</td>\n",
       "      <td>0.006937</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066053</td>\n",
       "      <td>0.028378</td>\n",
       "      <td>0.006985</td>\n",
       "      <td>-0.002752</td>\n",
       "      <td>0.066053</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>4.805734e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>35.710000</td>\n",
       "      <td>35.840000</td>\n",
       "      <td>0.088210</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.033268</td>\n",
       "      <td>0.007670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074998</td>\n",
       "      <td>0.032960</td>\n",
       "      <td>0.008904</td>\n",
       "      <td>-0.030962</td>\n",
       "      <td>0.074998</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>6.126303e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>35.770000</td>\n",
       "      <td>35.890000</td>\n",
       "      <td>0.117614</td>\n",
       "      <td>-0.000256</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.037021</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.027522</td>\n",
       "      <td>0.043347</td>\n",
       "      <td>0.037021</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.027522</td>\n",
       "      <td>0.043347</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>8.837530e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>35.770000</td>\n",
       "      <td>35.840000</td>\n",
       "      <td>0.151541</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1243</td>\n",
       "      <td>0.042130</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.040968</td>\n",
       "      <td>0.043696</td>\n",
       "      <td>-0.039802</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>-0.041346</td>\n",
       "      <td>-0.038618</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>2.254555e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>31.483510</td>\n",
       "      <td>31.544919</td>\n",
       "      <td>0.164873</td>\n",
       "      <td>-0.000189</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1244</td>\n",
       "      <td>0.039764</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.039219</td>\n",
       "      <td>0.039907</td>\n",
       "      <td>0.039764</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.039219</td>\n",
       "      <td>0.039907</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>1.862190e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>33.660000</td>\n",
       "      <td>33.741212</td>\n",
       "      <td>0.131407</td>\n",
       "      <td>-0.000133</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1245</td>\n",
       "      <td>0.041990</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.039308</td>\n",
       "      <td>0.043921</td>\n",
       "      <td>0.003807</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>0.006112</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1.590967e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>33.889531</td>\n",
       "      <td>33.958050</td>\n",
       "      <td>0.142617</td>\n",
       "      <td>-0.000093</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1246</td>\n",
       "      <td>0.040894</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.040569</td>\n",
       "      <td>0.041733</td>\n",
       "      <td>0.040894</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.040569</td>\n",
       "      <td>0.041733</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>9.076908e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>32.740925</td>\n",
       "      <td>32.847081</td>\n",
       "      <td>0.137624</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1247</td>\n",
       "      <td>0.021020</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.020642</td>\n",
       "      <td>0.022018</td>\n",
       "      <td>0.021020</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.020642</td>\n",
       "      <td>0.022018</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>1.957902e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>31.222351</td>\n",
       "      <td>31.291567</td>\n",
       "      <td>0.147902</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1248 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      net_acc_mean  net_acc_std  net_acc_min  net_acc_max  ACC_x_mean  \\\n",
       "0         0.025961     0.013811     0.000000     0.087383    0.023431   \n",
       "1         0.027640     0.010597     0.002752     0.054356    0.027640   \n",
       "2         0.028389     0.006937     0.000000     0.066053    0.028378   \n",
       "3         0.033268     0.007670     0.000000     0.074998    0.032960   \n",
       "4         0.037021     0.001284     0.027522     0.043347    0.037021   \n",
       "...            ...          ...          ...          ...         ...   \n",
       "1243      0.042130     0.000328     0.040968     0.043696   -0.039802   \n",
       "1244      0.039764     0.000271     0.039219     0.039907    0.039764   \n",
       "1245      0.041990     0.000231     0.039308     0.043921    0.003807   \n",
       "1246      0.040894     0.000132     0.040569     0.041733    0.040894   \n",
       "1247      0.021020     0.000285     0.020642     0.022018    0.021020   \n",
       "\n",
       "      ACC_x_std  ACC_x_min  ACC_x_max  ACC_y_mean     ACC_y_std  ...  \\\n",
       "0      0.017769  -0.037843   0.087383    0.000016  1.222594e-05  ...   \n",
       "1      0.010597   0.002752   0.054356    0.000019  7.290999e-06  ...   \n",
       "2      0.006985  -0.002752   0.066053    0.000020  4.805734e-06  ...   \n",
       "3      0.008904  -0.030962   0.074998    0.000023  6.126303e-06  ...   \n",
       "4      0.001284   0.027522   0.043347    0.000025  8.837530e-07  ...   \n",
       "...         ...        ...        ...         ...           ...  ...   \n",
       "1243   0.000328  -0.041346  -0.038618   -0.000027  2.254555e-07  ...   \n",
       "1244   0.000271   0.039219   0.039907    0.000027  1.862190e-07  ...   \n",
       "1245   0.000231   0.001499   0.006112    0.000003  1.590967e-07  ...   \n",
       "1246   0.000132   0.040569   0.041733    0.000028  9.076908e-08  ...   \n",
       "1247   0.000285   0.020642   0.022018    0.000014  1.957902e-07  ...   \n",
       "\n",
       "       TEMP_min   TEMP_max  BVP_peak_freq  TEMP_slope  subject  label  0  1  \\\n",
       "0     35.750000  35.870000       0.081425   -0.000253        2      0  1  0   \n",
       "1     35.660000  35.750000       0.147017   -0.000161        2      0  1  0   \n",
       "2     35.710000  35.840000       0.088210    0.000535        2      0  1  0   \n",
       "3     35.770000  35.890000       0.117614   -0.000256        2      0  1  0   \n",
       "4     35.770000  35.840000       0.151541    0.000260        2      0  1  0   \n",
       "...         ...        ...            ...         ...      ...    ... .. ..   \n",
       "1243  31.483510  31.544919       0.164873   -0.000189       11      3  0  0   \n",
       "1244  33.660000  33.741212       0.131407   -0.000133       13      3  0  0   \n",
       "1245  33.889531  33.958050       0.142617   -0.000093        9      3  0  0   \n",
       "1246  32.740925  32.847081       0.137624    0.000187        5      3  0  0   \n",
       "1247  31.222351  31.291567       0.147902   -0.000013        9      3  0  0   \n",
       "\n",
       "      2  3  \n",
       "0     0  0  \n",
       "1     0  0  \n",
       "2     0  0  \n",
       "3     0  0  \n",
       "4     0  0  \n",
       "...  .. ..  \n",
       "1243  0  1  \n",
       "1244  0  1  \n",
       "1245  0  1  \n",
       "1246  0  1  \n",
       "1247  0  1  \n",
       "\n",
       "[1248 rows x 52 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = pd.concat([df_new,pd.get_dummies(df_new['label'])],axis=1)\n",
    "#df_new.drop(['label'],axis=1, inplace=True)\n",
    "df_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=df_new[df_new['subject']<=9]\n",
    "test=df_new[df_new['subject']>9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Dense(12, input_dim=8, activation='adam'))\n",
    "# model.add(Dense(8, activation='adam'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# model.compile(loss='binary_crossentropy', optimizer='relu', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "1238    0\n",
       "1239    0\n",
       "1245    0\n",
       "1246    0\n",
       "1247    0\n",
       "Name: 0, Length: 643, dtype: uint8"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 1.5565 - acc: 0.5241\n",
      "Epoch 2/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.9252 - acc: 0.6796\n",
      "Epoch 3/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.3838 - acc: 0.8585\n",
      "Epoch 4/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.8309 - acc: 0.8212\n",
      "Epoch 5/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.3716 - acc: 0.8771\n",
      "Epoch 6/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.2488 - acc: 0.9300\n",
      "Epoch 7/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.1932 - acc: 0.9440\n",
      "Epoch 8/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.1556 - acc: 0.9580\n",
      "Epoch 9/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.8010 - acc: 0.7434\n",
      "Epoch 10/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.4303 - acc: 0.8445\n",
      "Epoch 11/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.3044 - acc: 0.8849\n",
      "Epoch 12/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.2743 - acc: 0.9051\n",
      "Epoch 13/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.2000 - acc: 0.9393\n",
      "Epoch 14/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.2821 - acc: 0.9114\n",
      "Epoch 15/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.1972 - acc: 0.9347\n",
      "Epoch 16/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.2650 - acc: 0.9253\n",
      "Epoch 17/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.3288 - acc: 0.8927\n",
      "Epoch 18/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.2208 - acc: 0.9114\n",
      "Epoch 19/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.2145 - acc: 0.9222\n",
      "Epoch 20/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.1178 - acc: 0.9642\n",
      "Epoch 21/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.3080 - acc: 0.9409\n",
      "Epoch 22/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.6920 - acc: 0.8118\n",
      "Epoch 23/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.4450 - acc: 0.8103\n",
      "Epoch 24/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.2607 - acc: 0.9020\n",
      "Epoch 25/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.2449 - acc: 0.9067\n",
      "Epoch 26/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.4841 - acc: 0.8818\n",
      "Epoch 27/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.2047 - acc: 0.9191\n",
      "Epoch 28/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.1503 - acc: 0.9456\n",
      "Epoch 29/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.6985 - acc: 0.7854\n",
      "Epoch 30/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.5575 - acc: 0.8569\n",
      "Epoch 31/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.3682 - acc: 0.8756\n",
      "Epoch 32/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.1950 - acc: 0.9300\n",
      "Epoch 33/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.6635 - acc: 0.8258\n",
      "Epoch 34/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.8942 - acc: 0.6174\n",
      "Epoch 35/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.5902 - acc: 0.7745\n",
      "Epoch 36/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.3726 - acc: 0.8491\n",
      "Epoch 37/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.2885 - acc: 0.8896\n",
      "Epoch 38/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.3050 - acc: 0.8911\n",
      "Epoch 39/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.3565 - acc: 0.9145\n",
      "Epoch 40/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 1.0927 - acc: 0.7574\n",
      "Epoch 41/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.3633 - acc: 0.8709\n",
      "Epoch 42/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.1859 - acc: 0.9331\n",
      "Epoch 43/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.1668 - acc: 0.9471\n",
      "Epoch 44/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.1485 - acc: 0.9533\n",
      "Epoch 45/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.1466 - acc: 0.9378\n",
      "Epoch 46/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.4071 - acc: 0.8616\n",
      "Epoch 47/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.6553 - acc: 0.8336\n",
      "Epoch 48/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 1.4548 - acc: 0.8351\n",
      "Epoch 49/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 3.9239 - acc: 0.7201\n",
      "Epoch 50/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 3.5897 - acc: 0.6267\n",
      "Epoch 51/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.5400 - acc: 0.7963\n",
      "Epoch 52/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.2135 - acc: 0.9425\n",
      "Epoch 53/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.1568 - acc: 0.9518\n",
      "Epoch 54/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.5063 - acc: 0.8756\n",
      "Epoch 55/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.4580 - acc: 0.8709\n",
      "Epoch 56/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.2782 - acc: 0.8958\n",
      "Epoch 57/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.1694 - acc: 0.9502\n",
      "Epoch 58/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.1001 - acc: 0.9596\n",
      "Epoch 59/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.1257 - acc: 0.9549\n",
      "Epoch 60/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.2247 - acc: 0.9222\n",
      "Epoch 61/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.3783 - acc: 0.8911\n",
      "Epoch 62/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.3993 - acc: 0.8942\n",
      "Epoch 63/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.2624 - acc: 0.9269\n",
      "Epoch 64/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.1297 - acc: 0.9627\n",
      "Epoch 65/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.0631 - acc: 0.9813\n",
      "Epoch 66/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.4587 - acc: 0.8631\n",
      "Epoch 67/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.2145 - acc: 0.9316\n",
      "Epoch 68/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.1690 - acc: 0.9471\n",
      "Epoch 69/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.1638 - acc: 0.9565\n",
      "Epoch 70/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.3927 - acc: 0.8709\n",
      "Epoch 71/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.9083 - acc: 0.8849\n",
      "Epoch 72/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 1.2195 - acc: 0.8149\n",
      "Epoch 73/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.6528 - acc: 0.8538\n",
      "Epoch 74/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.7776 - acc: 0.8258\n",
      "Epoch 75/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.8709 - acc: 0.8087\n",
      "Epoch 76/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.4950 - acc: 0.8849\n",
      "Epoch 77/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.3717 - acc: 0.9098\n",
      "Epoch 78/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.2397 - acc: 0.9300\n",
      "Epoch 79/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.6091 - acc: 0.7994\n",
      "Epoch 80/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.5246 - acc: 0.7869\n",
      "Epoch 81/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.4738 - acc: 0.7932\n",
      "Epoch 82/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.3817 - acc: 0.8383\n",
      "Epoch 83/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.2123 - acc: 0.9222\n",
      "Epoch 84/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.2156 - acc: 0.9440\n",
      "Epoch 85/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.3211 - acc: 0.9129\n",
      "Epoch 86/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.2412 - acc: 0.9253\n",
      "Epoch 87/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.3188 - acc: 0.9114\n",
      "Epoch 88/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.1909 - acc: 0.9456\n",
      "Epoch 89/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.1852 - acc: 0.9362\n",
      "Epoch 90/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.2673 - acc: 0.9253\n",
      "Epoch 91/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 1.2553 - acc: 0.8212\n",
      "Epoch 92/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.9722 - acc: 0.6283\n",
      "Epoch 93/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.5565 - acc: 0.7838\n",
      "Epoch 94/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.4712 - acc: 0.8289\n",
      "Epoch 95/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.4819 - acc: 0.8445\n",
      "Epoch 96/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.4786 - acc: 0.8647\n",
      "Epoch 97/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.3618 - acc: 0.8911\n",
      "Epoch 98/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.3604 - acc: 0.8942\n",
      "Epoch 99/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.3462 - acc: 0.8942\n",
      "Epoch 100/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.5849 - acc: 0.8865\n",
      "Epoch 101/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 1.0851 - acc: 0.7558\n",
      "Epoch 102/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.6790 - acc: 0.8040\n",
      "Epoch 103/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.5670 - acc: 0.8103\n",
      "Epoch 104/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.4472 - acc: 0.8351\n",
      "Epoch 105/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.4425 - acc: 0.8631\n",
      "Epoch 106/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.9167 - acc: 0.7512\n",
      "Epoch 107/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.5309 - acc: 0.8460\n",
      "Epoch 108/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.5076 - acc: 0.8538\n",
      "Epoch 109/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.3838 - acc: 0.8787A: 0s - loss: 0\n",
      "Epoch 110/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.2894 - acc: 0.9082\n",
      "Epoch 111/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.4341 - acc: 0.9005\n",
      "Epoch 112/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.9470 - acc: 0.8072\n",
      "Epoch 113/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.2716 - acc: 0.9114\n",
      "Epoch 114/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.5221 - acc: 0.8429\n",
      "Epoch 115/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.5237 - acc: 0.8103\n",
      "Epoch 116/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.3894 - acc: 0.8818\n",
      "Epoch 117/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.3394 - acc: 0.8927\n",
      "Epoch 118/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.2455 - acc: 0.9238\n",
      "Epoch 119/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.2032 - acc: 0.9362\n",
      "Epoch 120/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.1920 - acc: 0.9300\n",
      "Epoch 121/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.5458 - acc: 0.8227\n",
      "Epoch 122/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.7770 - acc: 0.7854\n",
      "Epoch 123/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.8294 - acc: 0.7745\n",
      "Epoch 124/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.6354 - acc: 0.7589\n",
      "Epoch 125/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.8509 - acc: 0.7729\n",
      "Epoch 126/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.5035 - acc: 0.8460\n",
      "Epoch 127/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.6044 - acc: 0.7745\n",
      "Epoch 128/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.4087 - acc: 0.8445\n",
      "Epoch 129/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.3097 - acc: 0.8989\n",
      "Epoch 130/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.3036 - acc: 0.8974\n",
      "Epoch 131/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.4132 - acc: 0.9036\n",
      "Epoch 132/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.3137 - acc: 0.9145\n",
      "Epoch 133/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 1.3357 - acc: 0.6267\n",
      "Epoch 134/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 1.9918 - acc: 0.6252\n",
      "Epoch 135/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 1.5038 - acc: 0.5925\n",
      "Epoch 136/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.9466 - acc: 0.6096\n",
      "Epoch 137/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.8395 - acc: 0.6796\n",
      "Epoch 138/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.7577 - acc: 0.6905\n",
      "Epoch 139/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 1.0383 - acc: 0.6345\n",
      "Epoch 140/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.7368 - acc: 0.6998\n",
      "Epoch 141/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 1.0324 - acc: 0.6874\n",
      "Epoch 142/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 1.5527 - acc: 0.6563\n",
      "Epoch 143/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 1.0802 - acc: 0.6781\n",
      "Epoch 144/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.9634 - acc: 0.5677\n",
      "Epoch 145/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.6899 - acc: 0.6905\n",
      "Epoch 146/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.5857 - acc: 0.7543\n",
      "Epoch 147/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.6311 - acc: 0.7512\n",
      "Epoch 148/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.6069 - acc: 0.7621\n",
      "Epoch 149/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.6769 - acc: 0.7232\n",
      "Epoch 150/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.9526 - acc: 0.6921\n",
      "Epoch 151/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.8963 - acc: 0.6485\n",
      "Epoch 152/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.8401 - acc: 0.7061\n",
      "Epoch 153/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 1.3119 - acc: 0.6625\n",
      "Epoch 154/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.6860 - acc: 0.7372\n",
      "Epoch 155/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.7588 - acc: 0.7278\n",
      "Epoch 156/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.9621 - acc: 0.7185\n",
      "Epoch 157/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.6786 - acc: 0.7496\n",
      "Epoch 158/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.9342 - acc: 0.6283\n",
      "Epoch 159/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.8360 - acc: 0.7045\n",
      "Epoch 160/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 1.0404 - acc: 0.7201\n",
      "Epoch 161/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.7060 - acc: 0.7356\n",
      "Epoch 162/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.6647 - acc: 0.7698\n",
      "Epoch 163/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.6122 - acc: 0.8087\n",
      "Epoch 164/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.6801 - acc: 0.7341\n",
      "Epoch 165/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.7448 - acc: 0.6765\n",
      "Epoch 166/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.6214 - acc: 0.7294\n",
      "Epoch 167/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.5460 - acc: 0.7994\n",
      "Epoch 168/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "643/643 [==============================] - 1s 2ms/step - loss: 0.6280 - acc: 0.7589\n",
      "Epoch 169/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.5329 - acc: 0.7994\n",
      "Epoch 170/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.6370 - acc: 0.7558\n",
      "Epoch 171/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.5239 - acc: 0.7963\n",
      "Epoch 172/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.5164 - acc: 0.8040\n",
      "Epoch 173/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.5914 - acc: 0.8103\n",
      "Epoch 174/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.5059 - acc: 0.8165\n",
      "Epoch 175/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.6088 - acc: 0.7589\n",
      "Epoch 176/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.4778 - acc: 0.8429\n",
      "Epoch 177/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.5751 - acc: 0.8180\n",
      "Epoch 178/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.5698 - acc: 0.7885\n",
      "Epoch 179/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.9135 - acc: 0.6407\n",
      "Epoch 180/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.7090 - acc: 0.6905\n",
      "Epoch 181/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.5690 - acc: 0.7792\n",
      "Epoch 182/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.5517 - acc: 0.8087\n",
      "Epoch 183/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.6972 - acc: 0.7247\n",
      "Epoch 184/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.5890 - acc: 0.7900\n",
      "Epoch 185/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.5860 - acc: 0.8056\n",
      "Epoch 186/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.7493 - acc: 0.7325\n",
      "Epoch 187/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 1.0502 - acc: 0.7760\n",
      "Epoch 188/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 1.0336 - acc: 0.7636\n",
      "Epoch 189/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.5778 - acc: 0.7978\n",
      "Epoch 190/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.5452 - acc: 0.8103\n",
      "Epoch 191/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.6145 - acc: 0.7418\n",
      "Epoch 192/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 0.6173 - acc: 0.7636\n",
      "Epoch 193/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 1.4652 - acc: 0.5925\n",
      "Epoch 194/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 2.2138 - acc: 0.3297\n",
      "Epoch 195/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 2.0086 - acc: 0.3110\n",
      "Epoch 196/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 1.5873 - acc: 0.3810\n",
      "Epoch 197/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 1.3847 - acc: 0.4323\n",
      "Epoch 198/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 1.2014 - acc: 0.5583\n",
      "Epoch 199/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 2.0638 - acc: 0.4946\n",
      "Epoch 200/200\n",
      "643/643 [==============================] - 1s 2ms/step - loss: 2.2594 - acc: 0.5583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5bd8550978>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train[feature],train[[0,1,2,3]] , epochs=200, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# y_pred= model.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.78211588e-01, 1.89585760e-02, 5.68590820e-01, 3.42390426e-02],\n",
       "       [5.91818213e-01, 9.71862208e-03, 3.81879598e-01, 1.65835693e-02],\n",
       "       [9.67559040e-01, 3.42960731e-04, 3.20831798e-02, 1.49345624e-05],\n",
       "       ...,\n",
       "       [8.06114972e-02, 1.52870305e-02, 3.49825472e-01, 5.54275990e-01],\n",
       "       [1.26814755e-21, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00],\n",
       "       [3.49510663e-11, 0.00000000e+00, 2.35323795e-24, 1.00000000e+00]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred= model.predict(test[feature])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cnv = []\n",
    "for i in y_pred:\n",
    "#     print (i.index(max(i)))\n",
    "    result = np.where(i == np.amax(i))\n",
    "    y_pred_cnv.append(result[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.37      0.50       147\n",
      "           1       0.00      0.00      0.00       161\n",
      "           2       0.38      0.35      0.36       147\n",
      "           3       0.29      0.76      0.42       150\n",
      "\n",
      "    accuracy                           0.37       605\n",
      "   macro avg       0.35      0.37      0.32       605\n",
      "weighted avg       0.34      0.37      0.31       605\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test['label'], y_pred_cnv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "413     0\n",
       "414     0\n",
       "415     0\n",
       "416     0\n",
       "417     0\n",
       "       ..\n",
       "1240    3\n",
       "1241    3\n",
       "1242    3\n",
       "1243    3\n",
       "1244    3\n",
       "Name: label, Length: 605, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "605"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred_cnv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "605"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting clasifier to the Training set\n",
    "# Loading libraries\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Instantiate learning model (k = 3)\n",
    "classifier = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "# Fitting the model\n",
    "classifier.fit(train[feature], train['label'])\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(test[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of our model is equal 47.27 %.\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score( test['label'], y_pred)*100\n",
    "print('Accuracy of our model is equal ' + str(round(accuracy, 2)) + ' %.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating list of K for KNN\n",
    "k_list = list(range(1,50,2))\n",
    "# creating list of cv scores\n",
    "cv_scores = []\n",
    "\n",
    "# perform 10-fold cross validation\n",
    "for k in k_list:\n",
    "    knn = KNeighborsClassifier(n_neighbors=1)\n",
    "    scores = cross_val_score(knn,train[feature], train['label'], cv=10, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# changing to misclassification error\n",
    "MSE = [1 - x for x in cv_scores]\n",
    "\n",
    "plt.figure()\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.title('The optimal number of neighbors', fontsize=20, fontweight='bold')\n",
    "plt.xlabel('Number of Neighbors K', fontsize=15)\n",
    "plt.ylabel('Misclassification Error', fontsize=15)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.plot(k_list, MSE)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal number of neighbors is 1.\n"
     ]
    }
   ],
   "source": [
    "# finding best k\n",
    "best_k = k_list[MSE.index(min(MSE))]\n",
    "print(\"The optimal number of neighbors is %d.\" % best_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Masking, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding layer\n",
    "model.add(\n",
    "    Embedding(input_dim=num_words,\n",
    "              input_length = training_length,\n",
    "              output_dim=100,\n",
    "              weights=[embedding_matrix],\n",
    "              trainable=False,\n",
    "              mask_zero=True))\n",
    "\n",
    "# Masking layer for pre-trained embeddings\n",
    "model.add(Masking(mask_value=0.0))\n",
    "\n",
    "# Recurrent layer\n",
    "model.add(LSTM(64, return_sequences=False, \n",
    "               dropout=0.1, recurrent_dropout=0.1))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Dropout for regularization\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(num_words, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=6)\n",
    "pri= pca.fit_transform(df_new[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "pri_df = pd.DataFrame(data = pri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "pri_df['label']=df_new['label']\n",
    "pri_df['subject']=df_new['subject']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>label</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>355.703333</td>\n",
       "      <td>161.695543</td>\n",
       "      <td>-29.601887</td>\n",
       "      <td>-4.555489</td>\n",
       "      <td>15.444845</td>\n",
       "      <td>1.812838</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>46.356853</td>\n",
       "      <td>-33.122912</td>\n",
       "      <td>3.650295</td>\n",
       "      <td>-9.073137</td>\n",
       "      <td>1.521089</td>\n",
       "      <td>2.876832</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>206.353186</td>\n",
       "      <td>32.211138</td>\n",
       "      <td>6.270031</td>\n",
       "      <td>-10.922045</td>\n",
       "      <td>1.322663</td>\n",
       "      <td>2.017480</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>436.441082</td>\n",
       "      <td>-166.672579</td>\n",
       "      <td>2.617984</td>\n",
       "      <td>-7.945180</td>\n",
       "      <td>4.248592</td>\n",
       "      <td>0.115255</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-128.977463</td>\n",
       "      <td>23.441992</td>\n",
       "      <td>6.039927</td>\n",
       "      <td>-9.034971</td>\n",
       "      <td>0.577453</td>\n",
       "      <td>2.403714</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1243</td>\n",
       "      <td>-282.407709</td>\n",
       "      <td>-0.857713</td>\n",
       "      <td>1.583712</td>\n",
       "      <td>2.874818</td>\n",
       "      <td>-2.141742</td>\n",
       "      <td>1.283678</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1244</td>\n",
       "      <td>-203.168126</td>\n",
       "      <td>-22.727117</td>\n",
       "      <td>-18.428431</td>\n",
       "      <td>0.526973</td>\n",
       "      <td>-3.126724</td>\n",
       "      <td>5.530540</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1245</td>\n",
       "      <td>-194.919006</td>\n",
       "      <td>-7.388183</td>\n",
       "      <td>-14.672750</td>\n",
       "      <td>-1.706346</td>\n",
       "      <td>-1.636393</td>\n",
       "      <td>1.581972</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1246</td>\n",
       "      <td>-180.660972</td>\n",
       "      <td>-26.678439</td>\n",
       "      <td>4.898846</td>\n",
       "      <td>-0.379967</td>\n",
       "      <td>-0.813657</td>\n",
       "      <td>0.070765</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1247</td>\n",
       "      <td>-224.868257</td>\n",
       "      <td>-7.896414</td>\n",
       "      <td>-10.114670</td>\n",
       "      <td>-2.462097</td>\n",
       "      <td>-0.530388</td>\n",
       "      <td>-2.791037</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1248 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1          2          3          4         5  \\\n",
       "0     355.703333  161.695543 -29.601887  -4.555489  15.444845  1.812838   \n",
       "1      46.356853  -33.122912   3.650295  -9.073137   1.521089  2.876832   \n",
       "2     206.353186   32.211138   6.270031 -10.922045   1.322663  2.017480   \n",
       "3     436.441082 -166.672579   2.617984  -7.945180   4.248592  0.115255   \n",
       "4    -128.977463   23.441992   6.039927  -9.034971   0.577453  2.403714   \n",
       "...          ...         ...        ...        ...        ...       ...   \n",
       "1243 -282.407709   -0.857713   1.583712   2.874818  -2.141742  1.283678   \n",
       "1244 -203.168126  -22.727117 -18.428431   0.526973  -3.126724  5.530540   \n",
       "1245 -194.919006   -7.388183 -14.672750  -1.706346  -1.636393  1.581972   \n",
       "1246 -180.660972  -26.678439   4.898846  -0.379967  -0.813657  0.070765   \n",
       "1247 -224.868257   -7.896414 -10.114670  -2.462097  -0.530388 -2.791037   \n",
       "\n",
       "      label  subject  \n",
       "0         0        2  \n",
       "1         0        2  \n",
       "2         0        2  \n",
       "3         0        2  \n",
       "4         0        2  \n",
       "...     ...      ...  \n",
       "1243      3       11  \n",
       "1244      3       13  \n",
       "1245      3        9  \n",
       "1246      3        5  \n",
       "1247      3        9  \n",
       "\n",
       "[1248 rows x 8 columns]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pri_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pri_df[pri_df['subject']<=9]\n",
    "test=pri_df[pri_df['subject']>9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>355.703333</td>\n",
       "      <td>161.695543</td>\n",
       "      <td>-29.601887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>46.356853</td>\n",
       "      <td>-33.122912</td>\n",
       "      <td>3.650295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>206.353186</td>\n",
       "      <td>32.211138</td>\n",
       "      <td>6.270031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>436.441082</td>\n",
       "      <td>-166.672579</td>\n",
       "      <td>2.617984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-128.977463</td>\n",
       "      <td>23.441992</td>\n",
       "      <td>6.039927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1238</td>\n",
       "      <td>-229.751879</td>\n",
       "      <td>-6.792178</td>\n",
       "      <td>-9.323451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1239</td>\n",
       "      <td>-116.799906</td>\n",
       "      <td>57.486178</td>\n",
       "      <td>1.185857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1245</td>\n",
       "      <td>-194.919006</td>\n",
       "      <td>-7.388183</td>\n",
       "      <td>-14.672750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1246</td>\n",
       "      <td>-180.660972</td>\n",
       "      <td>-26.678439</td>\n",
       "      <td>4.898846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1247</td>\n",
       "      <td>-224.868257</td>\n",
       "      <td>-7.896414</td>\n",
       "      <td>-10.114670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>643 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1          2\n",
       "0     355.703333  161.695543 -29.601887\n",
       "1      46.356853  -33.122912   3.650295\n",
       "2     206.353186   32.211138   6.270031\n",
       "3     436.441082 -166.672579   2.617984\n",
       "4    -128.977463   23.441992   6.039927\n",
       "...          ...         ...        ...\n",
       "1238 -229.751879   -6.792178  -9.323451\n",
       "1239 -116.799906   57.486178   1.185857\n",
       "1245 -194.919006   -7.388183 -14.672750\n",
       "1246 -180.660972  -26.678439   4.898846\n",
       "1247 -224.868257   -7.896414 -10.114670\n",
       "\n",
       "[643 rows x 3 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[[0,1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  21 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=10)]: Done  50 out of  50 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 50\n",
      "building tree 2 of 50\n",
      "building tree 3 of 50\n",
      "building tree 4 of 50\n",
      "building tree 5 of 50\n",
      "building tree 6 of 50\n",
      "building tree 7 of 50\n",
      "building tree 8 of 50\n",
      "building tree 9 of 50\n",
      "building tree 10 of 50\n",
      "building tree 11 of 50building tree 12 of 50\n",
      "\n",
      "building tree 13 of 50\n",
      "building tree 14 of 50building tree 15 of 50\n",
      "building tree 16 of 50\n",
      "building tree 17 of 50\n",
      "\n",
      "building tree 18 of 50\n",
      "building tree 19 of 50\n",
      "building tree 20 of 50\n",
      "building tree 21 of 50\n",
      "building tree 22 of 50building tree 23 of 50\n",
      "\n",
      "building tree 24 of 50\n",
      "building tree 25 of 50\n",
      "building tree 26 of 50\n",
      "building tree 27 of 50\n",
      "building tree 28 of 50\n",
      "building tree 29 of 50\n",
      "building tree 30 of 50\n",
      "building tree 31 of 50\n",
      "building tree 32 of 50\n",
      "building tree 33 of 50building tree 34 of 50\n",
      "building tree 35 of 50\n",
      "\n",
      "building tree 36 of 50\n",
      "building tree 37 of 50\n",
      "building tree 38 of 50\n",
      "building tree 39 of 50\n",
      "building tree 40 of 50\n",
      "building tree 41 of 50\n",
      "building tree 42 of 50\n",
      "building tree 43 of 50\n",
      "building tree 44 of 50building tree 45 of 50\n",
      "building tree 46 of 50\n",
      "\n",
      "building tree 47 of 50\n",
      "building tree 48 of 50\n",
      "building tree 49 of 50\n",
      "building tree 50 of 50\n",
      "CPU times: user 172 ms, sys: 119 ms, total: 291 ms\n",
      "Wall time: 224 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "                     max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                     min_samples_leaf=1, min_samples_split=2,\n",
       "                     min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=10,\n",
       "                     oob_score=False, random_state=None, verbose=2,\n",
       "                     warm_start=False)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "et = ExtraTreesClassifier(n_estimators=50, n_jobs=10, verbose=2)\n",
    "et.fit(train[[0,1,2,3,4,5]],train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.34      0.43       147\n",
      "           1       0.47      0.73      0.57       161\n",
      "           2       0.57      0.42      0.49       147\n",
      "           3       0.54      0.58      0.56       150\n",
      "\n",
      "    accuracy                           0.52       605\n",
      "   macro avg       0.54      0.52      0.51       605\n",
      "weighted avg       0.54      0.52      0.51       605\n",
      "\n",
      "CPU times: user 30.9 ms, sys: 25.9 ms, total: 56.8 ms\n",
      "Wall time: 112 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  21 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=10)]: Done  50 out of  50 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred= et.predict(test[[0,1,2,3,4,5]])\n",
    "print(classification_report(test['label'], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variation per principal component: [0.93812302 0.05900392 0.00183012]\n"
     ]
    }
   ],
   "source": [
    "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting the Cumulative Summation of the Explained Variance\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') #for each component\n",
    "plt.title('Pulsar Dataset Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
